[ { "title": "Murder on the Mississippi (Activision) - 1986", "url": "/blog/2023/05/10/murder-on-the-mississippi/", "categories": "Let's Adventure!", "tags": "adventure, Activision", "date": "2023-05-10 06:27:31 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Murder on the Mississippi is a 1986 detective adventure game developed and published by Activisi...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Murder on the Mississippi is a 1986 detective adventure game developed and published by Activision for the Commodore 64, Commodore 128 and the Apple II computers, where you assume the role of Sir Charles Foxworth and must solve a murder mystery on the luxury ship “Delta Princess”. The story is based loosely on the Agatha Christie novel Death on the Nile.When playing these earlier PC games it’s important to call out what they did right, and where they may have been innovative. This game starts off with some theme music and an animated introduction, the sets the stage for our character’s journey. Once you’re in your cabin and gain control of Sir Charles, you have an opportunity to move around and get acquainted with the controls.You leave your cabin and start walking around, testing doors and learning how to navigate the ship. Eventually you’ll make your way from cabin #3 (your home cabin) to the other side of the ship and cabin #4. Once inside you discover a murder has taken place and it’s crime solving time.Unlike most verb-based adventure games, Murder on the Mississippi offers you only a handful of options on every screen. These options will change depending on whether there’s an NPC on screen or not, but ultimately you can always WALK AROUND, INSPECT or EXAMINE EVIDENCE. Inspecting the environment is contextual, so you need to be “near” something that you can interact with (such as a table, cabinet, etc).If you’re lucky, there may be a clue hidden in, around, behind or under whatever you happen to be inspecting.Well look at that, there was a key under the pillow!The ship contains 28 locations you can investigate (27 cabins and the wheelhouse where the captain is). Though this may seem like a lot, there are really only a handful of cabins that contain anything notable, and the cast of characters you can interact with is actually pretty small.Since this is a murder mystery, interacting with characters to piece together their stories is a big part of the game. I found the approach they took here to be pretty clever, though it may not have been intuitive.Every character you meet gives you the option to ask them about either (a) the victim, or (b) any other character you’ve already met. For each of these interactions you’re able to “take notes”, which involves moving the cursor over each word in their response and forming a sentence you may want to use when speaking to other characters.This mechanic is clever, but actually extremely hard to get right. The thing is, you can ask characters about the victim as many times as you want, but you can only ask them about other characters once. If you forget to take notes, or you take the wrong note - you’re fucked.As you collect these notes, you can share them with other characters. This is how you advance the plot - assuming you wrote the write thing down in the first place. Incorrect notes give you a generic response, which sucks because you have no idea that you’re on the right track but asked the wrong thing.I used a walkthrough for this portion so I’m not sure if it would have worked to just copy the entire response as the note (like copy literally everything every character said).As you find clues you’ll periodically have to go back to your cabin to stow them in the steamer trunk. This is both because your sidekick (Regis) can only carry so many items. Within the steamer trunk you’re able to examine individual items, or combine them by placing two items on the examination table and selecting EXAMINE.There are a number of clues you can combine, and I’d highly recommend you examine everything (and all combinations) more than once. For example, once you unlock the rosewood box with the key and find what’s inside, examining it a second time reveals a false bottom with more clues in it.When you’re finally satisfied that you’ve solved the murder, the last thing to do is go to the character’s cabin you think is guilty and accuse them. This results in the characters being gathered together, a long exchange between all of them and Sir Charles and hopefully a confession from the killer.Overall I found this game to be pretty well paced and engaging. There are a couple places where you can die because the killer has set traps that you’ll need to avoid and these will catch you by surprise the first time - so save A LOT. Similarly every time I wanted to ask a character about what another character was saying I’d save before taking notes in case what I noted ended up not getting a response.The note taking/sharing was interesting but ultimately frustrating as it’s so easy to get wrong. Thankfully there weren’t too many combinations you of note and characters, and it became increasingly easy to determine when characters weren’t sharing anything noteworthy (so you could skip taking notes on those responses).If all goes well you make it to New OrleansI didn’t think I was going to like this game, but in the end I really did. The music is pretty simple - but it’s there. Every character has their own tune that plays when you enter their cabin, there’s background music as you walk around the ship and you get occasional sound effects as you bump into things as you investigate.Murder on the Mississippi is not a great game, but I had a good time working through the mystery and eventually finding the killer. Though you literally have to investigate EVERYTHING to try and surface clues where there’s no indication anything might exist, the game is still pretty short and can be enjoyable (unless you’re this Japanese gamer that spent 2 years making a strategy guide to clear the game).Game Information Game Murder on the Mississippi Developer Activision Publisher Activision Release Date 1986 Systems Apple II, Commodore 64/128, MSX2, Atari 800/XE/XL Game Engine   Play Information How Long To Beat? 4 hours Version Played Commodore 64 via VICE Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 7 Story (25) 9 Experience (15) 5 Impact (10) 3   34% Gallery" }, { "title": "Round Trips to Authenticate a MongoDB Client Connection", "url": "/blog/2023/05/04/round-trips-to-authenticate-a-mongodb-client-connection/", "categories": "MongoDB", "tags": "mongodb, connections, drivers", "date": "2023-05-04 14:40:52 -0400", "snippet": "When MongoDB Drivers establish a connection with a MongoDB cluster a number of network round trips are performed. This can result in increased latency when measuring the time to response of an oper...", "content": "When MongoDB Drivers establish a connection with a MongoDB cluster a number of network round trips are performed. This can result in increased latency when measuring the time to response of an operation following a cold start, so it’s worth understanding what the anatomy of an authenticated connection is - as well as what can be done to improve an initial operations round trip time (RTT).Current StateConnection ProtocolTypically a MongoDB connection string will contain a standard seed list, which is represented by the mongodb:// protocol followed by a list of servers (ex: mongodb://localhost:27017,localhost:27018....).Starting with MongoDB 3.6 instead of having to provide the seed list in the connection string manually a DNS-constructed seed list could be used as well. With this configuration the mongodb+srv:// protocol is used to communicate both the seed list as well as any options by performing two (2) DNS queries to resolve the following DNS records: SRV and TXT.See “MongoDB 3.6: Here to SRV you with easier replica set connections” for more information regarding this topic.Additionally there may be an RTT for A/AAAA/CNAME resolution, however these may be done in parallel and may also be cached. DNS Caching will likely improve the performance of these queries, but it’s worth noting their presence within the connection establishment and authentication lifecycle./* Network Round Trips */ [0 , 3] // ProtocolNote that if the SRV record returns multiple hosts, those A/AAAA/CNAME records will be resolved in parallel. And DNS servers typically will optimize the traversal returning any intermediate CNAMEs followed by the A/AAAA in the same request.This also assumes UDP-based DNS resolution. If you exceed the UDP packet size, you might first try UDP, receive an error, and retry using TCP (and possibly requiring a TCP handshake to the DNS server as well).TCP HandshakeSource: makeuseof.comOnce a host is known from the seed list, next we need to connect to it. This is done using a standard TCP 3-way Handshake, which constitutes 1 RTT. Note that as there is an ACK sent following the SYN/ACK this handshake is sometimes considered to be 1.5 RTT, however most TCP stacks will send the first data packet with the ACK./* Network Round Trips */ [0 , 3] // Protocol+ 1 // TCPTLS HandshakeSource: cloudflare.comTo ensure all connections to MongoDB Atlas are secure, Transport Layer Security (TLS) is enabled by default. Following a successful TCP handshake, a TLS handshake will be performed./* Network Round Trips */ [0 , 3] // Protocol+ 1 // TCP+ 2 // TLSMongoDB HandshakeNow that we have established a TLS secured TCP socket connection to a MongoDB host (mongos or mongod), the MongoDB Driver will send a hello command to perform the initial handshake.This step is required to determine that the host at the other end of the socket is actually a MongoDB server. Assuming the version of the MongoDB Driver supports MongoDB 4.4+ the handshake will also include a speculativeAuthenticate argument. Specifying this argument to hello will speculatively include the first command of an authentication handshake, thus eliminating one round trip as the saslStart command doesn’t need to be sent during the authentication handshake./* Network Round Trips */ [0 , 3] // Protocol+ 1 // TCP+ 2 // TLS+ 1 // MongoDBAuthentication HandshakeMongoDB supports a number of SASL (Simple Authentication and Security Layer) mechanisms. By default the SASL mechanism that will be used will be a SCRAM mechanism (either SCRAM-SHA-1 or SCRAM-SHA-256), which effectively means “username and password”. Note that this is a “challenge response” mechanism, so these credentials aren’t broadcast in the clear.As outlined in the MongoDB Authentication specification, a SCRAM-SHA-256 conversation will be made up of 2 round trips as follows:&gt;&gt; {saslStart: 1, mechanism:\"SCRAM-SHA-256\", options: {skipEmptyExchange: true}, payload: BinData(0, \"...=\")}&lt;&lt; {conversationId: 1, payload: BinData(0, \"...=\"), done: false, ok: 1}&gt;&gt; {saslContinue: 1, conversationId: 1, payload: BinData(0, \"...==\")}&lt;&lt; {conversationId: 1, payload: BinData(0, \"...==\"), done: true, ok: 1}For backwards compatibility with MongoDB 4.2 or earlier, MongoDB Drivers support a longer SCRAM conversation which includes an additional saslContinue command being sent as follows:&gt;&gt; {saslStart: 1, mechanism: \"SCRAM-SHA-1\", payload: BinData(0, \"...\"), options: {skipEmptyExchange: true}}&lt;&lt; {conversationId : 1, payload: BinData(0,\"...\"), done: false, ok: 1}&gt;&gt; {saslContinue: 1, conversationId: 1, payload: BinData(0, \"...\")}&lt;&lt; {conversationId: 1, payload: BinData(0,\"...\"), done: false, ok: 1}&gt;&gt; {saslContinue: 1, conversationId: 1, payload: BinData(0, \"\")}&lt;&lt; {conversationId: 1, payload: BinData(0,\"\"), done: true, ok: 1}RTT was improved with MongoDB 4.4+ as 2 round trips can potentially be avoided: when speculativeAuthenticate is used the saslStart command is incorporated into the initial MongoDB handshake when the saslStart command contains the skipEmptyExchange: true option, the second saslContinue command can be skipped/* Network Round Trips */ [0 , 3] // Protocol+ 1 // TCP+ 2 // TLS+ 1 // MongoDB+ [2 , 3] // AuthenticationReducing Round TripsAs outlined above there are a number of network round trips required to authenticate a client connection to a MongoDB host using a username and password:/* Network Round Trips */ [0 , 3] // Protocol+ 1 // TCP+ 2 // TLS+ 1 // MongoDB+ [2 , 3] // Authentication---------------------------[6 , 10]MongoDB 4.4 has been out since at least September 2020, so chances are most applications are connecting to at least this version or newer. This would put the average round trip count for authenticating a connection between 6 and 9 (depending on what protocol is being used and if DNS results were previously cached).Next let’s review what can be done to reduce these round trips where possible.Use x.509 AuthenticationWhen using x.509 certificates to authenticate clients, the conversation with the server does not require a saslContinue. Assuming this speculativeAuthenticate of the initial handshake succeeds (which it should), two full round trip can be removed!/* Network Round Trips */ [0 , 3] // Protocol+ 1 // TCP+ 2 // TLS+ 1 // MongoDB+ 0 // Authentication---------------------------[4 , 7]Note there’s no authentication round trips as the speculativeAuthenticate succeeding contains the information typically sent via the saslStart command.Use TLS 1.3+Source: thesslstore.comTLS 1.3 (RFC 8446) can authenticate a connection approximately twice as fast as TLS 1.2 (RFC 5246) by eliminating a full round trip.Though not necessarily supported (at time of writing) by the MongoDB Server, additional non-standard TLS 1.3 configurations can further help speed up encrypted connections - such as TLS false start and Zero Round Trip Time (0-RTT). See “TLS 1.3: Everything you need to know” for more information./* Network Round Trips */ [0 , 3] // Protocol+ 1 // TCP+ 1 // TLS+ 1 // MongoDB+ [0 , 3] // Authentication---------------------------[3 , 9]ConclusionIn some environments (such as Function as a Service) the cold start time of an application is critically important. The time to authenticate a connection to a MongoDB host and how this can be improved can be useful in improving operational latency of applications.Out of the box there may be upwards of 9 network round trips (SRV+TCP+TLS+MONGODB+AUTH), however this can potentially be cut in half (or more) by understanding what configuration and authentication options exist and how they can be applied." }, { "title": "Identifying Ruby Developers' Favourite IDEs for Ruby/Rails in 2023", "url": "/blog/2023/05/01/favourite-ruby-slash-rails-ides/", "categories": "Product Management", "tags": "survey, ruby, rails", "date": "2023-05-01 15:38:12 -0400", "snippet": " Cross posted to DEVAs a Product Manager, data is everything when it comes to making decisions. One of my responsibilities as a PM for Developer Interfaces is to understand how our developer commu...", "content": " Cross posted to DEVAs a Product Manager, data is everything when it comes to making decisions. One of my responsibilities as a PM for Developer Interfaces is to understand how our developer communities work most effectively, and what their preferred tooling and stacks look like.When focusing on the Ruby developer community, a simple question to ask would be “What is your favourite editor/IDE when working with Ruby / Rails?” JetBrains’ developer survey from 2021 found that 48% of Ruby developers mostly use RubyMine - which may be accurate - but the survey was run by the vendor responsible for RubyMine and may have skewed results based on their sample group consisting of fans/customers/users of their products.SetupI wanted to try running my own survey to see if the responses align with what JetBrains found. Since my Reddit survey was conducted 2 years after JetBrains’ how have things changed?This was my first time running a Reddit poll, but found the community to be extremely engaged and willing to provide honest and targeted feedback. I only setup the poll to run for 72 hours, but during that time period I received: 931 Votes 20K Total Views 50 Upvotes 41 CommentsUnfortunately Reddit doesn’t appear to consistently present engagement statistics (views, upvotes) so this is an approximation based on observation during the period the poll was open.ResultsThe poll contained 6 options (the maximum Reddit allows) and was open from 2023-04-25 to 2023-04-28.pie title Favourite Editors for Working with Ruby/Rails \"RubyMine\": 244 \"Visual Studio Code\": 418 \"Vim/Neovim\": 145 \"SublimeText\": 62 \"Emacs\": 25 \"Other\": 37Based on feedback I’d received in the comments I updated Vim to also include Neovim, however I’d have preferred to keep these options separate.The fact that Visual Studio Code was almost half of the poll’s respondents favourite editor wasn’t surprising, however over a quarter still favoured RubyMine. This is far less that what JetBrains found, but my sample size is much smaller and audience potentially more diverse. Still the size of the user base for RubyMine is large enough that it should not be discounted when developing strategies when discussing developer tooling for Ruby specifically!Also minor fun fact since the topic here is Ruby developer tooling. This blog is built using Jekyll, and the chart above was created using Mermaid and a script similar to the jekyll-mermaid plugin using a code block like the one below:pie title Favourite Editors for Working with Ruby/Rails \"RubyMine\": 244 \"Visual Studio Code\": 418 \"Vim/Neovim\": 145 \"SublimeText\": 62 \"Emacs\": 25 \"Other\": 37OutcomeThe goal of this exercise was to learn 2 things specifically:Based on Reddit, what are Ruby developers favourite IDEs or EditorsThe main IDEs/Editors currently favoured by the users I polled were Visual Studio Code, Neovim and RubyMine. I had an entry in the poll of “Other”, which I believe (based on the comments) is as follows: Textmate Atom IntelliJ IDEA Notepad++Developers also seem to prefer editors that support Solargraph for Intellisense.Is Reddit a good platform for this type of exerciseDepending on the community/subreddit you choose to run the poll in your mileage may vary. The r/rails subreddit has 57K users, so the chances of getting decent engagement were moderate, and given that I only ran this poll for 3 days I feel the response rate was quite high.I will definitely use this method again in the future.ConclusionWhen promoting this poll I posted to LinkedIn and Twitter only. Some of the feedback I got alluded to Reddit potentially discouraging some participation as users may not have accounts and may not want to create an account just to vote in a poll. Further to this, Reddit limits the number of items you can include in your poll to 6 items and it seems this is by design and won’t be increased any time soon.Being able to capture feedback from participants without there being a barrier to entry (like creating an account on Reddit) may improve engagement, however there is definite value in targeting a poll directly to the community represented in a subreddit." }, { "title": "Mission Asteroid (On-Line Systems) - 1980", "url": "/blog/2023/04/26/mission-asteroid/", "categories": "Let's Adventure!", "tags": "adventure, On-Line Systems, ADL", "date": "2023-04-26 20:27:47 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Mission Asteroid is a graphic adventure game for the Apple II written by Ken and Roberta William...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Mission Asteroid is a graphic adventure game for the Apple II written by Ken and Roberta Williams and released in 1980 by On-Line Systems. It was later ported to the Atari 8-bit family and Commodore 64.You’re not really introduced to the game in any way other than being plopped from the title screen to the first playable screen, but apparently an asteroid is on a collision course with the earth, and you play as an astronaut tasked with flying to the asteroid and blowing it up before it’s too late.I don’t want to say the plot of the movie Armageddon was very loosely based on this game (because I have zero evidence to back this up), but I want to believe it’s possible …The graphics may not be great … but at least it’s not JUST text on screen right?For a game released in 1980, the graphics actually aren’t too bad - though the character portraits really look like they were drawn by a child. Seeing as these early games weren’t using bitmap graphics but vector-ish graphics as each scene is “drawn” from coordinate details, I’m guessing their tooling was limited and this was the best of the best at that time.Though ScummVM supports all these ADL engine games, I chose to play the Apple II version on microM8 just to give myself an excuse to try out a new emulator. I play a lot of these games on an old MacBook Pro, and my go to for emulating Apple II games was AppleWin, so it’s good to know I have a viable alternative for OSX now.Ah shit, I PUSHed the throttle instead of PULLing it … and now I’m deadIt wouldn’t be a Sierra On-Line game without you being able to fuck up and die by typing the wrong command, and Mission Asteroid is no different. There really aren’t that many game screens to explore so your chances of messing up are pretty slim - but they do exist.As with any of these early adventure games, save early and save often.Ken and Roberta thank you for saving the planet!This game was promoted as an entry level game to get players used to these types of adventure games, so you can forgive the simplicity. There’s no real character or plot development, no music and I don’t think there were really any sound effects.If you’re just starting out with adventure games I highly doubt you’d be going back this far as your entry point into the genre, but if you do this is a pretty easy game to get you going.Game Information Game Mission Asteroid Developer On-Line Systems Publisher On-Line Systems Release Date 1980 Systems Apple II, Atari 8-bit, Commodore 64 Game Engine ADL Play Information How Long To Beat? 1 hours Version Played Apple II via microM8 Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 4 Story (25) 3 Experience (15) 4 Impact (10) 2   19% Gallery" }, { "title": "King's Quest II: Romancing the Throne (Sierra On-Line) - 1985", "url": "/blog/2023/04/25/kings-quest-ii/", "categories": "Let's Adventure!", "tags": "adventure, Sierra On-Line, AGI", "date": "2023-04-25 06:13:16 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.King’s Quest II: Romancing the Throne is the second installment in the King’s Quest series of gr...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.King’s Quest II: Romancing the Throne is the second installment in the King’s Quest series of graphic adventure games by Sierra On-Line. It was originally released in 1985 for PC DOS/PCjr, and later made available for the Apple II/IIGS, Atari ST, and Amiga. It uses the same AGI game engine as King’s Quest I: Quest for the Crown and features King Graham as the player character. The title is a spoof of the 1984 film Romancing the Stone.Since the events of the first game you’ve taken over as king of Daventry, but are now getting lonely and want to go out and meet some women. Since Tinder doesn’t exist yet you rely on your magic mirror to find you a suitable match - which it does. Having seen a princess locked up in a tower somewhere you set off on an adventure to save said princess, bring her back with you and make her your queen.This followup entry to King’s Quest: Quest for the Crown uses the same AGI game engine. Being parser-based you have to type your commands in and they will be interpreted based on how well you’ve conveyed your intent. The types of interactions you can have are pretty limited, but you can forgive game engines from the mid-80s for their lack of sophistication.Most interactions will follow the VERB SUBJECT format, such as TALK MAN or GET TRIDENT.Make sure you LOOK at everything, and if it is potentially something you can pick up, GET it. These early Sierra games are essentially just a series of item combination puzzles, and you can find yourself in unwinnable situations if you forgot to get something early on. Personally I find this to be part of the appeal of these games, as it really forces you to be meticulous with how you approach each screen.For some reason this world is populated with fairy tale characters such as Little Red Riding hood, a fairy godmother … and Dracula. You’re not really given any background as to why these characters exist in this universe so you sort of just have to accept it.What this game introduces that I don’t believe was in the previous game is time-delayed events on certain screens. For example, you’ll need to give Red Riding Hood a basket of food, but she only appears on certain screens - and only after waiting on that screen for a random amount of time. These types of puzzles are needlessly frustrating as you have no indication that (a) you’re on the right screen, (b) you’ve waited long enough and should exit/re-enter/try-again or (c) the event will be triggered at all.Again, I’ll forgive these puzzle designs due to how old the game is … but it is extremely frustrating when you have to move between screens and just stand there waiting - hoping to trigger some event.Though the graphics really aren’t all that much better than the first game, for 1985 this was pretty impressive. The AGI engine is essentially a vector graphics engine as each screen is “drawn” based on the coordinate and fill instructions provided by the artists. Priority maps on each screen also create a sense of depth as you’re able to walk in front of or behind scenery, or be blocked from progressing if you collide with something you shouldn’t be able to walk through.Your goal is to find 3 golden keys that can be used to unlock the 3 nested doors to the realm where Valanice is imprisoned. Reading the inscription on the doors will give you a clue as to where the key is, which you then need to find and ue to unlock the door. The clues are pretty vague, but since the game world is small as long as you pick everything up, talk to everyone and try every combination of GIVE &lt;X&gt; TO &lt;Y&gt; on each screen you should be able to sort most puzzles out.Being a Sierra game, expect to die a lot. Though there aren’t too many stairs you can fall from in this game, there are still plenty of missteps that will kill you. If you fall into water, don’t forget to type SWIM so you don’t drown. This game gives you a shortcut for that action (I think you press the =), so this is an improvement over the original at least.Assuming you try every combination of items, character interactions and wait on every screen for any potential timed events to trigger you’ll still be able to breeze through this game in a couple hours. If you use a walkthrough, you can finish in about 45 minutes.Once you open all the magic doors and step through, you’ll make your way to the ivory tower and rescue the princess. To finish the game you need to KISS her (make sure you’re standing close enough), then say HOME to magically return to Daventry. There must have been clues alluding to these steps somewhere during the game that I missed, but I found myself stuck here for a bit until I resorted to a walkthrough. The whole “say the magic word” thing wasn’t clear at all …The EndOverall, this game’s fine. It’s a sequel using the same game engine that tells the continuation of King Graham’s story. There’s not a lot of improvement over the original, there’s no real background music or sound effects other than certain screens. Other than your motivation for going from point A to B being “find the girl” there’s not much story or character development.AGD Interactive released a remake of this game with updated graphics, sound and music in 2002. Though I haven’t played it myself I’ve heard nothing but good things about it (and their other games), so if you’re looking to play this game maybe skip the original and go straight to the remake.Game Information Game King’s Quest II: Romancing the Throne Developer Sierra On-Line Publisher Sierra On-Line Release Date May 1985 Systems DOS, Macintosh, Apple II, Apple IIGS,Amiga, Atari ST, PCjr Game Engine AGI Play Information How Long To Beat? 3 hours Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 12 Story (25) 8 Experience (15) 7 Impact (10) 2   41% Gallery" }, { "title": "Countdown (Access Software) - 1990", "url": "/blog/2023/04/20/countdown/", "categories": "Let's Adventure!", "tags": "adventure, Access Software", "date": "2023-04-20 06:41:28 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.In Countdown you play as Mason Powers, a CIA agent who wakes up in a Turkish mental hospital, su...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.In Countdown you play as Mason Powers, a CIA agent who wakes up in a Turkish mental hospital, suffering from partial amnesia and accused of murdering his supervisor. Powers must escape the hospital, find out who framed him, piece together his memory, and prevent terrorists from blowing up a peace conference.As the title of the game implies, you’ve got a limited time to finish this game before the bomb goes off. I’m not actually sure if this is communicated as you first start playing or if it’s in the game manual, but you’ve apparently only got 96 hours of game time to get everything sorted out.You start off in the insane asylum and can see something glinting under the bed. Using the mouse you can select from a number of verbs, which you then use to interact with the game environment.Character movement appears to be rotoscoped, and instead of tiny sprites on screen everything is big and detailed. Though this game seems to use a similar engine as Mean Streets (which only came out a year earlier), the quality of the graphics is much much better. You can move Mason around the screen with the keyboard and use the mouse to interact with the environment.As you progress you’ll encounter certain characters you can interact with. This is done via a menu that offers options as to how you can approach the conversation. For example, you can ask for HELP, HASSLE them, be PLEASANT, try to BLUFF, OFFER something or ASK ABOUT a keyword. There’s actually a HUGE amount of trial and error required here as you need to try different approaches (combinations of actions) with each character before they’ll let you ask them about anything.This isn’t immediately obvious and if you don’t approach a conversation correctly you need to exit and re-initiate or you can’t proceed. This is not intuitive and can leave you feeling like you’ve hit a dead end.Oh goody, a mazeSome game sequences involve an overhead view where you have to avoid guards as you make your way to the next room. The guards will all follow a predetermined path so once you know what this is it’s not that hard to avoid them. Unfortunately when you leave a room and go back into one of these scenes the guards may be nearby and if they spot you it’s game over, so make sure to save often.Trying to escape from the asylum will result in you getting caught … a lotTo escape the initial sequence in the asylum you also have to navigate an arbitrary maze. Thankfully there are no guards in this section, but I hate mazes and find them tedious … and this one was no different. It really feels like it’s artificially delaying your progress and is just “busy work” for the player. Thankfully this is the only maze in the game :)Travel in this game is done via a menu, and you can select to either take a train or a plane to most destinations. Taking the train will take much longer (time-wise) and since the game is on a timer this can be problematic. The endgame sequence requires you to catch a train to Paris, but if you’ve taken too long getting to the terminal the train will be gone and you have to start over (either from an old save file, or from the very beginning).Unlike Mean Streets the travel mechanics in this game are pretty straightforward, but you have limited money to move around Europe with so make sure you’re paying attention as the story unfolds and try not to backtrack too often.When you return to your apartment you’ll find a CAD system, which is sort of like a laptop. This can be used to look up information on characters you learn about, analyze inventory items or read email. You’ll only get a couple email messages throughout the game, and when you read them they’re gone … so pay close attention.Certain items (such as notes, memos or documents) can be analyzed. This is done by zooming in and out using the CAD system, which at a certain resolution will result in you being able to read some additional text on screen. Once you find the correct resolution you’ll typically be given an additional keyword you can ask characters about or a new location you can travel to.Unfortunately as new information becomes available you’re not given any audio or visual cue.The story is actually quite good, and each character you interact with drops clues as to how you ended up in the asylum, who you work for, what the mission was and ultimately what you need to do to stop the terrorist plot. Assuming you don’t miss anything by approaching a conversation incorrectly you’ll be able to piece everything together from these interactions.Not everything is obvious though … For example, in Venice after you meet with the stripper and get some additional information from her you’ll want to search her dressing room. You don’t know this is a goal, but assuming you do you’ll need to wait for the guard to leave. There are no clues that the guard even will leave, so unless you have the foresight to just stand still for 2-3 minutes until he moves … you’re stuck.This was not obvious, and from the character interactions in this area there are no clues that he might leave on his own for some reason.Move the brick next to his leg. No, not that brick, the other one … no, the other one … ARGHAs good as the story may be, this game is HARD. You can’t move the mouse around the screen and have hotspots identified - you have to select a verb and click on EVERYTHING. What’s worse is on some screens there are SO MANY dead ends that you can interact with but don’t really offer anything useful. Additionally, you’ll need to try and MOVE literally EVERYTHING.This game mechanic must be a carryover from Mean Streets - where it was also extremely frustrating. You never know when something will be hidden beneath something else.Nice kitty!There are a handful of puzzle sequences as well. Most puzzles are pretty straightforward, but you’ll likely still die a dozen times trying to get the timing “just right”. For example, when you’re captured and tied up above a tiger, once the rope burns and you fall you need to lure the tiger into a cage, then run out and close the cage. This seems easy, but trying to navigate the mouse and keyboard together fast enough will result in the tiger getting out and eating you the first few tries.If all goes well and you didn’t forget to pick up any key items, and you didn’t screw up any of the character interactions, and you didn’t run out of money, and you didn’t run out of time, and you didn’t get stuck because a puzzle was non-obvious … you’ll make it to Paris where you have 60 seconds to defuse an atomic bomb. No pressure …Assuming you’ve used the CAD to review EVERYTHING you picked up one of the notes does give you the info you need to properly defuse the bomb (or you can just use a walkthrough). Once you complete this, the game is over and you’ve saved the world. Good job!I really liked this game, but found it to be pretty challenging. You’ll die A LOT just trying to get out of the initial sequence in the asylum, and you might blow a good hour in the basement maze if you suck at mazes like I do …The underlying story is compelling and kept me interested enough to keep on plugging away to the end. If you’re going to try this out for yourself, be warned that this is definitely an adventure game from the “non-LucasArts” camp, where you can die literally everywhere, and end up in unwinnable situations.Game Information Game Countdown Developer Access Software Publisher Access Software Release Date 1990 Systems DOS Game Engine   Play Information How Long To Beat? 4 hours Version Played DOS via DOSBox-X Notes Walkthrough, Longplay ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 13 Story (25) 16 Experience (15) 10 Impact (10) 4   61% Gallery" }, { "title": "Beavis and Butt-Head in Virtual Stupidity (Viacom New Media) - 1995", "url": "/blog/2023/04/16/beavis-and-butthead/", "categories": "Let's Adventure!", "tags": "adventure, Viacom New Media, BBVS", "date": "2023-04-16 05:55:33 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Beavis and Butt-Head in Virtual Stupidity is a point-and-click adventure computer game based on ...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Beavis and Butt-Head in Virtual Stupidity is a point-and-click adventure computer game based on the American animated television series created by Mike Judge, Beavis and Butt-Head, that was developed by ICOM Simulations and published by Viacom New Media.I have fond memories of Beavis and Butthead so I expected this playthrough to be done through some rose coloured glasses as a result. To be fair, the voice acting was excellent and the plot really felt like something lifted from one of the episodes.GWAR still kicks ass! RIP Dave Brockie :(If you click on some of the TVs in the game they’ll play music videos mixed with Beavis and Butthead’s commentary. From what I could remember watching this show growing up that commentary made up the vast majority of the show and was the best part, so including it in the game was a nice bonus when you found it. I actually don’t remember there being very much “cartoon content” in each episode …As far as adventure games go this title doesn’t really add anything to the overall gaming landscape. Instead of a verb menu or action bar, when you right click on the screen you get a ring menu (similar to Full Throttle) which you use to select an action. The standard fare are here such as LOOK, MOVE, TALKand USE, as well as access to your inventory.I’m guessing the interactions in this game were heavily inspired by LucasArts, as the character interactions feel like they were lifted directly from Sam &amp; Max Hit the Road. As you navigate the game world you talk to various characters or interact with elements of the environment which will then be available as options when talking to certain characters. This is how you advance through the plot of this game.The plot is pretty straightforward, and feels perfectly aligned with an episode of Beavis and Butthead. You run into Todd, who you think is super cool because he’s always threatening you so you want to join his gang. Once you escape from your high school, you roam around Highland (Texas) trying get “in” with Todd, which results in you getting up to all sorts of mischief, including being thrown in jail.Throughout the game there are a series of mini-games that you can optionally play. The Hock-a-Loogie game is the only one you actually have to complete as hitting the principal with a super-loogie is the only way to get him to run inside, which allows you to sneak out of school and progress the game.Each mini-game can be replayed whenever you want, including from a menu present at the title screen. These mini-games are all pretty stupid and extremely repetitive, but seeing as they’re mostly optional it’s kind of nice knowing they exist as a distraction only.This wouldn’t be an adventure game if it wasn’t full of fetch quests and item combination puzzles. Make sure you try an pickup everything on every screen as the solutions to some of the puzzles in this game aren’t obvious. I chuckled a bit in the prison sequence where you had to distract an inmate by blinding him with a camera and flash, but to do this you had to find a flashcube and combine it with the camera first.I’ve got a feeling anyone born after 2000 might not know what a flashcube is …Animated cutscenes are mixed in throughout the game, which further assist with progressing the plot. The graphics of the game are descent and the downsampled graphics on the cutscenes actually make them feel more immersive with the game as the transition is more subtle.When you finally reach the endgame you’ve rescued Todd, he’s about to let you join the gang - but you’re raided by the police and accidentally finger Todd as the perpetrator and he gets hauled off to prison. Better luck next time … I guess :PThere is a very narrow audience that this game would appeal to - though I have to admit I can count myself a member of that audience. It’s unlikely you’ll try this game out if you didn’t watch the original show, though some folks might stumble into this if they enjoyed 2022’s Beavis and Butt-Head Do the Universe.I don’t think you’d consider the humour in this game “raunchy” by any means, but in the mid 90’s standards were a lot higher (or lower?). You know what you’re getting into when you pick up this title, and honestly it doesn’t really disappoint. It’s not a great game, but for a one time playthrough you’ll have a good time.Game Information Game Beavis and Butt-Head in Virtual Stupidity Developer Viacom New Media Publisher Viacom New Media Release Date August 31, 1995 Systems Windows, PlayStation (JP Only) Game Engine BBVS Play Information How Long To Beat? 4 hours Version Played Windows via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 12 Story (25) 12 Experience (15) 8 Impact (10) 4   51% Gallery" }, { "title": "Déjà Vu: A Nightmare Comes True (ICOM Simulations) - 1985", "url": "/blog/2023/04/11/deja-vu/", "categories": "Let's Adventure!", "tags": "adventure, ICOM Simulations, Mindscape, Kemco, MacVenture", "date": "2023-04-11 12:11:59 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Déjà Vu is a point-and-click adventure game set in the world of 1940s hardboiled detective novel...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Déjà Vu is a point-and-click adventure game set in the world of 1940s hardboiled detective novels and movies. It was released in 1985 for Macintosh – the first in the MacVenture series – and later ported to several other systems.The game takes place in Chicago during December 1941. The game character is Ace Harding, a retired boxer working as a private eye. Ace awakes one morning in a bathroom stall, unable to remember who he is. A dead man is found in an upstairs office, and Ace is about to be framed for the murder. There are some clues as to the identity of the murdered man and to Ace himself. A strap-down chair, mysterious vials, and a syringe are found, suggesting (together with a needle mark on Ace’s arm) that an interrogation has taken place.As this is a MacVentures game the interface is laid out in a familiar fashion. The game world occupies a third of the screen, which you engage with by selecting a verb from the bottom section of the screen then clicking on. Having been designed with a mouse in mind, the NES port (which I’m playing) is a bit cumbersome to navigate, but as this was my first exposure to these titles I’m willing to be forgiving of these shortcomings.Your goal is to figure out who you are and to clear your name once you realize you’re being framed for murder. This is done by collecting and analyzing evidence on every screen. Be warned - there is A LOT you can pick up and it’s not all useful.This is done by first selecting a verb (EXAM, SPEAK, HIT, CLOSE, OPEN, TAKE, LEAVE, USE), then moving the cursor over the target you want to interact with. There is no visual indication that what you’re trying to interact with will be valid, so there’s a lot of trial and error as you move through the game.Remember that random pencil you found … I hope you remembered to TAKE it with you …Inventory management is a huge part of this game as the goal is to find clues not only to your identity but eventually to clear your name as you’ve been framed for murder. Items can be nested 2 or 3 levels deep, so you’ll need to make sure you try and OPEN everything. Additionally there are item combination puzzles so you’ll want to make sure to try using items with each other.The game world is split between multiple locations that you need to navigate between by taking a cab. Each cab ride costs $0.75, but you have a limited number of coins. In the basement of Joe’s Bar there’s a casino where you can play the slot machines to try and earn some extra money. If you gamble away all your coins you’ll end up in an unwinnable situation … so save early and save often ;)Make sure you save your game before playing the slot machines … just in caseAs you progress through the story you’ll realize you’ve been drugged, which is affecting your memory. Once you identify the what you’ve been given and what the remedy is, figure out how to administer this to yourself and do it without accidentally dying … you learn who you are, but this doesn’t really clear up what you need to do to proceed through to the end of the game.I actually found this part a bit frustrating as I didn’t originally find the capsules and couldn’t take the medicine that’s supposed to restore your memory. Once I got a walkthrough and backtracked I was able to sort this out, but it sort of felt like a forced fetch quest - which I guess is pretty common of this genre of game.In the deepest parts of the sewer there’s an area where you can discard evidence permanently. This is how you win the game as you need to take the evidence you find that frames you for murder and dispose of it. If you don’t do this JUST RIGHT, when you go to the police station at the end of the game you’ll wind up just being arrested instead of cleared of all charges.You can’t just randomly drop evidence either; it needs to be discarded in exactly this spot. This isn’t really clear while you’re playing the game, and I didn’t find any clues that indicated the sewer was where you needed to do this so I’m guessing it was meant to be a “trial and error” type of discovery for players. I didn’t love this as it’s not very intuitive, and really the game doesn’t make it apparent how you’re supposed to “win”.Make sure you save often as you play as there are multiple areas where you can be randomly killed. In the sewers you may run into a crocodile that will eat you and there are muggers randomly trying to rob and kill you. You can knock the mugger out a couple times using HIT, but after that you have to give him cash or he’ll shoot you.You’ll likely also die a few times by taking the wrong medicine trying to clear up your lost memory, or just using the wrong items on yourself trying to figure out what is useful. This is part of the fun of adventure games though so roll the dice and see what does and doesn’t kill you :DOverall this game has a decent story, and the NES port has pretty good music. I never tried playing any of the other ports so I’m not sure how they fare, but aside from the DOS version with the gross colour palette they seem to look ok as well.Seeing as this was their first game I can forgive some of the technical and procedural challenges I had finishing it. It is kind of weird knowing that this is the same company that ended up making licensed Beavis and Butthead games in the ’90s like Beavis and Butt-Head in Virtual Stupidity.I’m looking forward to playing the sequel to see where this story goes.Game Information Game Déjà Vu: A Nightmare Comes True Developer ICOM Simulations Publisher Mindscape, Kemco Release Date 1985 Systems Apple IIGS, Macintosh, Atari ST, Commodore 64,Amiga, DOS, Game Boy Color, Windows, NES Game Engine MacVenture Play Information How Long To Beat? 6 hours Version Played NES via BizHawk Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 11 Story (25) 15 Experience (15) 6 Impact (10) 3   50% Gallery" }, { "title": "I Have No Mouth, and I Must Scream (The Dreamers Guild) - 1995", "url": "/blog/2023/03/30/i-have-no-mouth-and-i-must-scream/", "categories": "Let's Adventure!", "tags": "adventure, The Dreamers Guild, Cyberdreams, SAGA", "date": "2023-03-30 06:41:33 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.I Have No Mouth, and I Must Scream is a 1995 point-and-click adventure game developed by Cyberdr...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.I Have No Mouth, and I Must Scream is a 1995 point-and-click adventure game developed by Cyberdreams and The Dreamers Guild, co-designed by Harlan Ellison, published by Cyberdreams and distributed by MGM Interactive. The game is based on Ellison’s short story of the same title. It takes place in a dystopian world where a mastermind artificial intelligence named “AM” has destroyed all of humanity except for five people, whom he has been keeping alive and torturing for the past 109 years by constructing metaphorical adventures based on each character’s fatal flaws. The player interacts with the game by making decisions through ethical dilemmas that deal with issues such as insanity, rape, paranoia, and genocide.The premise of the game is that the three superpowers, Russia, China, and the United States, have each secretly constructed a vast subterranean complex of computers to wage a global war too complex for human brains to oversee. One day, the American supercomputer, better known as the Allied Mastercomputer, gains sentience and absorbs the Russian and Chinese supercomputers into itself and redefines itself as simply AM (“I think, therefore I am”). Due to its immense hatred for humanity, stemming from the logistical limits set onto him by programmers, AM uses its abilities to kill off the population of the world. However, AM refrains from killing five people (four men and one woman) in order to bring them to the center of the earth and torture them. With the aid of research carried out by one of the five remaining humans, AM is able to extend their lifespans indefinitely as well as alter their bodies and minds to his liking.You take control of the 5 characters in any order you like and play through their scenario in a typical point-and-click adventure fashion. The interface provides you with a verb list and inventory view, as well as your character’s “health” (depicted by their headshot and a coloured background). I didn’t really understand the health mechanic as I played through this, even with a walkthrough, but apparently keeping your character healthy (I think “health” had a different name) actually affects your ability to get the best ending.The goal is to overcome the character’s issue that they’re struggling with in order to escape the scenario and end up back in their torture chamber.The game’s interface is intuitive if you’ve ever played an adventure game. You have the option to interact with NPCs or machines, which is controlled via a typical menu-driven dialogue tree. Some interaction options aren’t revealed until you’ve solved some puzzle, located an item or interacted with another character in some other fashion, but this is all standard adventure game fare.Though there are a number of fetch quests throughout the scenarios, they actually don’t feel out of place or like “busy work”. The solution to each puzzle isn’t overly complicated to identify if you read everything and interact with everything on each screen. Seeing as this game is an adaption of a short story there’s quite a bit to read throughout the game that helps advance the plot.What really sets this game apart is the story. There is great effort put into developing each of the five characters, and by the end of the game you feel you’ve helped them overcome their fears and insecurities as they confront the ghosts of their pasts. Harlan Ellison collaborated with the game’s writer to adapt his story to this format, and even leant his voice talents to the game by voicing AM.Though his delivery is a little over the top, it’s not out of place. The game is fully voice acted, and each of the characters are well done. Their delivery helps keep you immersed in their scenarios and doesn’t detract from the overall game play.My favourite scenario in this game was Nimdok’s. Nimdok is an ex-Nazi doctor that is sent back to the camps to resume human experimentation. The goal is to find “the lost tribe”, which he actually winds up being a member of. As he slowly regains some memories of his past it turns out he turned in his Jewish parents to the regime, and helped develop the tech that AM uses to prolong the lives of the 5 captive humans. Nimdok redeems himself by helping the Jewish captives of the camp escape, and gives them control of a large golem, which they use to kill him for his past crimes.The sound effects and background music feel appropriate and maintain the dark and brooding atmosphere. Most of the scenes are very dark and depressing, which you’d expect given the source material.After all five humans have overcome their fatal flaws, they meet again in their respective torture cells while AM retreats within himself, pondering what went wrong. With the help of the Russian and Chinese supercomputers, you select one of the humans to be translated into binary and enter the world of AM’s mind. Each of the 5 characters needs to navigate this landscape and activate a pillar and confront the Id, Ego and Superego.I didn’t get the best ending …Apparently how you complete the character’s scenarios actually plays into the ending here, and you can find yourself missing key items if you didn’t finish with a high enough “health” level (or something to that effect). I started the game with the Gorrister scenario, and when I made it to the end I was missing the heart that I needed to activate his pillar.You can still win the game if you’re missing these items, but you won’t get the best ending … which I didn’t.Overall I had a good time with this game, and plan on reading the short story in the near future. I really enjoy the dark psychological introspective stories such as this so the subject matter resonated with me personally.As far as adventure games go I Have No Mouth, and I Must Scream doesn’t really introduce anything unique or innovative to the genre, but still makes for a good experience. The strong story is what sets this apart, and though I highly doubt I’d ever play this again I very much enjoyed my time with this game.Game Information Game I Have No Mouth, and I Must Scream Developer The Dreamers Guild Publisher Cyberdreams Release Date October 31, 1995 Systems Mac OS, DOS Game Engine SAGA Play Information How Long To Beat? 6 hours Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 15 Story (25) 21 Experience (15) 11 Impact (10) 4   73% Gallery" }, { "title": "Comparing MongoDB to not-MongoDB (ex: DocumentDB or Cosmos DB)", "url": "/blog/2023/03/29/comparing-mongodb-to-not-mongodb-ex-documentdb-or-cosmos-db/", "categories": "MongoDB", "tags": "mongodb, documentdb, cosmosdb, drivers", "date": "2023-03-29 13:12:37 -0400", "snippet": "Let’s start off first with what is MongoDB? MongoDB’s document model is simple for developers to learn and use, while still providing all the capabilities needed to meet the most complex requiremen...", "content": "Let’s start off first with what is MongoDB? MongoDB’s document model is simple for developers to learn and use, while still providing all the capabilities needed to meet the most complex requirements at any scale. To enable developers to quickly integrate their data with their applications, drivers for 10+ languages as well dozens more community supported libraries exist that expose idiomatic interfaces and intuitive APIs.The ease of use of the drivers and the low barrier to entry to working with your data programmatically has been a huge draw for developers coming to MongoDB, however there are now choices in the market as big names such as Microsoft and Amazon release similar offerings that expose a MongoDB “compatibility layer” atop cloud database products.According to the product blurb for Amazon DocumentDB (with MongoDB compatibility), Amazon DocumentDB (with MongoDB compatibility) is a fully managed native JSON document database that makes it easy and cost effective to operate critical document workloads at virtually any scale without managing infrastructure. Amazon DocumentDB simplifies your architecture by providing built-in security best practices, continuous backups, and native integrations with other AWS services.Note that the branding for DocumentDB includes “with MongoDB compatibility” right in the product name, and in a similar fashion Azure’s Cosmos DB provides a variation called Azure Cosmos DB for MongoDB. MongoDB Atlas, which is MongoDB’s cloud database service can make the same claims Amazon and Microsoft make, however with the main differentiating factor being that it offers the MongoDB Server natively - not through some abstraction layer.The major area of focus for developers is on building software using MongoDB’s drivers, so we’ll be focusing this article on the availability of MongoDB’s database commands across both Amazon Document DB as well as Azure Cosmos DB. The drivers use these commands internally to deliver consistent API experiences (based on published specifications such as the Driver CRUD API) and inconsistencies in implementation or outright omissions could effect an application using MongoDB’s drivers as a result.AWS DocumentDBAs of 2022-08-22, Amazon DocumentDB is 35.31% compatible. This number is being established based on the result of compatibility tests that can be run by anyone to validate the results, but what does compatibility really mean here to the end user?According to the MongoDB Software Lifecycle Schedules the versions of MongoDB that DocumentDB lists as supported reached end of life in 2021 and 2022. At the time of writing MongoDB 6.0 is the latest version available, with 7.0 scheduled for sometime during the second half of 2023. As such, it’s important to check your selected driver’s compatibility to ensure you’re potentially using features that won’t be supported or exist (for example, see the Ruby Driver Compatibility documentation).Minor frustration - provisioning either an elastic or instance based DocumentDB cluster takes 10+ minutes. I also found the network restrictions made it extremely difficult to connect to the cluster even after downloading the PEM file for the security group. To interact with the cluster in any meaningful way I setup an Amazon Linux EC2 container, which I then installed the MongoDB Shell on.To do this, connect to the container, get a download url for mongosh for “RHEL / CentOS / Fedora / Suse 64-bit” and install that in the container to connect to the cluster. , #_ ~\\_ ####_ Amazon Linux 2023 ~~ \\_#####\\ ~~ \\###| ~~ \\#/ ___ https://aws.amazon.com/linux/amazon-linux-2023 ~~ V~' '-&gt; ~~~ / ~~._. _/ _/ _/ _/m/'[ec2-user@ip-172-31-7-1 ~]$ wget https://downloads.mongodb.com/compass/mongodb-mongosh-1.8.0.x86_64.rpm[ec2-user@ip-172-31-7-1 ~]$ sudo rpm -i mongodb-mongosh-1.8.0.x86_64.rpm[ec2-user@ip-172-31-7-1 ~]$ mongosh mongodb://&lt;user&gt;:&lt;pass&gt;@ab-elastic-docdb-051834932553.us-east-1.docdb-elastic.amazonaws.com:27017 -sslWARNING: argument --ssl is deprecated and will be removed. Use --tls instead.Current Mongosh Log ID: 64247c7a6aaa0a9d29f5ada3Connecting to: mongodb://&lt;credentials&gt;@ab-elastic-docdb-051834932553.us-east-1.docdb-elastic.amazonaws.com:27017/?directConnection=true&amp;tls=true&amp;appName=mongosh+1.8.0Using MongoDB: 5.0.0Using Mongosh: 1.8.0[direct: mongos] test&gt;The two cluster types you can create effectively map to either a replica set or sharded cluster. To see what commands are supported by each cluster type, from a mongosh session we run the following script that uses the listCommands command generate the list:JSON.stringify(Object.keys(db.runCommand({ listCommands: 1 })[\"commands\"]).sort())// instance based (commands: 69)[\"abortTransaction\",\"aggregate\",\"authenticate\",\"buildInfo\",\"collMod\",\"collStats\",\"commitTransaction\",\"connectionStatus\",\"count\",\"create\",\"createIndexes\",\"createRole\",\"createUser\",\"currentOp\",\"dataSize\",\"dbStats\",\"delete\",\"deleteIndexes\",\"distinct\",\"driverOIDTest\",\"drop\",\"dropAllRolesFromDatabase\",\"dropAllUsersFromDatabase\",\"dropDatabase\",\"dropIndexes\",\"dropRole\",\"dropUser\",\"explain\",\"find\",\"findAndModify\",\"forceerror\",\"getLastError\",\"getMaxChangeStreamTimestamp\",\"getMore\",\"getnonce\",\"grantPrivilegesToRole\",\"grantRolesToRole\",\"grantRolesToUser\",\"hello\",\"hostInfo\",\"insert\",\"isMaster\",\"killAllSessions\",\"killCursors\",\"killOp\",\"killSessions\",\"listCollections\",\"listCommands\",\"listCursors\",\"listDatabases\",\"listIndexes\",\"logout\",\"modifyChangeStreams\",\"ping\",\"replSetGetConfig\",\"replSetGetStatus\",\"revokePrivilegesFromRole\",\"revokeRolesFromRole\",\"revokeRolesFromUser\",\"rolesInfo\",\"saslContinue\",\"saslStart\",\"serverStatus\",\"top\",\"update\",\"updateRole\",\"updateUser\",\"usersInfo\",\"whatsmyuri\"]// elastic (commands: 50)[\"aggregate\",\"buildinfo\",\"collStats\",\"count\",\"create\",\"createIndexes\",\"createUser\",\"currentOp\",\"dataSize\",\"dbStats\",\"delete\",\"deleteIndexes\",\"distinct\",\"drop\",\"dropAllUsersFromDatabase\",\"dropDatabase\",\"dropIndexes\",\"dropUser\",\"enablesharding\",\"endsessions\",\"find\",\"findandmodify\",\"getMore\",\"getcmdlineopts\",\"getengineversion\",\"getfreemonitoringstatus\",\"getlasterror\",\"getlog\",\"grantRolesToUser\",\"hello\",\"insert\",\"isMaster\",\"killCursors\",\"killOp\",\"listCollections\",\"listCommands\",\"listDatabases\",\"listIndexes\",\"logout\",\"ping\",\"replsetgetstatus\",\"revokeRolesFromUser\",\"rolesInfo\",\"saslstart\",\"serverStatus\",\"shardcollection\",\"update\",\"updateUser\",\"usersInfo\",\"whatsmyuri\"]For sake of comparison I setup a sharded cluster using MongoDB 5.0.15 and using mongosh ran the following command against both a mongos as well as a mongod:JSON.stringify(Object.keys(db.runCommand({ listCommands: 1 })[\"commands\"]).filter(item =&gt; !item.startsWith(\"_\")).sort())// mongod (commands: 152, unfiltered: 224)[\"abortTransaction\",\"aggregate\",\"appendOplogNote\",\"applyOps\",\"authenticate\",\"autoSplitVector\",\"availableQueryOptions\",\"buildInfo\",\"checkShardingIndex\",\"cleanupOrphaned\",\"cloneCollectionAsCapped\",\"collMod\",\"collStats\",\"commitTransaction\",\"compact\",\"connPoolStats\",\"connPoolSync\",\"connectionStatus\",\"convertToCapped\",\"coordinateCommitTransaction\",\"count\",\"create\",\"createIndexes\",\"createRole\",\"createUser\",\"currentOp\",\"dataSize\",\"dbCheck\",\"dbHash\",\"dbStats\",\"delete\",\"distinct\",\"donorAbortMigration\",\"donorForgetMigration\",\"donorStartMigration\",\"driverOIDTest\",\"drop\",\"dropAllRolesFromDatabase\",\"dropAllUsersFromDatabase\",\"dropConnections\",\"dropDatabase\",\"dropIndexes\",\"dropRole\",\"dropUser\",\"endSessions\",\"explain\",\"exportCollection\",\"features\",\"filemd5\",\"find\",\"findAndModify\",\"flushRouterConfig\",\"fsync\",\"fsyncUnlock\",\"getAuditConfig\",\"getCmdLineOpts\",\"getDatabaseVersion\",\"getDefaultRWConcern\",\"getDiagnosticData\",\"getFreeMonitoringStatus\",\"getLastError\",\"getLog\",\"getMore\",\"getParameter\",\"getShardMap\",\"getShardVersion\",\"getnonce\",\"grantPrivilegesToRole\",\"grantRolesToRole\",\"grantRolesToUser\",\"hello\",\"hostInfo\",\"importCollection\",\"insert\",\"internalRenameIfOptionsAndIndexesMatch\",\"invalidateUserCache\",\"isMaster\",\"killAllSessions\",\"killAllSessionsByPattern\",\"killCursors\",\"killOp\",\"killSessions\",\"listCollections\",\"listCommands\",\"listDatabases\",\"listIndexes\",\"lockInfo\",\"logApplicationMessage\",\"logRotate\",\"logout\",\"mapReduce\",\"mergeChunks\",\"moveChunk\",\"ping\",\"planCacheClear\",\"planCacheClearFilters\",\"planCacheListFilters\",\"planCacheSetFilter\",\"prepareTransaction\",\"profile\",\"reIndex\",\"recipientForgetMigration\",\"recipientSyncData\",\"refreshSessions\",\"renameCollection\",\"replSetAbortPrimaryCatchUp\",\"replSetFreeze\",\"replSetGetConfig\",\"replSetGetRBID\",\"replSetGetStatus\",\"replSetHeartbeat\",\"replSetInitiate\",\"replSetMaintenance\",\"replSetReconfig\",\"replSetRequestVotes\",\"replSetResizeOplog\",\"replSetStepDown\",\"replSetStepUp\",\"replSetSyncFrom\",\"replSetUpdatePosition\",\"revokePrivilegesFromRole\",\"revokeRolesFromRole\",\"revokeRolesFromUser\",\"rolesInfo\",\"rotateCertificates\",\"saslContinue\",\"saslStart\",\"serverStatus\",\"setAuditConfig\",\"setDefaultRWConcern\",\"setFeatureCompatibilityVersion\",\"setIndexCommitQuorum\",\"setParameter\",\"setShardVersion\",\"shardingState\",\"shutdown\",\"splitChunk\",\"splitVector\",\"startRecordingTraffic\",\"startSession\",\"stopRecordingTraffic\",\"top\",\"update\",\"updateRole\",\"updateUser\",\"usersInfo\",\"validate\",\"validateDBMetadata\",\"voteCommitImportCollection\",\"voteCommitIndexBuild\",\"waitForFailPoint\",\"whatsmyuri\"]// mongos (commands: 133, unfiltered: 137)[\"abortReshardCollection\",\"abortTransaction\",\"addShard\",\"addShardToZone\",\"aggregate\",\"appendOplogNote\",\"authenticate\",\"availableQueryOptions\",\"balancerCollectionStatus\",\"balancerStart\",\"balancerStatus\",\"balancerStop\",\"buildInfo\",\"cleanupReshardCollection\",\"clearJumboFlag\",\"collMod\",\"collStats\",\"commitReshardCollection\",\"commitTransaction\",\"compact\",\"connPoolStats\",\"connPoolSync\",\"connectionStatus\",\"convertToCapped\",\"count\",\"create\",\"createIndexes\",\"createRole\",\"createUser\",\"currentOp\",\"dataSize\",\"dbStats\",\"delete\",\"distinct\",\"drop\",\"dropAllRolesFromDatabase\",\"dropAllUsersFromDatabase\",\"dropConnections\",\"dropDatabase\",\"dropIndexes\",\"dropRole\",\"dropUser\",\"enableSharding\",\"endSessions\",\"explain\",\"features\",\"filemd5\",\"find\",\"findAndModify\",\"flushRouterConfig\",\"fsync\",\"getAuditConfig\",\"getCmdLineOpts\",\"getDefaultRWConcern\",\"getDiagnosticData\",\"getLastError\",\"getLog\",\"getMore\",\"getParameter\",\"getShardMap\",\"getShardVersion\",\"getnonce\",\"grantPrivilegesToRole\",\"grantRolesToRole\",\"grantRolesToUser\",\"hello\",\"hostInfo\",\"insert\",\"invalidateUserCache\",\"isMaster\",\"isdbgrid\",\"killAllSessions\",\"killAllSessionsByPattern\",\"killCursors\",\"killOp\",\"killSessions\",\"listCollections\",\"listCommands\",\"listDatabases\",\"listIndexes\",\"listShards\",\"logApplicationMessage\",\"logRotate\",\"logout\",\"mapReduce\",\"mergeChunks\",\"moveChunk\",\"movePrimary\",\"netstat\",\"ping\",\"planCacheClear\",\"planCacheClearFilters\",\"planCacheListFilters\",\"planCacheSetFilter\",\"profile\",\"refineCollectionShardKey\",\"refreshSessions\",\"removeShard\",\"removeShardFromZone\",\"renameCollection\",\"repairShardedCollectionChunksHistory\",\"replSetGetStatus\",\"reshardCollection\",\"revokePrivilegesFromRole\",\"revokeRolesFromRole\",\"revokeRolesFromUser\",\"rolesInfo\",\"rotateCertificates\",\"saslContinue\",\"saslStart\",\"serverStatus\",\"setAllowMigrations\",\"setAuditConfig\",\"setDefaultRWConcern\",\"setFeatureCompatibilityVersion\",\"setIndexCommitQuorum\",\"setParameter\",\"shardCollection\",\"shutdown\",\"split\",\"splitVector\",\"startRecordingTraffic\",\"startSession\",\"stopRecordingTraffic\",\"update\",\"updateRole\",\"updateUser\",\"updateZoneKeyRange\",\"usersInfo\",\"validate\",\"validateDBMetadata\",\"waitForFailPoint\",\"whatsmyuri\"]The above output filters internal commands (commands prefixed with an underscore), however these are visible to the listCommands command. As Amazon controls the output of the commands and the source code for this compatibility layer is not public we cannot confirm or deny if any additional commands exist beyond what is listed.Azure Cosmos DB“What is Azure Cosmos DB for MongoDB” you might be asking. From their documentation, Cosmos DB for MongoDB implements the wire protocol for MongoDB. This implementation allows transparent compatibility with MongoDB client SDKs, drivers, and tools. Azure Cosmos DB doesn’t host the MongoDB database engine. Any MongoDB client driver compatible with the API version you’re using should be able to connect, with no special configuration.What’s notable about the above claim is “should be able to connect”. This type of hedging language is typical of marketing copy that doesn’t want to provide guarantees, but focus on a desired outcome without drawing attention to any of the underlying negative connotations of the statement.For example, the hello command was introduced in MongoDB 5.0, but backported to versions 4.4.2, 4.2.10, 4.0.21, and 3.6.21. Cosmos DB currently doesn’t offer 5.0 API compatibility (except in limited preview), given the backport list I’d expect using this command to work. I don’t see it listed in the “Azure Cosmos DB for MongoDB (4.2 server version): supported features and syntax” documentation, however as it’s an internal command (used for initial connections or monitoring) it might have been excluded on purpose.If you try and connect to your Cosmos DB cluster using a connection string the includes a Stable API version, the lack of the hello command will cause the connection to outright fail.mongosh ab-cosmosdb1.mongo.cosmos.azure.com:10255 -u ab-cosmosdb1 -p &lt;redacted&gt; --ssl --sslAllowInvalidCertificates --apiVersion 1WARNING: argument --ssl is deprecated and will be removed. Use --tls instead.WARNING: argument --sslAllowInvalidCertificates is deprecated and will be removed. Use --tlsAllowInvalidCertificates instead.Current Mongosh Log ID: 642472c47f4e6cdad20312b0Connecting to: mongodb://&lt;credentials&gt;@ab-cosmosdb1.mongo.cosmos.azure.com:10255/?directConnection=true&amp;tls=true&amp;tlsAllowInvalidCertificates=true&amp;appName=mongosh+1.6.0MongoServerSelectionError: Command hello not supported prior to authentication.Similar to what we did with DocumentDB, let’s try and identify what commands exist in Cosmos DB. Unfortunately, similar to the hello command it appears listCommands is also not supported:globaldb [direct: primary] test&gt; db.runCommand({ listCommands: 1 })MongoServerError: Command listCommands not supported.Since we generated the command list from a mongod process earlier, let’s assign those results to a variable below and try executing each of the 152 command with a null value to see how they fail. This can be used to build out a (somewhat) representative view of what commands are available.// commands from 5.0.15 mongodvar commands = [\"abortTransaction\",\"aggregate\",\"appendOplogNote\",\"applyOps\",\"authenticate\",\"autoSplitVector\",\"availableQueryOptions\",\"buildInfo\",\"checkShardingIndex\",\"cleanupOrphaned\",\"cloneCollectionAsCapped\",\"collMod\",\"collStats\",\"commitTransaction\",\"compact\",\"connPoolStats\",\"connPoolSync\",\"connectionStatus\",\"convertToCapped\",\"coordinateCommitTransaction\",\"count\",\"create\",\"createIndexes\",\"createRole\",\"createUser\",\"currentOp\",\"dataSize\",\"dbCheck\",\"dbHash\",\"dbStats\",\"delete\",\"distinct\",\"donorAbortMigration\",\"donorForgetMigration\",\"donorStartMigration\",\"driverOIDTest\",\"drop\",\"dropAllRolesFromDatabase\",\"dropAllUsersFromDatabase\",\"dropConnections\",\"dropDatabase\",\"dropIndexes\",\"dropRole\",\"dropUser\",\"endSessions\",\"explain\",\"exportCollection\",\"features\",\"filemd5\",\"find\",\"findAndModify\",\"flushRouterConfig\",\"fsync\",\"fsyncUnlock\",\"getAuditConfig\",\"getCmdLineOpts\",\"getDatabaseVersion\",\"getDefaultRWConcern\",\"getDiagnosticData\",\"getFreeMonitoringStatus\",\"getLastError\",\"getLog\",\"getMore\",\"getParameter\",\"getShardMap\",\"getShardVersion\",\"getnonce\",\"grantPrivilegesToRole\",\"grantRolesToRole\",\"grantRolesToUser\",\"hello\",\"hostInfo\",\"importCollection\",\"insert\",\"internalRenameIfOptionsAndIndexesMatch\",\"invalidateUserCache\",\"isMaster\",\"killAllSessions\",\"killAllSessionsByPattern\",\"killCursors\",\"killOp\",\"killSessions\",\"listCollections\",\"listCommands\",\"listDatabases\",\"listIndexes\",\"lockInfo\",\"logApplicationMessage\",\"logRotate\",\"logout\",\"mapReduce\",\"mergeChunks\",\"moveChunk\",\"ping\",\"planCacheClear\",\"planCacheClearFilters\",\"planCacheListFilters\",\"planCacheSetFilter\",\"prepareTransaction\",\"profile\",\"reIndex\",\"recipientForgetMigration\",\"recipientSyncData\",\"refreshSessions\",\"renameCollection\",\"replSetAbortPrimaryCatchUp\",\"replSetFreeze\",\"replSetGetConfig\",\"replSetGetRBID\",\"replSetGetStatus\",\"replSetHeartbeat\",\"replSetInitiate\",\"replSetMaintenance\",\"replSetReconfig\",\"replSetRequestVotes\",\"replSetResizeOplog\",\"replSetStepDown\",\"replSetStepUp\",\"replSetSyncFrom\",\"replSetUpdatePosition\",\"revokePrivilegesFromRole\",\"revokeRolesFromRole\",\"revokeRolesFromUser\",\"rolesInfo\",\"rotateCertificates\",\"saslContinue\",\"saslStart\",\"serverStatus\",\"setAuditConfig\",\"setDefaultRWConcern\",\"setFeatureCompatibilityVersion\",\"setIndexCommitQuorum\",\"setParameter\",\"setShardVersion\",\"shardingState\",\"shutdown\",\"splitChunk\",\"splitVector\",\"startRecordingTraffic\",\"startSession\",\"stopRecordingTraffic\",\"top\",\"update\",\"updateRole\",\"updateUser\",\"usersInfo\",\"validate\",\"validateDBMetadata\",\"voteCommitImportCollection\",\"voteCommitIndexBuild\",\"waitForFailPoint\",\"whatsmyuri\"];var supported = [];commands.forEach(function(cmd) { try { /* command doesn't fail when null is passed */ if (db.runCommand({ [cmd]: null }).ok == 1) { supported.push(cmd); } } catch(ex) { /* 59: command not found */ if (ex.code != 59 &amp;&amp; !ex.message.endsWith(\"not supported.\")) { supported.push(cmd); } }});JSON.stringify(supported);// commands: 47[\"abortTransaction\",\"aggregate\",\"authenticate\",\"buildInfo\",\"collStats\",\"commitTransaction\",\"connectionStatus\",\"count\",\"create\",\"createIndexes\",\"currentOp\",\"dbStats\",\"delete\",\"distinct\",\"drop\",\"dropDatabase\",\"dropIndexes\",\"endSessions\",\"explain\",\"filemd5\",\"find\",\"findAndModify\",\"getCmdLineOpts\",\"getLastError\",\"getLog\",\"getMore\",\"getParameter\",\"getnonce\",\"hello\",\"hostInfo\",\"insert\",\"isMaster\",\"killCursors\",\"listCollections\",\"listDatabases\",\"listIndexes\",\"logout\",\"ping\",\"reIndex\",\"renameCollection\",\"replSetGetStatus\",\"saslContinue\",\"saslStart\",\"serverStatus\",\"update\",\"validate\",\"whatsmyuri\"]ConclusionBy reviewing the database commands that Microsoft and Amazon have chosen to implement we can get a sense of what features may be accessible via MongoDB drivers. Though the bare minimum commands required to satisfy the CRUD API appear to be available, the feature set is still limited when it comes to administration and data manipulation.Azure documents which aggregation stages aren’t available, and AWS also documents the limitations of their MongoDB APIs and operation types.Though there are definitely benefits to working with AWS and Azure, when it comes to MongoDB compatibility it is likely a better bet to simply use MongoDB." }, { "title": "Geisha (Coktel Vision) - 1990", "url": "/blog/2023/03/23/geisha/", "categories": "Let's Adventure!", "tags": "adventure, Coktel Vision, Tomahawk, Gob", "date": "2023-03-23 04:19:14 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here. This game is terrible. Also the screenshots contain nudity … which doesn’t change the fact tha...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here. This game is terrible. Also the screenshots contain nudity … which doesn’t change the fact that this game sucks!Geisha is an erotic adventure video game developed by Coktel Vision and MDO and published by Tomahawk in 1990 for Amiga, Atari ST, and DOS. The game uses a point-and-click interface, and includes several minigames including a card game and an action sequence.You read that right … another “erotic adventure” from the makers of Emmanuelle, the previous pile of shit from Coktel Vision and Tomahawk.It takes like 45 seconds to draw this guy’s stupid face one line at a time!The story makes no sense, but from what I was able to interpret, something something something girl and robot body, something about an escape …. dragons? I really tried to be impartial and give this a chance, but I have no idea what I was supposed to be doing at any point other than play minigames - all of which were boring or stupid.To advance you have to play strip-“guess the numbers in the right order”Unlike the Leisure Suit Larry games which you could also potentially classify as “erotic”, Geisha just sort of forces the nudity down your throat. The mini games are unnecessarily hard (not challenging - just hard), and even though there’s only like 7 games you play to get to the end, I actually gave up after going through the maze and sliding piece puzzle like a dozen times.Fuck this stupid puzzleAccording to the longplay video on YouTube, this game only takes about 30 minutes to complete. I’ll be honest, even playing through this with a walkthrough it felt SO MUCH LONGER! My goal with this series is to FINISH every one of these games but this stupid sliding puzzle has to be completed in 45 seconds or you DIE. You can’t save before the puzzle, so to retry you have to finish the previous maze puzzle again.Fly your dick ship around and shoot stuffThe maze is pretty straightforward, but it was frustrating enough initially to get me to look for a map. This also lead me to DrMcCoy’s blog post from the time he was working on the Gob engine which I was really hoping would give me debug codes I could use to skip the stupid slide puzzle. No such luck :/Even though I played the Atari ST version of the game, visually the game isn’t the worst. That’s probably the only nice thing I’m going to say because the sound effects are jarring, there’s really no music, none of the games are fun, progressing isn’t intuitive and overall the experience just plain sucks.DON’T PLAY THIS “GAME”.Game Over … thank god!Game Information Game Geisha Developer Coktel Vision Publisher Tomahawk, Coktel Vision Release Date 1990 Systems DOS, Amiga, Atari ST Game Engine Gob Play Information How Long To Beat? 30 minutes? Version Played Atari ST via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 3 Story (25) 2 Experience (15) 2 Impact (10) 1   11% Gallery" }, { "title": "What documents did my TTL index delete?", "url": "/blog/2023/03/14/what-documents-did-my-ttl-index-delete/", "categories": "MongoDB", "tags": "indexing, ttl, mongodb", "date": "2023-03-14 07:32:05 -0400", "snippet": "IntroductionHave you ever had one or more documents removed from a MongoDB collection and you weren’t sure what they were? This can potentially occur if you have a TTL index defined, however the ap...", "content": "IntroductionHave you ever had one or more documents removed from a MongoDB collection and you weren’t sure what they were? This can potentially occur if you have a TTL index defined, however the approach being shared below can be adapted to filter the replica set oplog for deletes over any custom time frame you may be interested in. Once the _id values of the deleted documents have been identified, the replica set oplog can be further filtered to attempt to surface details about the detailed documents.OverviewTTL indexes are special single-field indexes that MongoDB can use to automatically remove documents from a collection after a certain amount of time or at a specific clock time. Data expiration is useful for certain types of information like machine generated event data, logs, and session information that only need to persist in a database for a finite amount of time.The actual removal of documents is handled by a separate thread within the mongod process called the TTLMonitor (enabled by default via ttlMonitorEnabled), which will wake up and check all TTL indexes for expired documents every 60 seconds (the default value of ttlMonitorSleepSecs).By default the mongod logs won’t record any information about TTL index activity unless the operation happens to exceed the slow query threshold (default of 100ms - may be different in MongoDB Atlas). For the purposes of this article we want to see more information regarding TTL index activity so we’ll begin by increasing the log verbosity for the index component:db.setLogLevel(1, 'index'); The setLogLevel helper calls the setParameter command, which is unsupported in MongoDB AtlasAs we continue to monitor the mongod log we’ll begin to see messages such as the following begin to be recorded:{\"t\":{\"$date\":\"2023-03-14T06:41:26.314-04:00\"},\"s\":\"D1\",\"c\":\"INDEX\",\"id\":22533,\"ctx\":\"TTLMonitor\",\"msg\":\"running TTL job for index\",\"attr\":{\"namespace\":\"config.tenantMigrationRecipients\",\"key\":{\"expireAt\":1},\"name\":\"TenantMigrationRecipientTTLIndex\"}}{\"t\":{\"$date\":\"2023-03-14T06:41:26.314-04:00\"},\"s\":\"I\",\"c\":\"INDEX\",\"id\":5479200,\"ctx\":\"TTLMonitor\",\"msg\":\"Deleted expired documents using index\",\"attr\":{\"namespace\":\"config.tenantMigrationRecipients\",\"index\":\"TenantMigrationRecipientTTLIndex\",\"numDeleted\":0,\"durationMillis\":0}}{\"t\":{\"$date\":\"2023-03-14T06:41:26.314-04:00\"},\"s\":\"D1\",\"c\":\"INDEX\",\"id\":22533,\"ctx\":\"TTLMonitor\",\"msg\":\"running TTL job for index\",\"attr\":{\"namespace\":\"config.external_validation_keys\",\"key\":{\"ttlExpiresAt\":1},\"name\":\"ExternalKeysTTLIndex\"}}{\"t\":{\"$date\":\"2023-03-14T06:41:26.315-04:00\"},\"s\":\"I\",\"c\":\"INDEX\",\"id\":5479200,\"ctx\":\"TTLMonitor\",\"msg\":\"Deleted expired documents using index\",\"attr\":{\"namespace\":\"config.external_validation_keys\",\"index\":\"ExternalKeysTTLIndex\",\"numDeleted\":0,\"durationMillis\":0}}{\"t\":{\"$date\":\"2023-03-14T06:41:26.315-04:00\"},\"s\":\"D1\",\"c\":\"INDEX\",\"id\":22533,\"ctx\":\"TTLMonitor\",\"msg\":\"running TTL job for index\",\"attr\":{\"namespace\":\"config.tenantMigrationDonors\",\"key\":{\"expireAt\":1},\"name\":\"TenantMigrationDonorTTLIndex\"}}{\"t\":{\"$date\":\"2023-03-14T06:41:26.315-04:00\"},\"s\":\"I\",\"c\":\"INDEX\",\"id\":5479200,\"ctx\":\"TTLMonitor\",\"msg\":\"Deleted expired documents using index\",\"attr\":{\"namespace\":\"config.tenantMigrationDonors\",\"index\":\"TenantMigrationDonorTTLIndex\",\"numDeleted\":0,\"durationMillis\":0}}{\"t\":{\"$date\":\"2023-03-14T06:41:26.315-04:00\"},\"s\":\"D1\",\"c\":\"INDEX\",\"id\":22533,\"ctx\":\"TTLMonitor\",\"msg\":\"running TTL job for index\",\"attr\":{\"namespace\":\"config.system.sessions\",\"key\":{\"lastUse\":1},\"name\":\"lsidTTLIndex\"}}{\"t\":{\"$date\":\"2023-03-14T06:41:26.315-04:00\"},\"s\":\"I\",\"c\":\"INDEX\",\"id\":5479200,\"ctx\":\"TTLMonitor\",\"msg\":\"Deleted expired documents using index\",\"attr\":{\"namespace\":\"config.system.sessions\",\"index\":\"lsidTTLIndex\",\"numDeleted\":0,\"durationMillis\":0}}The above logs represent pre-existing TTL indexes MongoDB creates to expire documents from internal collections. As these indexes are out of scope for this article we’ll skip over them for now and create a custom TTL index to fit our needs.Setupdb.foo.drop();db.foo.createIndex({ \"created_at\": 1 }, { expireAfterSeconds: 10 });db.foo.insertOne({ created_at: new Date()});Our new collection now has a single document, which should expire after 10 seconds of being created. Note that the timing will almost never be exact as the TTL monitor will only activate every minute.The test cluster we’re running on is a replica set, and as such we’re only monitoring the logs for the primary member. The reason for this can be found in the TTL Indexes documentation regarding replica set behavior: On replica set members, the TTL background thread only deletes documents when a member is in state primary. The TTL background thread is idle when a member is in state secondary. Secondary members replicate deletion operations from the primary.Within a minute or two a log entry similar to the following will be recorded that indicates our test.foo namespace has been processed, the created_at_1 index has been executed and our document was expired:{\"t\":{\"$date\":\"2023-03-14T06:47:41.336-04:00\"},\"s\":\"D1\",\"c\":\"INDEX\",\"id\":22533,\"ctx\":\"TTLMonitor\",\"msg\":\"running TTL job for index\",\"attr\":{\"namespace\":\"test.foo\",\"key\":{\"created_at\":1},\"name\":\"created_at_1\"}}{\"t\":{\"$date\":\"2023-03-14T06:47:41.336-04:00\"},\"s\":\"I\",\"c\":\"INDEX\",\"id\":5479200,\"ctx\":\"TTLMonitor\",\"msg\":\"Deleted expired documents using index\",\"attr\":{\"namespace\":\"test.foo\",\"index\":\"created_at_1\",\"numDeleted\":1,\"durationMillis\":0}}Unfortunately all this presents us with is confirmation that one document (\"numDeleted\":1) was removed, but is it possible to find out what was removed?Enter the OplogAs the cluster we’re connected to is a replica set all write operations are recorded in the operations log (oplog). The oplog is a capped collection present on each replica set member, and as such we can query it directly to see what write operations have propagated at a given time!Using the log entry above that confirmed 1 document was deleted we can prepare the following operation to filter the contents of the oplog collection:// the namespace from the log entry indicating nDeleted &gt; 0// that we're interested invar ns = \"test.foo\";// the time (with timezone) from the log messagevar t1 = new Date(\"2023-03-14T06:47:41.336-04:00\");// the number of seconds between TTLMonitor sweeps - default: 60var ttlSleep = db.adminCommand({ getParameter: 1, ttlMonitorSleepSecs: 1}).ttlMonitorSleepSecs;// get the time (in milliseconds) of the starting time and convert// the TTLMonitor sleep threshold to millisecondsvar t2 = new Date(t1.getTime() + (ttlSleep * 1000));db.getSiblingDB(\"local\").oplog.rs.find({ op: \"d\", ns: ns, wall: { $gte: t1, $lt: t2 }}); Any query targeting the oplog will perform a full collection scan as you cannot create indexes on the oplog collection!The above operation is purposely more verbose than necessary to illustrate where all the necessary pieces of information came from. Once this is executed it should return a single document as seen below, which represents the document that was deleted:[ { \"op\": \"d\", \"ns\": \"test.foo\", \"ui\": { \"$binary\": { \"base64\": \"K4vB+Oh9RgC5scSLCiYWiA==\", \"subType\": \"04\" } }, \"o\": { \"_id\": { \"$oid\": \"641050bae52e6d96ee3c40fa\" } }, \"ts\": { \"$timestamp\": { \"t\": 1678790861, \"i\": 1 } }, \"t\": 8, \"v\": 2, \"wall\": { \"$date\": \"2023-03-14T10:47:41.336Z\" } }]The only information the oplog records for delete commands is the _id value of the document that was removed. Unless you manage your own custom _id generation, the values will likely just be generated ObjectId values.We can try to find out additional information about this deleted document by extracting the _id from the oplog document’s o field, and using that to further filter the oplog:db.getSiblingDB(\"local\").oplog.rs.find({ // filter by namespace ns: \"test.foo\", // filter by insert or update op: { $in: [\"i\", \"u\"] }, // filter on the o2._id field as it will be present // in both inserts and updates \"o2._id\": ObjectId(\"641050bae52e6d96ee3c40fa\")}).sort({ ts: -1 });As documents within the local.oplog.rs namespace will eventually roll over the above query is not guaranteed to return anything, however it’s possible that document creation or update commands may still exist that can give you additional information regarding what this removed document contained:[ { \"lsid\": { \"id\": { \"$binary\": { \"base64\": \"SHocj0XsSTC25zlbzywaOA==\", \"subType\": \"04\" } }, \"uid\": { \"$binary\": { \"base64\": \"47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=\", \"subType\": \"00\" } } }, \"txnNumber\": 1, // an insert operation \"op\": \"i\", // the namespace the document was created in \"ns\": \"test.foo\", \"ui\": { \"$binary\": { \"base64\": \"K4vB+Oh9RgC5scSLCiYWiA==\", \"subType\": \"04\" } }, // this field contains the values that were set when // the document was created \"o\": { \"_id\": { \"$oid\": \"641050bae52e6d96ee3c40fa\" }, \"created_at\": { \"$date\": \"2023-03-14T10:47:22.445Z\" } }, \"o2\": { \"_id\": { \"$oid\": \"641050bae52e6d96ee3c40fa\" } }, \"stmtId\": 0, \"ts\": { \"$timestamp\": { \"t\": 1678790842, \"i\": 4 } }, \"t\": 8, \"v\": 2, // when the document was created \"wall\": { \"$date\": \"2023-03-14T10:47:22.448Z\" }, \"prevOpTime\": { \"ts\": { \"$timestamp\": { \"t\": 0, \"i\": 0 } }, \"t\": -1 } }]The result above represents an insert and under the o field contains the values that were initially set.As stated earlier, this method is not guaranteed to work, however it may prove useful if you’re trying to identify what document was deleted and your oplog window is large enough that it still contains the document’s creation. See Change the Size of the Oplog for more information regarding sizing the oplog." }, { "title": "Full Throttle (LucasArts) - 1995", "url": "/blog/2023/03/10/full-throttle/", "categories": "Let's Adventure!", "tags": "adventure, LucasArts, SCUMM", "date": "2023-03-10 05:43:40 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Full Throttle is a graphic adventure game set in the near future that follows Ben, the leader of...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Full Throttle is a graphic adventure game set in the near future that follows Ben, the leader of a biker gang, who is framed for the murder of a beloved motorcycle manufacturing mogul and seeks to clear his and his gang’s names.What immediately stands out when you play this game is the music and voice acting. As this is a biker game you’d expect a rock or metal soundtrack, which is done by The Gone Jackals, and suits the game incredibly well. The intro song (Legacy) that plays while you’re introduced to the characters and the story is setup really gets you pumped up and sets the tone for the rest of the game.Mark Hamill provides the voice for Adrian Ripburger, who you know right from the beginning is going to be the main villain. Having grown up watching Batman: The Animated Series as soon as I heard how this character was voiced all I could hear was the Joker - which just felt perfect. Roy Conrad plays Ben, and also feels perfectly cast for this role.All dialogue is voiced in this game, and honestly I can’t point to a single character who’s performance feels “phoned in”.As with all adventure games of this era, you communicate with other characters via a dialogue tree. Depending on the situation, you can uncover new information or advance the plot by picking from a list of options - reading the response - then repeating. Certain scenes can’t advance until you navigate a specific combination of options, but you can just keep repeating these conversations until you get them right and allow the game to progress.To interact with a scene or a character you just move the cursor over it and click to bring up a context menu. This is a far simpler version of the action bar that Sierra On-Line games would present you with and actually makes the game a bit more enjoyable by removing a lot of the “verb-cycling” you’d do in other games as you right click away until you get the right icon. Once the context menu is on screen, you just move to one of the four verbs associated positionally with an icon (LOOK, GET, KICK, MOUTH). “MOUTH”, though not really a verb cleverly rolls a number of actions into a single option, as you can use this to talk to characters, blow on something, drink something … etc.The graphics in this game still stand up. Though I played through this game using an hq2x scaler, in retrospect I think it would have been better to just experience the graphics unmodified as they really are fantastic. Double Fine re-released this game in 2017 with updated graphics, but when you look at their side-by-side comparisons it’s clear the original still looks pretty good … just a bit lower resolution.Though this segment only happens twice, you have to fight other biker gangs in an arcade sequence to progress through these sections of the game. This is an interesting mechanic as you can pick up various weapons from bikers you beat to improve your odds of beating harder bikers. These sequences can be a bit challenging, though if you lose you just get back on your bike and try again.What I actually appreciated even more is that you can effectively skip these scenes by just pressing SHIFT-V. I’m not sure if this was in the original game or just introduced by ScummVM as a quality-of-life feature, but if you’re getting stuck on these fights and want to just progress the story you have the option.The same option exists in the demolition derby sequence. The goal here is to create a distraction so the Vultures can steal a custom motorcycle that is the first prize in the derby. To finish the segment you have to jump off a ramp with your car onto another car to stall it, then push this car off the ramp onto another car to stall it then ram another car to make it blow up … confused yet?This demolition derby sequence is actually a bit frustrating and the fact that you can skip it too if you’re not interested in slogging through the arcade sequence is appreciated. I do remember muddling my way through this segment many times when I was a kid and not finding it particularly challenging, but this time around even after reading the walkthrough I still resorted to the sequence break shortcut to move along …As far as adventure games go, Full Throttle is a really good intro to the genre as it’s fun, has a compelling story and is very well paced. There are limited options on each screen for you to interact with, and since this is a LucasArts title you can’t die or end up in an un-winnable situation.Though I personally love this game and find the story to be what keeps me coming back, it’s actually really short. Though the How Long To Beat? time lists this at 6.5 hours, I think I actually finished in just over 2 hours. Everything progresses in an extremely linear fashion, which is a bit of a let down as this is a world I would have wanted to explore further.If you’ve never played an adventure game before, I would highly recommend Full Throttle (or the remastered version). This title really embodies what was being produced at the height of the golden age of graphical adventure games and is still a great experience today.Game Information Game Full Throttle Developer LucasArts Publisher LucasArts Release Date April 30, 1995 Systems DOS, Mac OS, Windows Game Engine SCUMM Play Information How Long To Beat? 6.5 hours Version Played Windows via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 18 Story (25) 23 Experience (15) 13 Impact (10) 8   89% Gallery" }, { "title": "Duckman: The Graphic Adventures of a Private Dick (The Illusions Gaming Company) - 1997", "url": "/blog/2023/03/03/duckman/", "categories": "Let's Adventure!", "tags": "adventure, The Illusions Gaming Company, Playmates Interactive Entertainment, Illusions", "date": "2023-03-03 05:34:56 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Duckman: The Graphic Adventures of a Private Dick is a point-and-click adventure video game deve...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Duckman: The Graphic Adventures of a Private Dick is a point-and-click adventure video game developed by the Illusions Gaming Company and published by Playmates Interactive based on the American television series, Duckman. In the game, Duckman has been usurped by a similar-looking impersonator, whom the real Duckman must expose.You start the game off with some exposition by Cornfed, your sidekick introducing us to the current state of the world with Duckman and his family being given a TV show, but Duckman’s been off at a spa getting drunk and ignoring his responsibilities. I don’t remember the show all that well - other than I enjoyed the lewd humour and voice acting - so whether this plot feels appropriate to the characters or not I can’t really tell.I seem to remember this show was all about innuendo and dick and fart jokes, which the writing of the game and artistic direction definitely tries to capture. It doesn’t feel overdone or “in your face” so unless you’re completely put off by this style of humour, the game should be playable (and possibly enjoyable).The game is a basic point and click adventure, so once you leave the first area (the spa) you’re presented with a map screen from which you can choose another location to investigate. New locations pop up as the game progresses, and you can revisit locations repeatedly as you learn more about the plot against Duckman.This game is all about item combination puzzles and fetch quests, so move the cursor around each screen and interact with anything that changes the cursor. The solution to most puzzles are pretty obvious, but a couple were challenging enough to get me to refer back to the walkthrough to keep this game moving along.When the time comes to get the plunger from another character and you have to first insult him in chinese by rearranging characters from a fortune cookie in order to say something rude so he kicks you (with his plunger foot), which you then have to block with a trash can lid to get the plunger to stick to the lid … that verged on moon logic.Character interactions are done using a dialogue system that relies on icons to select topics (similar to Sam &amp; Max Hit the Road). There’s actually not a lot of these interactions throughout the game as most characters just dump a wall of text on you to keep you moving along.You’re able to make mistakes that result in Duckman dying, but the game will just back you up to before this mistake so you can try again. I actually think this only happened to me once during this playthrough (using the remote on the robot villain) so I don’t know that risk of death is even a concern … but saving once in a while probably wouldn’t hurt.For a game that came out in 1997 the graphics seem a bit dated. The game isn’t really all that much fun, the music isn’t memorable (I think there was background music) and the plot isn’t all that interesting.If this game seems even the slightest bit compelling to you it might be best to just go find a copy of the TV show and watch that instead. You’ll likely have a better time, and get more out of it as this game is pretty short too.Game Information Game Duckman: The Graphic Adventures of a Private Dick Developer The Illusions Gaming Company Publisher The Illusions Gaming Company,\tPlaymates Interactive Entertainment Release Date May 31, 1997 Systems Windows Game Engine Illusions Play Information How Long To Beat? 3 hours Version Played Windows via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 10 Story (25) 12 Experience (15) 9 Impact (10) 3   49% Gallery" }, { "title": "Space Quest II: Vohaul's Revenge (Sierra On-Line) - 1987", "url": "/blog/2023/02/24/space-quest-2/", "categories": "Let's Adventure!", "tags": "adventure, Sierra On-Line, AGI", "date": "2023-02-24 12:03:43 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Space Quest II is a graphic adventure game released on November 14, 1987 by Sierra On-Line. It w...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Space Quest II is a graphic adventure game released on November 14, 1987 by Sierra On-Line. It was the sequel to Space Quest I: The Sarien Encounter, again using Sierra’s AGI game engine.Unlike other entries in the Space Quest series, the main character is not Roger Wilco … well, not exactly. You get to enter your name before the game begins and that’s the name that will be used throughout. I really like this type of personalization as it makes for a far more immersive experience.Since you’re still a master of the custodial arts, the game starts off with you sweeping the deck of Xenon Orbital Station 4, where you’ve the head janitor. The villain from the first game, Sludge Vohaul is out for revenge and captures you and ships you off to the Labion labour mines. Along the way you learn about Sludge Vohaul’s plan to eradicate sentient life from Xenon by launching millions of cloned insurance salesmen at the planet.The Space Quest series does not take itself seriously, and this game is filled with puns and jokes. The writing is excellent and the descriptions of most things you can interact with are pretty funny. I found myself taking a little longer than usual looking around the various scenes to see what I could interact with and read about as a result.As with most (all?) Sierra On-Line games you could very easily die on just about every screen. I was playing this game on ScummVM on a MacBook so I didn’t have a numpad, which meant I didn’t have extended directional control. If you wanted to go up or down stairs you had to drop the speed to slow, then alternate between up/down and left/right (depending on where you’re going).This was particularly annoying at the very end of the game where you have to climb the stairs after beating Sludge Vohaul, and you have almost zero margin for error. I forgot to save for a while leading up to this point and fell and died after a couple steps …Basically everything you do in this game requires you to pick stuff up and use it later … and you can miss things and end up stuck. Unlike the first game, to make this even more complicated if you figure out the right item to use to get out of a certain situation, you may also have to choose the right timing or you’ll still end up dead.For example, if you think to stick the plunger to the wall to avoid drowning in a pool of acid, if you stick the plunger too early you’ll lose your grip and still fall to your death. Fun times :PThis sequence took FOREVERSome of these early AGI games rely a little too heavily on making you navigate mazes. Getting the berries on Labion is a pain in the ass as you have to be extremely precise so as not to touch the vines (or you die), so you end up having to just drop the game speed to slow and take a few steps/save/repeat. This isn’t fun - just tedious busy work.Space Quest I had a couple of mini games in it (the slot machine and the speeder), which mixed up the game play a bit. Space Quest II has a sequence where you have to time the release of a vine so as not to get eaten, but that’s about it. The rest of the game is what you’d expect from a text-parser driven adventure game.Visually this is an improvement over the original game, but not by all that much. The AGI engine is very limited, but Sierra’s artists knew how to get the most bang for their (and your) buck out of it. There are a couple of cut scenes and close ups, but mostly you just move your stick figure around and type in commands that paint a picture in your mind using words.Don’t let the xenomorph catch you and kiss you!I really liked Space Quest I, so I’m harder on the sequel as I didn’t find it added much to the original. It wasn’t as much fun to play, though it was extremely well written. I found some sequences (like escaping Labion in the shuttle) frustrating, but overall this was an ok game.Infamous Adventures did a VGA remake of this game in 2018, but I never gave it a spin. Honestly this looks extremely well done - as does their remake of King’s Quest III - so if you’re looking to play this game it may be worth starting there.Game Information Game Police Quest II: The Vengeance Developer Sierra On-Line Publisher Sierra On-Line Release Date November 14, 1987 Systems DOS, Macintosh, Apple II, Apple IIGS, Amiga, Atari ST Game Engine AGI Play Information How Long To Beat? 3.5 hours Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 14 Story (25) 18 Experience (15) 9 Impact (10) 5   66% Gallery" }, { "title": "Police Quest II: The Vengeance (Sierra On-Line) - 1988", "url": "/blog/2023/02/21/police-quest-2/", "categories": "Let's Adventure!", "tags": "adventure, Sierra On-Line, SCI", "date": "2023-02-21 08:17:26 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Police Quest 2 is a 1988 police procedural adventure video game developed and published by Jim W...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Police Quest 2 is a 1988 police procedural adventure video game developed and published by Jim Walls and Sierra On-Line. It is the second installment in the Police Quest series. The game continues the story of police officer Sonny Bonds as he attempts to apprehend an escaped convict.This second installment in the Police Quest series focuses more on detective and forensics work than the traffic-cop beginning of the original, while keeping the same realistic setting. The proper procedures for collecting and handling evidence are the main focus of many of the puzzles in Police Quest II.Though still a text-parser driven game, the SCI engine is used which makes for a much more visually impressive game. A lot has been improved over the original game (though I reviewed the remake), though being a “police procedure simulator” above all else, there is still a fair amount of backtracking and checking the manual.I don’t miss copy protection in gamesYou’ll start the game off by having to look up a mugshot in the manual. I guess this is good because I will force you to take note of the various radio codes, vehicle codes and penal codes that you may need to reference later. Unlike the previous game you don’t really need to memorize these codes as you’ll be playing with a partner throughout. Keith provides you with a bit of comic relief, as well as interacting with dispatch.I don’t remember having to manually provide any codes, but if you happen to need them, they’re all in the manual.Though the manual gives you the map of Lytton that you needed for navigation in the first game, this time around you just need to type in where you want to drive and you’ll automatically go there. This is a huge improvement as you don’t need to waste time with the needless stopping/starting that was imposed on you in Police Quest 1 as you muddled your way from location to location.You’ll also get updates from dispatch as you’re driving, which Keith will provide the narrative around. As he throws out various radio and vehicle codes you can just read along without having to worry about potentially throwing the wrong code out there.Good ol’ fashioned Italian stereotypesThe game has you following Jesse Bains, who you locked up in the previous game but has escaped from prison. He breaks out of jail, kidnaps a guard and is trying to skip town, so you’re trailing him based on the clues you collect at the various locations he’s been spotted at. The focus in this game is on collecting evidence so you’ll need to familiarize yourself with the field kit and its contents pretty early on.Death waits for you at literally every turnSince this is a Sierra On-Line game, expect to die … a lot. Just remember to cycle through about 5-6 save slots, saving as you enter each screen and you shouldn’t be caught off guard when (not if) you find yourself hitting a game over situation.A big part of this game is your gun, which you have hotkeys available to “draw”, “holster” and “fire”. Since this is a parser-driven game you can also just type DRAW GUN to perform this action, which is preferable in certain situations as the text prompt pauses the action on screen.If your gun isn’t properly sighted, you’ll miss whenever you need to shoot somethingThe Police Quest games are all about proper police procedure, so before you can use your gun you need to make sure you hit the shooting range and adjust your elevation and windage. Basically, you aim at the center of the target, fire a couple times, view the target and ADJUST GUN. Replace the target and repeat until you’re hitting what you’re aiming at.At one point in the game you’ll spin out of the way of a shotgun firing through a door and slam your gun into the wall. This is meant to be a hint that you should go re-sight your gun as the next time you fire you’re likely going to miss.Aside from your field kit (which contains a lot of different items you’ll need to collect evidence), there isn’t all that much inventory management in this game. Most of the stuff you pick up will either be evidence you can book back at the station, or a clue to the next location you may need to visit.In Police Quest 1 you needed to pay really close attention to the penal codes being used when you’d bring in evidence or suspects. This time around you can just go up to the evidence window at the station and BOOK EVIDENCE.Though most of the game is pretty linear and easy enough to identify what comes next, there is one sequence early on where you’ll need to dive underwater to look for evidence. It’s really important to check the tanks in the van before you head out as only one has enough oxygen to last the entire dive (hint: look for the one with 2200 “AIR”).This is an interesting sequence that will require you to visit 3 underwater screens and find evidence at the bottom of the lake, as well as a body. It’s not obvious at first how you even need to initiate this sequence, but it’s something along the lines of Keith asking you what you want to do after Bains escapes at Cotton Cove, and you respond with REQUEST DIVE(or something like that).Unless you already know this is the next step to proceed with the story it’s pretty easy to get stuck here.The story of Police Quest 2 is a lot more detailed than its predecessor. Bains is out to kill everyone involved in his arrest, including yourself and your girlfriend Marie. He’ll end up kidnapping her and taking her to Steelton, which you have to get approval to book a flight to. On that flight you’ll have to stop a hijacking and diffuse a bomb, which doesn’t end up being too difficult (read the instructions and just reverse the steps to disarm it).Hopefully you’re smart enough to hide and return fire before Bains can silence you permanentlyOnce you make your way through the sewers of Steelton you’ll find Marie and rescue her. The death angel will try to take you out, but if you hid behind the pipe here and opened fire on Bains quickly enough, you should be able to stop him once and for all.I’ve played through this game a number of times since I was a kid and it’s been in my top 10 Sierra games as a result. I didn’t find the police procedure focus to be as distracting as it is in Police Quest 1, and the story just seems a lot more solid.The music in this game is also top notch. I finally got around to figuring out how to get Roland MT-32 working in ScummVM and can honestly say this makes the experience exponentially better. If you’ve never given the game’s intro a listen in MT-32 mode, go do that now!Police Quest 2 has aged pretty well, which may be partly due to it being a text-parser driven game. The graphics are decent, the story is pretty good and though there’s limited background music, what you get to hear is also really well done.This can be a challenging game if you’re not well versed in adventure games, but if you’ve never played it before I’d definitely recommend giving it a shot.Game Information Game Police Quest II: The Vengeance Developer Sierra On-Line Publisher Sierra On-Line Release Date November 1988 Systems DOS, Amiga, Atari ST, NEC PC-9801 Game Engine SCI Play Information How Long To Beat? 5 hours Version Played DOS via ScummVM Notes Manual, Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 13 Story (25) 18 Experience (15) 12 Impact (10) 7   71% Gallery" }, { "title": "Manhunter: New York (Evryware) - 1988", "url": "/blog/2023/02/18/manhunter-new-york/", "categories": "Let's Adventure!", "tags": "adventure, Evryware, Sierra On-Line, AGI", "date": "2023-02-18 08:24:33 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Manhunter: New York is a post-apocalyptic adventure game designed by Barry Murry, Dave Murry and...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Manhunter: New York is a post-apocalyptic adventure game designed by Barry Murry, Dave Murry and Dee Dee Murry of Evryware and published in 1988 by Sierra On-Line. Built using Sierra’s AGI (Adventure Game Interpreter) engine, it stands out as it doesn’t involve side-scrolling views or a text-parser to interact with the world.You play the role of a manhunter, tasked by the ORB Alliance to hunt down other humans the Orbs have deemed threats. There’s no real backstory presented via an intro sequence or tutorial mission - you have to read the manual to learn all this.The game is split up into four days. Each day begins with an Orb (floating eyeball things) coming into your apartment to give you the day’s assignment. You then fire up your MAD (Manhunter Assignment Device) which tracks the day’s “targets” to show you where they’ve visited and what other people they’ve encountered.Using the MAD locations within New York City will be made available on an overworld map for you to visit. The only way to open these locations for exploration is for them to either be uncovered by tracking targets using the MAD, or looking up people’s names via the MAD to get their address.As you’re a manhunter, and the job the Orb’s have given you is to identify individuals, you’ll need to pay attention any time a name is dropped. This can come in the form of toe tags on a corpse, posters on a bulletin board, notes or scraps of paper, signs, pictures, etc.I wonder if I can find more info about “Reno Davis” by just entering that into the MAD …Each location you uncover and visit will have limited hotspots to interact with, but moving the cursor around the screen will uncover these pretty quickly. The background artwork and character portraits are extremely well done and highly detailed.As one of the last AGI games made the team knew how to push the engine to the limits and make the best possible game as a result (only King’s Quest 4 and Manhunter 2 came out afterwards as AGI games).This game relies VERY heavily on arcade sequences to add additional interactive elements. These sequences tend to all play out in a very similar fashion as you’re either trying to navigate a maze, or waiting for (something) to move from left to right/top to bottom for you to click ENTER to throw.I didn’t love the arcade sequences and found them to be a bit too drawn out, and making a mistake would likely kill you and cause you to have to start over again from the beginning. The sequence where you have to get past 4 thugs by jumping or ducking was just tedious. You only progress when you jump or punch, and it takes forever to get to the person throwing stuff at you. If they happen to hit you you start over again and start the process of inching back towards them.Luckily, you can save everywhere in this game, so when you make a bit of progress you can save so you can restore from that point when you inevitably die moments later.There are SO many ways to die throughout this game, and each death typically results in the game designers giving you some cheeky feedback about how you screwed up and what you might want to do next time. This is such a Sierra On-Line trope at this point you’d almost be surprised NOT to see it in one of their games, but I still enjoyed reading the various messages.You also are sent back to right before you died whenever you do, so if you forgot to save there’s a lot less backtracking you’ll need to do to try again. This feature is extremely helpful as you’ll die A LOT.The difficulty of this game is pretty high, and requires a lot of trial and error to solve most puzzles. For example, below the pawn shop when you need to solve the 4 number-based puzzles you’ll likely have to resort to a walkthrough to get the last 3. I tried to do this on my own but just couldn’t figure it out :(The answer is 264, because one less than 3, 3 plus 3 and one greater than 3. Obvious, right?There’s almost never any form of background music throughout the game, but the “bleeps and bloops” you get as you navigate the various screens do add a bit to the experience. Honestly the sound quality really pales in comparison to the visuals - which I found surprising.I really love the tracking mechanic using the MAD. It’s really cool to be able to tag other moving characters to see where they go throughout the day and unlock additional places in New York City to investigate. There’s only a handful of items you pick up, so you never really feel you’re on a fetch-quest at any point.Manhunter is a lot more gory than typical Sierra titles. It’s great that Sierra didn’t feel the need to censor these games it was only acting as the publisher for and allowing them to express themselves with more mature content. I always enjoyed these titles more as a result of them not feeling “watered down”, as you’re playing in a dystopian future where death lies around every corner.The main villain’s name is Phil … just PhilYou have to piece the story together from the info you get in the manual then bits and pieces throughout the game. The endgame sequence is extremely complex and really assumes you’ve been paying very close attention to every possible piece of information so you’ll know what to drop bombs on, where they are and why you’re doing it.Use the crowbar on the stereo because that’s clearly what you’d be thinking of doing in this situationI loved this game when I first played it as a kid. I really enjoyed the subject matter and the whole MAD tracking mechanic just felt natural within this world. I’ve never really played another game the recreated this system of working backwards from a recording that you could engage with to discover how subjects moved around the game world.Though the game is pretty hard to beat without a walkthrough, I’d honestly recommend giving it a go if you’re a fan of adventure games. Just remember to be patient in the sewers as that maze is a pain in the ass. Interestingly enough though, the Apple II version of Manhunter allows the option of skipping the sewer maze because memory limitations prevented saving the player’s progress in that section.Maybe that’s the version I should have played …Game Information Game Manhunter: New York Developer Evryware Publisher Sierra On-Line Release Date 1988 Systems DOS, Amiga, Atari ST, Apple IIGS, Tandy 1000 Game Engine AGI Play Information How Long To Beat? 4.5 hours Version Played DOS via ScummVM Notes Manual, Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 17 Story (25) 14 Experience (15) 11 Impact (10) 6   69% Gallery" }, { "title": "The Crimson Crown - Further Adventures in Transylvania (Penguin Software) - 1985", "url": "/blog/2023/02/15/the-crimson-crown-further-adventures-in-transylvania/", "categories": "Let's Adventure!", "tags": "adventure, Penguin Software", "date": "2023-02-15 21:56:56 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Released in 1985 under the title The Crimson Crown, on the same platforms as its predecessor. Th...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Released in 1985 under the title The Crimson Crown, on the same platforms as its predecessor. The game tasks the player with a quest to defeat a magical vampire with the assistance of Princess Sabrina (who is now a fledgling magician) and the heir to the throne, Prince Erik.Make sure to use a 32bit build of Steem SSE if you want to play an STX ROM image on WindowsSince I haven’t really played with an Atari ST ever before, I wanted to make sure I tried at least one of the games in this series on that platform just to get a sense of what the emulation tools were like. There were quite a few options, though most were abandoned by their developers in the early 2000s but Steem SSE is still being maintained and is what I wound up using.Having already played through Transylvania I can honestly say this game comes with a pretty large improvement to the quality of the graphics. Background artwork is a lot more detailed, and inventory items you pick up and drop are a lot more identifiable as well.You have to indicate which character should perform a particular actionThis time around both Prince Erik and Princess Sabrina are “playable” characters. As this is a parser-driven game you can “control” your characters individually by forming a sentence with them as the subject (ex: SABRINA, CAST SPELL or ERIK, TAKE SWORD). The text parser is about as intuitive as last time, and you need to be precise with your inputs.I actually found this title a lot more difficult than the original as there was an added time-based element to some of the puzzles. For example, later in the game you’ll need to wait (literally by entering WAIT as the command) for several cycles for something in a room to change, or for some event to unfold.This process of delaying the game wasn’t intuitive (thanks walkthrough!) and I’m not really sure that this game mechanic is introduced earlier in the game text or in a manual. Once you know it’s an option though you’ll find yourself using this command periodically if you get stuck just to see if it will chance anything in the current room - which it won’t on like 99% of the game screens.The vast majority of this game is item collection and backtracking to figure out where that item needs to be used. Inventory management comes into play again as you can only carry so many items so you’ll have to occasionally drop something to make room for a new item. Thankfully you can see the items you dropped on the screen, plus the game will let you know that there are items you can pick up.My gaming experience here consisted of the following: get all items on current screen try all items on what’s on screen read the available directions to move on screen pick one and go in that direction repeatIt’s a bit tedious, but early adventure games - especially parser-driven games - were still figuring out how to make this experience “fun”. Honestly Infocom had already figured this out, but graphic adventure games were still re-learning those lessons in the mid-80s …You’ll eventually make your way to the vampire’s lair, and with the help of some “friends” you meet along the way (and a bunch of items you’ll need to find) get back Prince Erik’s crown and escape from the castle in time for it to collapse. The battle between the dragon and the vampire introduces a bit of animation, which I didn’t expect in this title so it was definitely a welcome change from all the static screens up to this point.To finish this game you’ll need a lot of patience, and a map. Most enemies can’t be beaten without taking multiple actions, and if you’ve missed any it’s game over. Saving often is a requirement, and the Atari ST version gives you 4 slots you can use. I found myself rotating through the save slots every couple of screens as I’d try different combinations of items in various scenarios until something worked (or I looked it up …).WAIT, WAIT, WAIT, WAIT, WAIT, WAIT … game overI didn’t have a lot of fun with this game, though it was an upgrade from the first title. It is interesting to see how parser-based games evolved in the early to mid 80’s, especially when it came to puzzle and dungeon design. Antonio Antiochia did all the artwork and game design solo for this and the previous title, so seeing as this was again essentially a one man show, it’s worth appreciating the accomplishment even if the game doesn’t really stand the test of time.For a deeper dive into the development journey of Transylvania I highly recommend The Digital Antiquarian’s retrospective of the game.Game Information Game The Crimson Crown: - Further Adventures in Transylvania Developer Penguin Software Publisher Penguin Software Release Date 1985 Systems Amiga, Apple II, Atari ST, Commodore 64, DOS, FM-7, Macintosh, PC-88, PC-98 Game Engine   Play Information Time to Completion 2 hours Version Played Atari ST via Steem SSE Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 9 Story (25) 10 Experience (15) 6 Impact (10) 3   40% Gallery" }, { "title": "Urban Runner (Coktel Vision) - 1996", "url": "/blog/2023/02/15/urban-runner/", "categories": "Let's Adventure!", "tags": "adventure, Coktel Vision, Sierra On-Line, Gob", "date": "2023-02-15 07:01:49 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Urban Runner is a French produced computer game developed by Coktel Vision and published by Sier...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Urban Runner is a French produced computer game developed by Coktel Vision and published by Sierra On-line. The game is an interactive movie or visual novel spanning four CD-ROMs.You play as Max Gardner, an American journalist in Paris, investigating a story about a big drug dealer who is covered by some influential politician. To get the dealer talking, you offer him incriminating photographs in exchange for some information. When Max arrives at the meeting point, the drug lord is dead and Max is mistaken for the killer.While evading the authorities, Max continues his investigation and finds an ally in Adda - the murdered drug dealer’s lover - and the two of them work to uncover the conspiracy behind the murders.That face right there sums up the acting in this gameThere are going to be a few FMV-based games in this series, and one thing they overwhelmingly have in common is terrible acting. Brandon Massey, who plays the main character Max, has 2 acting credits to his name … this and Police Quest: Open Season. I didn’t really go into this playthrough with high expectations … and this game did not disappoint (or it did … whatever).The game world consists of a series of static screens you move your cursor around to uncover hotspots. Depending on the type of hotspot, you can pick up an item, examine an object, speak to a character or move to the next screen. Most screens don’t really give you too many options so it’s pretty easy to figure out what to do next, but unlike most FMV games you’re constantly on a timer.When you first start the game you’ll have a hitman chasing you, who will eventually catch up to you if you take too long. There is no indication most of the time how long you have before you’re “caught” by whoever you’re trying to get away from, though some scenes do give you an event timer in the form of a depleting gauge at the bottom of the screen.If the action stops momentarily for you to react (such as throwing a ball, rolling under a moving car or knocking a detective out as he leans into the trunk of a car), the amount of time you have is extremely limited and will likely end with you dying … so save often!You can hardly tell he’s on the phone … it’s just so tiny!As this is more of a visual novel than a “game”, everything plays out in an extremely linear fashion. All character interactions are scripted and voice acted, and there’s generally only one way you can deal with any situation (since they only recorded one video to show you the outcome). The main problem I had with the actual “game” though was the quality of the static screens is extremely low.A glint you say? I guess I’ll have to take your word for it as it’s pretty hard to see ANYTHING … EVERThe majority of the game plays out with you picking up various items, examining them to learn clues about who to call or where to go. Every “level” has numerous puzzles, and honestly most of these are pretty hard. I failed A LOT while playing through this game and found myself resorting to the walkthrough pretty frequently.There are some pretty interesting puzzle solutions that really require you to pay close attention to what the characters are telling you, and then there are some that you just sort of “have to know” (or have read in the walkthrough). For example, to get a sleeping guard away from his desk you need to put his thumb in water. This will make him wake up needing to go to the bathroom.This is the type of thing you maybe learned at summer camp as a kid … or from movies about kids playing pranks on each other in summer camp … regardless, it’s giving you a lot of credit as a player to have to figure that out as a solution.You’ll eventually meet up with your love interest Adda, who is responsible for certain objectives throughout the game. You are given a choice periodically who’s story you want to progress, which creates the illusion of “control” - but you ultimately need to do both plot lines to advance the story. It does add some variety to the game, even if it’s only on the surface and doesn’t really do anything for the overall experience.Though her acting is a little over the top at times, Adda is a good character and actually makes the story a lot more enjoyable. Seeing as you’re essentially just helping to move the movie along its scripted path, thankfully the underlying story you’re advancing is decent. Though the premise is a little absurd, your character is thrown into an impossible situation and has to keep a step ahead of the mysterious syndicate he’s gotten himself mixed up with.If you can make it through the seemingly impossible puzzles, once you reach the game over screen you can select each character and get a summary of their involvement in the overall plot. I found this was actually a really clean way of closing out the story and tying everything together.I love visual novels, but this one wasn’t great. Maybe I’m spoiled by more recent titles like Danganronpa that actually blend multiple gameplay elements into the story progression to make advancing the plot more entertaining. Urban Runner was “ok”, and I know there are some FMV titles on the list that are even worse (cough … Phantasmagoria … cough), so maybe I shouldn’t judge too harshly.So far though I’m not loving these Coktel Vision titles.Game Information Game Urban Runner Developer Coktel Vision Publisher Sierra On-Line Release Date 1996 Systems DOS, Windows Game Engine Gob Play Information How Long To Beat? 5.5 hours Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 10 Story (25) 14 Experience (15) 5 Impact (10) 5   49% Gallery" }, { "title": "Ruby on Rails Global Summit '23", "url": "/blog/2023/02/13/ruby-on-rails-global-summit-23/", "categories": "Presentations", "tags": "ruby, rails, ruby-on-rails, conferences, talks", "date": "2023-02-13 13:08:21 -0500", "snippet": "This year (January 24, 2023) I had the opportunity to present at Ruby on Rails Global Summit ‘23, which was a Geekle sponsored event. As I’m currently a Senior Product Manager at MongoDB focusing o...", "content": "This year (January 24, 2023) I had the opportunity to present at Ruby on Rails Global Summit ‘23, which was a Geekle sponsored event. As I’m currently a Senior Product Manager at MongoDB focusing on the Ruby and Node.js Drivers this was a chance to show the audience what a great fit the Mongoid ODM is for Ruby on Rails developers.The full talk is available on YouTube, and I’ve published my notes as well: Ruby on Rails Global Summit ‘23 – Junior Track Ruby on Rails Global Summit ‘23: Ruby on Rails and MongoDB - NotesIf you’re unsure what an ODM (Object-Document Mapper) is, I have another article that provides more information regarding MongoDB ORMs, ODMs, and Libraries." }, { "title": "Rise of the Dragon (Dynamix) - 1990", "url": "/blog/2023/02/13/rise-of-the-dragon/", "categories": "Let's Adventure!", "tags": "adventure, Dynamix, Sierra On-Line, DGDS", "date": "2023-02-13 08:52:26 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Rise of the Dragon is a graphic adventure game released in 1990 for DOS and Macintosh, and later...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Rise of the Dragon is a graphic adventure game released in 1990 for DOS and Macintosh, and later remade for the Sega CD (1993) as well as the Amiga. It was one of the few adventure game titles developed by Dynamix, a company that was better known as an action and flight simulator game developer. The game is set in a dark future cyberpunk version of Los Angeles.This game really had “Sierra vibes”, which is kind of foreshadowing as it was actually published the same year that Dynamix was bought by Sierra On-Line.Rise of the Dragon is a “Blade Runner”-esque dystopian future game. You play the role of William ‘Blade’ Hunter, who is investigating the death of the mayor’s daughter from an overdose of a design drug called MTZ that mutates victims and kills them.You start off in Blade’s apartment and by moving the mouse around can interact with various things like the bed, the computer, the sink, cabinet … etc. There are very few interactive areas on each screen, and almost everything is important to advancing the plot.Picking items up involves dragging and dropping them onto your character portrait in the lower right hand corner of the screen. Clicking the portrait also brings up you inventory, where you can examine items, drop them or drag them out of the inventory to be used on the current screen. Certain items can also be used directly on Blade, such as his clothes, trench coat, bullet proof vest and gun.This threw me off at first because I knew I needed to “equip” some of the stuff I’d picked up, but I couldn’t figure out how to do it. Turns out you need to look at the item first (by right click on it), then when you exit that view you’re back on the inventory screen with Blade on the right - where you can now drag and drop stuff to him.You move around Los Angeles by means of a quick travel map. You’ll discover new locations periodically that get added to this map, and you can go to them at any time once they’re uncovered. Depending on the time of day though, certain places may be closed or certain characters may not be available.I actually found this to be a really interesting mechanic, as you had to pay attention to the time it took you to perform your investigation. For example, you can only go to city hall between 09:00 and 17:00, otherwise it’s closed.When he says 8:30PM - he means it. Go there earlier and no one’s thereWatching the time can become a little tedious if you don’t know you can speed it up from the inventory screen. On first playthrough when you don’t know there are certain times events will occur, you can wander around aimlessly - but once you’re aware of when events will trigger you can just go to the right location and skip ahead an hour at a time really quickly to advance the story.You’ll find multiple different ways to electrocute yourself in this gameIt’s pretty easy to screw up in this game, so it’s important to save often. Near the end of the game events are timed so you have to complete certain puzzles and sequences quickly or you’ll be captured and/or killed.To mix it up a bit there are also a couple of arcade sequences mixed into this game. One involves moving crosshairs around the screen to try and shoot snipers before they shoot you, and the other is a side-scrolling shooter.Ask me how I know you can skip this sequence if you fail too many times …In the DOS version of the game the arcade sequences are actually optional-ish. If you lose too many times it’ll actually prompt you to ask if you want to automatically win and skip ahead, which is kind of humiliating … but appreciated …Atmospherically this game looks, feels and sounds like a cyberpunk thriller. Though there isn’t much music, what is there does enhance the experience. For a 256 colour game the artwork is appealing and brings the story to life.You’ll see this screen on a game over as well - but the end of the game is also technically “game over” …I really love this game, but it’s important to point out that it’s incredibly short. There really isn’t all that much to do, the story progresses in an extremely linear fashion and the puzzles (other than tapping the phone line in the sewers!) aren’t too challenging.As a fun piece of personal trivia, I actually started implementing the Dynamix Game Development System (DGDS) as an engine for ScummVM in 2012 but didn’t really get very far. This is the engine Rise of the Dragon, as well as a couple other Dynamix titles were scripted in, and my hopes were to make these titles more accessible on modern hardware.Playing through this game gave me a chance to not only relive my past playthrough experiences, but also dig up some of my old disassembly notes from when I was trying to understand the inner workings of the engine over a decade ago.A lot about Rise of the Dragon still resonates with me, and for anyone looking for a quick cyberpunk adventure game, I’d still recommend this title.Game Information Game Rise of the Dragon Developer Dynamix Publisher Sierra On-Line Release Date March, 1994 Systems Amiga, DOS, Macintosh, Mega-CD Game Engine DGDS Play Information How Long To Beat? 5 hours Version Played DOS via DOSBox-X Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 17 Story (25) 17 Experience (15) 12 Impact (10) 7   76% Gallery" }, { "title": "Beneath a Steel Sky (Revolution Software) - 1994", "url": "/blog/2023/02/10/beneath-a-steel-sky/", "categories": "Let's Adventure!", "tags": "adventure, Revolution Software, Virgin Interactive Entertainment", "date": "2023-02-10 09:46:54 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Beneath a Steel Sky is a 1994 cyberpunk science fiction point-and-click adventure game developed...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Beneath a Steel Sky is a 1994 cyberpunk science fiction point-and-click adventure game developed by British developer Revolution Software and published by Virgin Interactive Entertainment for MS-DOS and Amiga home computers. The game was made available as freeware (download at GOG.com) – and with the source code released – for PC platforms in 2003.Set in a dystopian future, the player assumes the role of Robert Foster, who was stranded in a wasteland known as “the Gap” as a child and adopted by a group of local Aboriginals, gradually adjusting to his life in the wilderness. After many years, armed security officers arrive, killing the locals and taking Robert back to Union City. He escapes and soon uncovers the corruption which lies at the heart of society.Dave Gibbons collaborated on the artwork for this game, which explains why the intro sequence is so visually stunning. Having been a huge fan of the Watchmen trade paperback when I was younger, seeing the same style translated directly into an adventure game made this an instant classic for me.You start the game following a helicopter crash in a factory. You have a circuit board in your inventory and only two real options for interacting with the world: “move” or “interact”. These options are presented via an arrow cursor on the screen that will show a label if you hover over a hot spot you can interact with. There are very few actionable areas per screen you can interact with, but it doesn’t make the game feel limited as nothing really seems to be there to act as “filler”.When you encounter other characters they can be interacted with via a typical dialogue menu. Each character has limited topics they can discuss, with certain options only being unlocked after you’ve either talked to another character or interacted with something else on the same screen. As there aren’t that many characters to engage with, everyone you do talk to tends to actually share something useful and helps advance the plot.The story is clearly inspired by Neuromancer as you have to get the main character fitted with a port so they can physically interface with a machine and enter into a virtual world. This is the human/computer interface you’d expect in a cyberpunk-type game, and when you’re in this world you can access data files and unlock more information.Foster is followed around throughout the game by a robot sidekick called Joey. When the game begins your only item is Joey’s circuit board, which you can plug into various robot shells throughout the game to give Joey additional capabilities. The first shell allows Joey to interface with various machines using a probe, as well as fly down from high places using a helicopter like rotor. Later he gets into a welder robot so can cut things with a torch.Talking to Joey can can give you additional information about the current situation you’re in, or you can give him instructions. Joey’s responses can be pretty funny as Joey clearly has a personality and is not just a bland robotic assistant. The torch allows him to separate certain background items so you can pick them up, which is needed as there are some basic inventory management tasks needed to complete this game.Unlike a lot of adventure games, there really aren’t a lot of items you need to pick up and use in Beneath a Steel Sky. Throughout the entire game you maybe get a dozen unique items, and each is required to advance the plot. Some items such as the pipe and the wrench actually get used in multiple puzzles.This game clearly takes a lot of inspiration from the LucasArts games, though you are able to make mistakes that result in Foster’s death. The artwork is beautiful and the background music and sound effects really enhance the experience and ambiance. Since I played through the game using ScummVM I also grabbed a copy of James Woodcock’s Enhanced Soundtrack which makes a great soundtrack even better!Overall the story is compelling, the game play is intuitive and progresses naturally without feeling artificially stretched out. I enjoy a good “Blade Runner”-esque store, which Beneath a Steel sky definitely is. If you’re looking for a game to help introduce you to adventure games in general this would be a good choice.Game Information Game Beneath a Steel Sky Developer Revolution Software Publisher Virgin Interactive Entertainment Release Date March, 1994 Systems DOS, Amiga, Amiga CD32, Atari ST, iOS Game Engine Sky Play Information How Long To Beat? 6 hours Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 18 Story (25) 22 Experience (15) 12 Impact (10) 8   86% Gallery" }, { "title": "Emmanuelle (Coktel Vision) - 1989", "url": "/blog/2023/02/08/emmanuelle/", "categories": "Let's Adventure!", "tags": "adventure, Coktel Vision, Tomahawk", "date": "2023-02-08 15:29:06 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Emmanuelle is an erotic graphical adventure game from Coktel Vision, originally released in 1989...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Emmanuelle is an erotic graphical adventure game from Coktel Vision, originally released in 1989 for Amiga, Atari ST, and MS-DOS. The game was developed by Muriel Tramis (better known from her games in the Gobliiins series, Fascination &amp; Lost in Time) and is loosely inspired by Emmanuelle Arsan’s emmanuelle series of novels.Everyone’s DTF … you just gotta say the right thingI’m gonna be honest here … I had no idea what was going on, what I was supposed to be doing, where I was supposed to go, why I was going there, what these random “Idols” were or … anything. This game is really fucking confusing and if the walkthrough didn’t fit on a single page (no scrolling) I’d likely have ignored this title altogether.Copy protection is possibly the most intuitive part of this gameYou start off by having to lookup a couple of colour values from a massive table. This is the game’s copy protection and only lasts a few seconds, but may be the most fun part of the game :PEGA nudity … how “hot”The interface is unintuitive and the goal of the game is unclear. It’s possibly covered in the manual (I didn’t go looking for it) but apparently you’re trying to find “Emmanuelle”, and you need to get 75 or more “eroticism” points to make this happen. These points are added by hooking up with random women, which you do by choosing the right options from a dialogue tree.When you’re trying to choose a dialogue option they cycle by extremely quickly, which makes it hard to pick the right one. This may be an artifact of running the game in DOSBox (I could lower the CPU speed, but didn’t) but just in case the original game sucked just as badly on original hardware I figured it was worth calling out.CORRECTION: Turns out this is actually VERY MUCH due to running the game in DOSBox. I tried the Atari ST version using Steem SSE and the timing was not an issue. Also it appears the palette is only complete garbage when playing this game in DOS. When you play this game in an Atari ST emulator the 16 colors actually look pretty good as opposed to the baby vomit that the DOS colors seem to default to.I still wouldn’t recommend playing the game, but if you have to - do it on an Atari STThere are a half dozen cities you need to visit and each one has an airport you can buy tickets from. The amount of money you have is limited and if you run out it’s game over.You find Emmanuelle at the airport and leave … EXIT TO DOSThis game just makes no sense. Maybe if you’ve read the books this is loosely based on you’ll have some idea as to where to go but from the moment you get “control”, locations on the maps aren’t highlighted in any way so you’re guessing where to click to interact with the game world. What you’re supposed to do on each screen makes no sense. Why I had to buy a toucan at one point made no sense.Nothing about this game made any sense. If it made sense to YOU, please explain in the comments below :)Game Information Game Emmanuelle Developer Coktel Vision Publisher Tomahawk, Coktel Vision Release Date 1989 Systems DOS, Amiga, Atari ST Play Information How Long to Beat? 1 hour Version Played DOS via DOSBox-X Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 3 Story (25) 3 Experience (15) 2 Impact (10) 1   13% Gallery" }, { "title": "Eternam (Infogrames) - 1992", "url": "/blog/2023/02/07/eternam/", "categories": "Let's Adventure!", "tags": "adventure, Infogrames", "date": "2023-02-07 21:42:48 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Eternam combines futuristic elements with historical settings. The player assumes the role of of...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Eternam combines futuristic elements with historical settings. The player assumes the role of officer Don Jonz of the Orion United Forces, who is starting a vacation on the planet Eternam. The planet is described as a galactic amusement centre, where different islands represent different periods of Earth’s history.Check out the Razor1911 cracktro at DemozooThe copy of the game I was playing was apparently cracked by Razor1911, which means it came bundled with a cracktro. Though it has nothing to do with this review it did give me a chance re-experience one of these demos that I found so fascinating back in the day. Software piracy is bad and does have a material impact on these companies, but the demo scene that surrounded these groups really resulted in some great technical demos.As soon as you get past the initial intro and copy protection sequence you’re dropped in a pseudo-3D world. You can move around and “shoot” with the spacebar, but since this is supposed to be an adventure game the experience is a little disorienting at first. You’ll spend A LOT of time backtracking from location to location (and eventually island to island) so getting used to navigating this world using this weird view is important to being able to progress.One screen near the end of the game actually recreates the Drakkhen layoutApparently the overworld was inspired by the Infogrames’ Drakkhen, which came out a couple years earlier. I’m not sure if this was supposed to be a subtle plug or if they had game engine code they wanted to reuse, but it didn’t add to the experience in any positive way. Honestly navigating the overworld is tedious and boring and since you don’t even get a compass until later in the game it’s easy to get lost and disoriented.When you do find a building or location you can enter from the overworld the game switches to a more traditional layout. Don Jonz can walk around the screen, interact with (limited) objects, talk to characters, or use items he’s picked up. The game interface tries to create a “game within a game” feeling, so when you select an icon that represents a verb (GET, USE, SPEAK or LOOK) you see a “finger” hover over the “button” and click it.There’s typically very little you can do on most screens, and a lot of these screens are just filler. The LOOK option generally does nothing, and if you want to GET something, you have to wait for the “line of sight” indicator to show you there’s something there. If the indicator goes away you can use LOOK to bring it back up, but that’s basically all you can “LOOK” at in this game.The various locations and islands you visit show the civilizations that live there advance technically. The game starts in a more medieval setting, but advance to more modern, then futuristic, then back to ancient Egyptian. Each location has characters you can interact with to gain additional information about the world for Eternam as well as to advance the plot and get clues as to what to do next.Character interactions will cut to a closeup view to give you a cartoonish portrait. The style of the portraits actually reminded me a little of Dragon’s Lair so I’m wondering if the artists working on Eternam were influenced by this early arcade game.Dialogue selections can result in your death … so choose carefully!Dialogue trees are used to navigate conversations with the game’s cast of characters. Most of the options are padding but there’s usually one piece of useful information each character interaction will produce so you’ll have to pay attention and take notes.The way the story is written as well as some of the over-the-top character animations make this a pretty funny game to play through. There are A LOT of pop-culture references that haven’t really aged all that well, but if you grew up in the 80’s you should be able to follow along and identify what they’re making fun of.Instead of Uhura it’s Ooh-la-la … get it … GET IT! … ughYou’ll need to backtrack a ton throughout this game as many characters will send you on fetch quests in order to advance the plot. This wouldn’t be so bad if the overworld navigation wasn’t such a chore - but it is and you can’t really fast travel so you slog your way around fetching items.You’ll see this screen a lotThe game’s narrator Tracy pops in periodically to give you information about a location or to give you plot details. More often than not though she’ll pop in to announce that you’ve died … again. There are so many ways to die in this game you’d think it was made by Sierra Online. Conversations can go wrong, you can be too slow using an item, you can pick up the wrong item, you can randomly die in the overworld because roaming monsters are attacking you.Why do I have guns on the overworld but nowhere else?Weirdly enough you get attacked while navigating the overworld and you can shoot back using lasers or guns or something. These monsters drain your health, which automatically recovers when you idle, but can run out and result in death. These battles serve no purpose as you don’t gain experience, can’t level up and they don’t drop items or information. Basically it’s just another meaningless obstacle to slow this game down even further.WHY AM I ON THE MOON???I appreciate what they tried to do with this game, but the plot is extremely confusing and the backtracking is a grind. Overworld navigation is boring and pointlessly slow and it’s too easy to miss key items.There are A LOT of items you need to collect throughout the game, and this doesn’t include the red herrings and junk you can collect as well. Using items is initially confusing but you quickly realize you need to select them then click on USE. Some item combinations are bizarre and verge on moon logic.Although there’s an underlying plot about your arch enemy (Mikhail Nuke) taking over Eternam when you get there none of this really stuck with me and I just found his cut scenes pointless and intrusive. This game’s story drew heavy influence from Westworld - which I’ve never seen - and Infogrames scattered tidbits throughout that references French culture and history - which I know little to nothing about as well.This game is really weird. The controls are lousy, the story is bizarre and playing it is a bit of a chore. I do appreciate how different this game tried to be and though it may have fallen short with me and probably most North American gamers, Infogrames at least swung for the fences here. I enjoyed a lot of the humour they injected throughout, but honestly I won’t ever be revisiting this title and wouldn’t recommend it either.No fuss, no muss - just “the end”Game Information Game Eternam Developer Infogrames Publisher Infogrames Release Date 1992 Systems IBM PC (DOS), FM Towns Play Information How Long To Beat? 7 hours Version Played DOS via DOSBox-X Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 10 Story (25) 12 Experience (15) 6 Impact (10) 2   43% Gallery" }, { "title": "Kabul Spy (Sirius Software) - 1982", "url": "/blog/2023/02/07/kabul-spy/", "categories": "Let's Adventure!", "tags": "adventure, Sirius Software", "date": "2023-02-07 16:03:31 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Kabul Spy is an early graphic adventure game by Sirius Software. Released in 1982 it supports so...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Kabul Spy is an early graphic adventure game by Sirius Software. Released in 1982 it supports some pretty limited noun/verb parsing and a basic story following an American secret agent sent to Afghanistan to rescue Professor Paul Eisenstadt, who is being held by the KGB. Why the KGB you ask? When this game was published the Soviet Union hadn’t withdrawn from Afghanistan yet … isn’t history fun?A little backstory and off you goProgress through the game is pretty straightforward and there aren’t all that many twists and turns. You need to read EVERYTHING and write EVERYTHING down, otherwise you’ll lose track of where you’re supposed to be going and what phrases you should say to certain people (ex: \"SAY SALAM ALEIKOM\").For an Apple II game the graphics are what you’d expect, and there isn’t really any music or sound effects to accompany the experience. Seeing as this was basically written by one person (Tim Wilson) who likely did the graphics as well the game doesn’t look too bad and is easy enough to navigate.Sometimes hints are drawn into the background, which can be extremely hard to make out but I appreciate the effort put into this.It wasn’t a Grue that got me, it was a Grud …This game offers some variety as to how you can complete it, and there are also a number of ways you can die. Being a fan of Sierra games and the over-the-top death scenarios they create in certain games I did enjoy periodically killing my character to see how it was portrayed.In case you can’t tell … the endI had a couple of issues with the original disk files I found to play this game as I kept winding up in the prison cell and was unable to BRIBE GUARD. Finding a different version of the game solved that for me and I was able to get through this pretty quickly.Overall it wasn’t bad for what it is - a very early adventure game on the Apple II.Game Information Game Kabul Spy Developer Sirius Software Publisher Sirius Software Release Date 1982 Systems Apple II Play Information How Long to Beat? 1 hour Version Played Apple II via AppleWin Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 6 Story (25) 5 Experience (15) 5 Impact (10) 3   27% Gallery" }, { "title": "Leisure Suit Larry Goes Looking for Love (in Several Wrong Places) (Sierra On-Line) - 1988", "url": "/blog/2023/02/01/leisure-suit-larry-2/", "categories": "Let's Adventure!", "tags": "adventure, Sierra On-Line, SCI", "date": "2023-02-01 04:39:04 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Leisure Suit Larry Goes Looking for Love (in Several Wrong Places) is the second game in the Lei...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Leisure Suit Larry Goes Looking for Love (in Several Wrong Places) is the second game in the Leisure Suit Larry series of graphical adventure games, designed by Al Lowe and published by Sierra On-Line in 1988. Like its predecessor, Leisure Suit Larry in the Land of the Lounge Lizards, it was developed for multiple platforms, including MS-DOS, Atari ST and Amiga. It utilizes Sierra’s Creative Interpreter (SCI0) engine, featuring 16-color EGA graphics and a mouse-based interface for movement. The story continues the exploits of Larry Laffer, who becomes stranded on a tropical island during an ill-fated vacation.Dr. Nonookee … get it …This game starts off sort of where Leisure Suit Larry in the Land of the Lounge Lizards left off. You’re mowing the lawn of the woman you hooked up with at the end of that game only to find out you may have jumped the gun a bit (she wants nothing to do with you). Eve doesn’t really know how you are or why you’re at her house and tells you to get your shit and go.The story proceeds linearly as you buy a lottery ticket that turns out to be a winner, bumble your way onto a dating game show and win the dream cruise, end up deserted on an island and end up saving the island natives from the mad scientist there.Early on you’re presented with a subplot that has the KGB following you around as you’ve accidentally ended up with some microfilm the mad scientist wants (for … reasons).We’re all adults here … let’s crank up the raunchiness to 11Unlike its predecessor, this game uses the new SCI engine which has full mouse support. Honestly the mouse is just a distraction as you really only use it to interact with the menu bar occasionally, which you can do more quickly using hot keys anyway. SCI0 games still relied on the text parser to interact with the game world, but as soon as you start typing all character movement stops so you don’t have to time your input around a moving Larry.Having the extended color palette (16 glorious colors!!!) and increased screen resolution (320x200 pixels!!!) allows the Sierra artists to really take full advantage of the hardware available at the time. Though I played through the SCI VGA remake of Larry 1, other games of this time such as King’s Quest really looked pale in comparison.Clearly this title has learned a lot from the previous AGI Sierra titles as the text interactions tend to be more informative when you make a mistake, and the typical LOOK verb you use helps direct your attention a lot more effectively to the items on screen you’d want to interact with. Though the text parser era was drawing to a close by this point, I still really enjoy this method of interacting with a game and kind of miss the jokes programmers built into these games to entertain you when you typed something stupid.All the cool kids write their games using assembly!SCI games were optimized to use sound cards when available, so the sound effects and music are well done. Other than the main theme though I don’t really remember anything standing out so I guess they did a good job of keeping the background effects in the background :POn the negative side, this game’s puzzles get to be frustratingly vague and can require a LOT of backtracking. I died A LOT trying to find a way off the plane, and when I did find it you had to position your character at exactly the right spot so the text parser would allow you to interact with the hidden third door on the lower part of the screen. You also have a limited time to get yourself off the plane so you need to reposition yourself repeatedly and keep retrying the text inputs only to die and have to reload and do it over again.Getting yourself past the KGB on the island is also a chore as you can’t go directly to any of the locations where parts of the fetch-quest puzzle are located. You have to wait through an animation of Larry bumbling through the forest over and over and over until you get where you want to be. If you forget an item, you have to go back and repeat this process while it cycles through the locations with this stupid scripted sequence over and over.Having unintuitive puzzles, dead ends and repeated game overs are just staples of these early Sierra Online games so I can’t really complain about that. I knew parts of this game would be frustrating and even with a walkthrough it … was frustrating.Timing various interactions with a text parser (ex: swinging from one vine to the next over a river of piranas), or navigating a narrow path through quicksand where a one pixel misstep results in death requires frequent incremental game saving and loading.Once you finally figure out how to make a molotov cocktail and drop it into the volcano, the (LONG) endgame sequence is triggered and you end up with the girl of your dreams. The game ends with you marrying the island girl and you’re both naked on the beach.I don’t know if there’s a final scene or if my game froze, but this was as far as I got … and I managed to be missing 1 point from having a perfect score :/These old text parser games are a lot of fun as great care has been put into developing the stories and characters. Al Lowe really likes to focus on juvenile humor that is essentially dick and fart jokes, but they’re still funny and make these games endearing.If crude, suggestive, innuendo-filled games are your thing, you’ll likely enjoy this game as well as the rest in the Leisure Suit Larry series of games.Game Information Game Leisure Suit Larry Goes Looking for Love (in Several Wrong Places) Developer Sierra On-Line Publisher Sierra On-Line Release Date October 1988 Systems DOS, Amiga, Atari ST Game Engine SCI Play Information How Long To Beat? 4 hours Version Played DOS via ScummVM Notes Walkthrough, Manual ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 14 Story (25) 16 Experience (15) 10 Impact (10) 5   64% Gallery" }, { "title": "Sam & Max Hit the Road (LucasArts) - 1993", "url": "/blog/2023/01/23/sam-and-max-hit-the-road/", "categories": "Let's Adventure!", "tags": "adventure, LucasArts, SCUMM", "date": "2023-01-23 04:42:18 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Sam &amp; Max Hit the Road is a graphic adventure video game released by LucasArts, based on the...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Sam &amp; Max Hit the Road is a graphic adventure video game released by LucasArts, based on the comic characters of Sam and Max, the “Freelance Police”, an anthropomorphic dog and “hyperkinetic rabbity thing”. The characters, created by Steve Purcell, originally debuted in a 1987 comic book series. Based on the 1989 Sam &amp; Max comic On the Road, the duo take the case of a missing bigfoot from a nearby carnival, traveling to many American culture tourist sites to solve the mystery.LucasArts really set the tone for their adventure games with Maniac Mansion as having excellent writing, character development, pacing and elements of humor injected throughout. Sam &amp; Max appeals to a slightly more mature audience as there are a lot of pop culture references and excessive cartoon violence that appeals to the 13+ crowd … as well as nostalgic middle aged men :PThe plot follows our heros - the freelance police - on a trek across America to solve the mystery of the disappearing sasquatch(es). The story unfolds across multiple locations where you interact with various characters via a menu system used to select topics via pictograms.Unlike some adventure games the number of topics you can discuss with most characters is extremely limited. This helps to keep you from going down endless dead ends like you’d find in games such as Cruise for a Corpse, where endless backtracking and growing topic lists make plot advancement tedious.In addition to interacting with a handful of characters, you also have to collect a number of items to satisfy various fetch quests. Several times throughout the game you’ll find yourself having to combine multiple items to complete a certain task, and though this can be difficult the game will drop enough hints to steer you in the right direction.Combine tar, mammoth hair and a toupee … boom … instant bigfoot costume!Game progression is pretty linear, and though there are some mildly frustrating item combination puzzles, overall the difficulty here is low. You can’t get yourself into an unwinnable situation nor can you miss an item you can’t circle back for later. This is typical of all LucasArts adventures though so I’m really only calling it out for consistency.This minigame is pretty stupid and just feels forcedA couple mini-games are included, with the most annoying being the highway jump/duck game. I don’t really get what the point is or if there’s any repercussion to failing, but you have to slog through this before you can take the highway to explore additional locations on the map. Honestly this may be an optional mini-game … all I know is I played it once, hated it and never played it again :PTo navigate between game locations you select them from a map of the USA. There are only a handful of locations you need to explore, and most require backtracking to gather info or items. Each location is pretty small - maybe 3-5 “screens” - but they all share a goofy cartoonish style that makes exploring them enjoyable.The only real “issue” I ran into while playing this game was getting stuck while on the view finder and the cleaning robot’s brain. Both times I didn’t realize you could exit out of the view by clicking the left/right mouse buttons at the same time. Since I was playing on a Macbook the touchpad wasn’t registering these button presses so I really thought there was an issue with ScummVM.I ended up just remapping the mouse buttons to keyboard keys (in ScummVM) at which point everything worked as expected.Click left/right to exit … or wait forever …This game is a lot of fun if you enjoy a good story. The comedic undertones of every interaction made the character and plot development a lot more enjoyable and the lack of dead ends and plentiful clues set a good pace. Though there were some weird item combinations to deal with, it didn’t feel at any point like the game was throwing meaningless puzzles your way just to pad things out. It’s a pretty short game with not too many locations to visit and a plot that progresses in a linear fashion.Although the graphics are a bit dated at this point, I don’t think anyone picking up this title today wouldn’t enjoy the experience. I would definitely recommend this title to anyone looking for a casual adventure game to pick up and sink a couple hours into.The end credits are interactive and let you shoot the dolls like you would at a carnival … fun!Game Information Game Sam &amp; Max Hit the Road Developer LucasArts Publisher LucasArts Release Date November 1993 Systems DOS, Mac OS, Windows, Amiga Game Engine SCUMM Play Information How Long To Beat? 5.5 hours Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 18 Story (25) 22 Experience (15) 12 Impact (10) 7   84% Gallery" }, { "title": "Mean Streets (Access Software) - 1989", "url": "/blog/2023/01/19/mean-streets/", "categories": "Let's Adventure!", "tags": "adventure, Access Software, Tex Murphy", "date": "2023-01-19 13:05:07 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Mean Streets is set in a dystopian cyberpunk neo-noir world is the first in the series of Tex Mu...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Mean Streets is set in a dystopian cyberpunk neo-noir world is the first in the series of Tex Murphy mysteries. In 1998, Mean Streets was remade as Tex Murphy: Overseer, which I’ll get to eventually :).You plays the role of Tex Murphy, a down-and-out private investigator living in post-apocalyptic San Francisco. Tex is hired by a beautiful young woman named Sylvia Linsky to investigate the death of her father, Dr. Carl Linsky, a professor at the University of San Francisco. Prior to his death, Carl would not talk to his daughter about the secret project he was working on, and days later, he was seen falling off the Golden Gate Bridge. Sylvia suspects murder, but the police say it was routine suicide.This can either be considered copy protection or saving a few bucks on programming a more engaging intro to the gameTo help get him started, Tex is given $10,000 and a few leads. The player is referred to the game’s manual for a list of their leads. If you’re not just following a walkthrough you won’t actually be able to do anything meaningful right out of the gate as you’ll have no idea how to begin the case.Travel involves navigating to various nav codes you’ll learn from characters as you question themMoving around the game world is done by means of a lander, which is sort of a flying car (similar to what you’d see in Blade Runner). You enter a nav code, then either pilot your way there manually or use the auto-pilot. There’s a Warp mode that’s supposed to get you from place to place faster, but I could never get it to work … so I just found another way to speed the game up instead.“Warp Mode” for me is just jacking up the CPU speed once the auto-pilot kicks inThe story progresses by means of various text dumps, faxes and conversation screens. That’s really all there is to it and it’s your job as Tex Murphy to solve the case based on the story elements you slowly uncover by interrogating people and exploring a handful of locations.Interrogation is also pretty straightforward. You have the option to ask questions, offer a bribe, or threaten the person with a little physical violence. Questioning involves asking about keywords, which will result in more information being provided or a generic response indicating you’ve picked the wrong keyword for that user. Note that you have to watch your spelling as the parser is pretty unforgiving and you may not realize a spelling mistake is what resulted in a generic response.In addition to navigating to known nav codes to interrogate people, you also have access to vid phone you can call two people on: Vanessa and Lee. Vanessa can be asked various keywords to try and uncover additional information and nav codes, but Lee will require bribes. If either of them is able to produce information though it will come in the form of a fax.Funny how in 1989 getting a fax was considered a futuristic form of communicationSome nav codes won’t lead you to a conversation, but instead to a single room you can move around and investigate. As you walk around you’ll find “areas” that you can interact with via a verb menu at the bottom of the screen. Here you can LOOK, GET, MOVE, OPEN, ON/OFF and TASTE select items to potentially uncover clues. I would HIGHLY recommend you MOVE EVERYTHING. This threw me off initially as you need to find keys in basically every location and they tend to be under or behind something unexpected (like a sandwich in one case).You will also need to GET everything you can. Make sure you LOOK at these items first since some contain story details or nav codes. You can go into your inventory later to examine these items too, but finding the right one can be tedious.Note that the inventory is also where you can pawn items. This is how you get money for bribes, so make sure to pick everything up that isn’t bolted down as you’ll need all the money you can get! You can reclaim items that you’ve pawned as well, but honestly I never had to do this so I’m not sure what purpose it really serves (maybe you can pawn key story items?).As you progress through the case you’ll learn you need to find the pass codes and key cards for 8 different scientists. These can be used to access various computer terminals to uncover more of the story from these scientists to help move the story along. Make sure you take notes as you read everything since you’ll need to extract keyword details to ask characters about to continue to progress.Sometimes I can convince you to give me more info by asking less than nicelyIf you question a character about a keyword and they refuse to give you an answer (but aren’t indicating they don’t know anything) you can threaten them. This involves punching them in the gut and then having them cough up the info. Trying to do this on a character that isn’t being cagey will just cut to a screen of you being thrown out of the area the conversation was taking place and you’ll have to go back and resume the discussion.Apparently this world also has mutants in itI didn’t really understand why the world Tex is exploring contains mutants, but there are a couple you’ll interact with. There’s allusions to some characters hating freaks so I’m guessing it’s woven into the plot somehow, but I must have missed it. I’m sure it would have added to the story but clearly I wasn’t paying close enough attention …Before you can investigate certain areas or interrogate certain charters you sometimes have to go through an arcade sequence. This is always 2 screens of baddies with guns trying to shoot you. They only stand and shoot so you can duck down and they’ll never hit you. There are obstacles you can’t shoot through when you’re crouched though so you’ll need to time when you return fire so you can stand up and shoot and keep moving to the right.You have limited ammo, however you can find more in the various areas you investigate so you shouldn’t have to worry about running out.“Sonny Fletcher” is the important bit here … now go ask everyone about himIt is a bit frustrating that there’s no real emphasis placed on any of the keywords you’re supposed to keep track of to use in your investigation, so you’ll need to take some pretty thorough notes while you play. Most proper names, business names or organization names should be written down as you’ll likely need to follow up on them with each and every character - so plan to backtrack as you learn new keywords.Eventually you’ll work your way to Alcatraz where the shadowy MTC organization you’ve been chasing is headquartered. The endgame involves using the 8 key cards and pass codes in a random order (the game will prompt you). Assuming you had all this written down and can get the info typed in correctly in 60 seconds, you win the game.If looks could kill indeed … this is some good VGA rendering right hereThough your interactions with the game world are pretty limited, for a late 80’s game this is actually quite well done. I would consider this to be more in line with what you’d call a “Visual Novel” nowadays though. The arcade sequences are pretty repetitive and there isn’t really any increase in the challenge or complexity - they’re just in there to kill time.Autopilot is a killer feature since you do SO MUCH backtracking and navigation, and though you have the option to pilot the lander yourself this gets old REAL fast.As far as detective stories go this is a pretty good one. The pacing is good, the interfaces are intuitive and though moving from conversation to conversation via the navigation system can be a bit of a drag, it’s not that bad overall. I’m a curious to see how the re-implementation of this game changes up these elements, but I guess I’ll find out when the time comes :)Game Information Game Mean Streets Developer Access Software Publisher Access Software Release Date 1989 Systems DOS, Amiga, Atari ST, Commodore 64 Game Engine   Play Information How Long To Beat? 8.5 hours Version Played DOS via DOSBox-X Notes Walkthrough, Manual ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 12 Story (25) 16 Experience (15) 7 Impact (10) 5   57% Gallery" }, { "title": "Hopkins FBI (MP Entertainment) - 1998", "url": "/blog/2023/01/16/hopkins-fbi/", "categories": "Let's Adventure!", "tags": "adventure, MP Entertainment", "date": "2023-01-16 06:24:01 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Hopkins FBI is a 1998 adventure game from MP Entertainment, most famous for very large (at the t...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Hopkins FBI is a 1998 adventure game from MP Entertainment, most famous for very large (at the time) amounts of gore. Players assume the role of FBI agent Hopkins, who is on the trail of a criminal mastermind named Bernie Berckson. The pursuit takes the player through a variety of locations, including the FBI headquarters in a modern fictional city, a tropical island, and a submarine base.They promised they wouldn’t hurt the hostages … LIARS!When this game first came out I actually bought a copy directly from MP Entertainment, which came in the mail on a CD they’d burned and attached a press-on label to. Since I was 17 when this game came out I can clearly remember being drawn to it because there were promises of excessive violence … and some nudity …The gameplay is pretty standard Point-and-Click Adventure fare at this point. You move Hopkins around the screen moving the mouse over everything to see if the cursor changes. If it does, right-clicking will cycle through various interaction options (Take, Read, Move, etc) depending on what the object is.Case closed because he’s clearly dead, right …. right?You start off watching a convict (Bernie) be executed, but somehow after seemingly being killed the lights go out and he escapes from prison. I can’t remember based on the story if you were the one that initially arrested him and put him on death row, but apparently he’s out to enact some complex revenge on you and you need to track him down and stop him.Bernie’s killing spree involves young women, each with a clue at their body leading to the location of the next victim. Moving around the first area of the game is pretty straightforward and is reminiscent of [Police Quest 1(/blog/2022/03/15/police-quest-1/), though you can just click on the location you want to go to and it’ll auto-drive you there.When you first leave your apartment the dispatcher will tell you there’s a bank robbery in progress, so you go there first. Interacting with characters is also standard fare with dialogue options presented as a list you choose from, with most options giving generic details and a key few helping to advance the plot.The switchboard operator’s performance is truly abysmalThe voice acting in this game is really something to behold. I’m not sure if they just got people around the MP Entertainment office to do one take of each character’s lines or what, but it really is bad. Like … really bad.Not having voice acting at all would have exponentially improved the overall experience of playing this game.What is interesting about this game though is they include some arcade-style elements to break up the monotony. While hunting for clues (using the key from your girlfriend you literally just shot and killed) on one of the office computers you find a Breakout game you can play. It’s not great … but it’s there.The most painfully slow Breakout clone can be found within the game for … reasonsThere are some interesting elements to the story, though the pacing seems a bit off. You’re trying to find Bernie by finding murdered women, who give you clues to other murdered women’s locations … but the motivation isn’t super clearn. The last clue takes you to the shooting range, where you murder your girlfriend by accident then … go back to her apartment to watch the video attached to her corpse.My grieving process appears to involve looting the dead body and immediately moving onAt one point you are killed and wind up in purgatory, which you need to escape from to be reincarnated in your body so you can continue the investigation. This actually ends up being a plot point as Bernie has figured out how to leverage this process to do something with clones and have them take his place (or something …).Grammar and spell checking was a lot harder in the late 90’sThis purgatory/clone thing was kind of interesting but wasn’t fully baked and winds up being really confusing. There is clearly a seed of a good idea planted in this game, but the execution fell flat.I’ll take optimized navigation to avoid tediumMy playthrough used ScummVM, which didn’t implement the Wolfenstein 3D styled maze/shooter in the underwater base during the final chapter. After watching a long play on YouTube though (Hopkins FBI LongPlay 2 of 2)I don’t think I really missed out on much, and it allowed me to move through this portion a lot faster.Yeah this looks cool, and is … once … then it just drags on and onThe endgame really fells mashed together. You need to resurrect your dead girlfriend, which involves you creating and killing a clone of yourself to go back to purgatory to find Samantha and convince her to come home. You then need to both push a button to open a ground hatch so you can jump through and have the final confrontation with Bernie play out as a scripted (boring) sequence.This might have been fun to play, but just sit back and watch because that’s it for this gameAfter you kill Bernie, you’re back in your apartment typing and …. that’s it. What about all those murdered women? What about the fact that you killed your own girlfriend then fled the scene? What about the fact that this entire underwater lab exists?The end … though the credit sequence here really makes no sense given the game you just played …This is one of the earliest examples (for me personally) of a game being released for Linux. At the time I was branching from Windows and was starting to play with the early Linux distributions available at that time. Though I didn’t buy this game for Linux, I think the fact that it was being made available for Linux helped draw me to the title.The game also attempted to up the musical quality by including a couple songs by The Troggs, but the way they looped these just makes them annoying. The music throughout this game is actually extremely distracting and only enhances the experience in rare occasions.Overall this was an “OK” adventure game, but I wouldn’t really recommend it.Game Information Game Hopkins FBI Developer MP Entertainment Publisher MP Entertainment Release Date July 16, 1998 Systems OS/2 Warp, Windows, Linux Game Engine Hopkins Play Information How Long To Beat? 4.5 hours Version Played Windows via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 7 Story (25) 13 Experience (15) 9 Impact (10) 4   47% Gallery" }, { "title": "Will Upgrading My MongoDB Server Version Break My Application?", "url": "/blog/2023/01/13/will-upgrading-my-mongodb-server-version-break-my-application/", "categories": "MongoDB", "tags": "upgrade, drivers, mongodb", "date": "2023-01-13 09:21:00 -0500", "snippet": "Upgrading components is an important part of maintaining a healthy application. The MongoDB Server is continually being developed to include new features and functionality, as well as to fix bugs, ...", "content": "Upgrading components is an important part of maintaining a healthy application. The MongoDB Server is continually being developed to include new features and functionality, as well as to fix bugs, potential vulnerabilities and attack vectors. To ensure users are using the “latest and greatest” whenever possible, MongoDB publicizes Software Lifecycle Schedules to make it clear when various components will reach end of life (EOL).The applications that you’ve built that connect to MongoDB are using MongoDB Drivers, which at the time of writing include official releases for C, C++, C# (.NET), Go, Java, Node.js, PHP, Python, Ruby, Rust, Scala and Swift.Note that “break my application” is extremely generic and is meant to encompass a category of issues such as “not being able to connect”, “application can’t start at all”, “can’t return data”, etc.I’m using the Stable API!The MongoDB Stable API was created to allow you to upgrade your MongoDB server at will, and ensure that behavior changes between MongoDB versions do not break your application.The Stable API provides long-term API stability for applications and supports more frequent releases and automatic server upgrades. This allows your applications to take advantage of rapidly released features without risking backwards-breaking changes.If your application is already using the Stable API via your current Driver you shouldn’t have to worry further about compatibility as it relates to a MongoDB Server upgrade.What version of MongoDB does my Driver support?Each official Driver listed above will contain a compatibility matrix (see the Node.js Compatibility as an example), which should clearly show you what MongoDB Server versions are actively being tested against which versions of the Driver. Note that at the time of writing MongoDB does not have an official Support Policy for Drivers. If this changes in the future it would be reflected in the Changes to MongoDB Support PolicyNeither the Legacy Support Policy nor the Software Support Policy call out drivers directly. As a result the best guidance to follow is to ensure the version of the MongoDB Driver your application is using is actively being tested against the version of the MongoDB Server you’re using.If you’re using any Community Supported Drivers, clarification regarding support and compatibility should be directed to those communities directly.What if I’m using a library that includes a MongoDB Driver?Many popular libraries, ORMs and ODMs depend upon an official MongoDB Driver. Under most scenarios package management will be used to manage the library’s dependencies, which can be used to determine which version of the MongoDB Driver is being used.For example, let’s consider some of the most popular libraries and ORMs. As the source code for most libraries are hosted on GitHub (or some other service) a similar strategy can be followed to identify what version of the MongoDB driver is being used for a given version of the library.Many libraries use Semantic Versioning, so if you’re unsure how to determine if a given constraint (ex: &gt;=2.4.1', '&lt;3.0.0') would include a version of a Driver (ex: 2.18.2), the Semver check tool can help.MongoidMongoid is the ORM (or ODM) of choice for Ruby on Rails developers. As it is developed and maintained by MongoDB directly a compatibility page is available that can be used to identify what version of the library can be used with which versions of the MongoDB Server.As a Ruby library however, dependencies are managed using Bundler, which implies either a Gemfile or &lt;library&gt;.gemspec will be present and should contain dependency and version details. Since the source code is available, if we wanted to identify what version of the Ruby Driver an older version of Mongoid (ex: 5.2.0) required we could do the following: Navigate to the appropriate branch/tag on Github Open the mongoid.gemspec file Identify what versions of the driver are pinned (\"mongo\", ['&gt;=2.4.1', '&lt;3.0.0'] in this case)Note that your application’s Gemfile.lock would indicate exactly which version of the Driver is being used, however if you don’t have access to this the above can help point you in the right direction.MongooseMongoose provides a straight-forward, schema-based solution to model your Node.js application’s data. It includes built-in type casting, validation, query building, business logic hooks and more, out of the box.As a Node.js library, dependencies are managed using Node Modules, which would indicate a package.json file would be present that describes the dependencies and version details. Since the source code is available, if we wanted to identify what version of the Node.js Driver an older version of Mongoose (ex: 4.9.9) required we could do the following: Navigate to the appropriate branch/tag on Github Open the package.json file Identify what versions of the driver are pinned (\"mongodb\": \"2.2.26\" in this case)Note that your application’s package-lock.json would indicate exactly which version of the Driver is being used, however if you don’t have access to this the above can help point you in the right direction.Spring Data MongoDBSpring Data MongoDB is part of the umbrella Spring Data project which aims to provide a familiar and consistent Spring-based programming model for new datastores while retaining store-specific features and capabilities. The Spring Data MongoDB project provides integration with the MongoDB document database. Key functional areas of Spring Data MongoDB are a POJO centric model for interacting with a MongoDB DBCollection and easily writing a Repository style data access layer.As a Java library, dependencies are managed using Maven, which would indicate a pom.xml file would be present that describes the dependencies and version details. Since the source code is available, if we wanted to identify what version of the Node.js Driver an older version of Spring Data MongoDB (ex: 3.0.9.RELEASE) required we could do the following: Navigate to the appropriate branch/tag on Github Open the pom.xml file Identify what versions of the driver are pinned (&lt;mongo&gt;4.0.6&lt;/mongo&gt; in this case, which is a variable referenced later to identify the appropriate driver version) &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver-core&lt;/artifactId&gt; &lt;version&gt;${mongo}&lt;/version&gt; &lt;/dependency&gt; What if my version of the Driver isn’t in the compatibility matrix?The most important thing to understand about Driver versions when it comes to MongoDB is that anything your current Driver was doing successfully while connected to MongoDB X.Y should continue to work correctly while connected to a newer MongoDB Server. This statement should be accurate if your upgrade path is taking you from a single major release to the next highest release (ex: 5.0 to 6.0). Backwards breaking changes that would prevent you from successfully connecting to a MongoDB Server from your current application occur extremely rarely. One example that is worth noting involves the removal of Legacy Opcodes in MongoDB 6.0. If you have a version of a MongoDB Driver that predates the support for MongoDB 3.6 and the OP_MSG opcode, trying to interact with MongoDB 6.0+ Servers via these antiquated drivers will result in only errors being thrown due to unknown opcodes being used.As a general rule of thumb, assuming the only change to your application stack is an upgraded MongoDB Server version: Your application should continue to operate without issue You should plan to upgrade to the latest driver to ensure you have the latest features and fixes You should always test your MongoDB Server upgrades and application compatibility in a non-production environment first prior to upgrading your production environment and application(s)" }, { "title": "Live Migration of Sharded Clusters to MongoDB Atlas could result in <tt>TooManyLogicalSessions</tt> Errors", "url": "/blog/2023/01/12/live-migration-of-sharded-clusters-can-result-in-toomanylogicalsessions/", "categories": "MongoDB", "tags": "mongodb, sharding, diagnostics, troubleshooting", "date": "2023-01-12 15:14:50 -0500", "snippet": " The following is more of a diagnostic journey than anything else, and does not reflect a current issue with MongoDB Atlas.While I was still working as a Technical Services Engineer at MongoDB in ...", "content": " The following is more of a diagnostic journey than anything else, and does not reflect a current issue with MongoDB Atlas.While I was still working as a Technical Services Engineer at MongoDB in 2021 a small number of customers were reporting that their applications would start throwing errors similar to the following after upgrading from MongoDB 3.6 to 4.0: Command failed with error 261: 'cannot add session into the cache' on server xxx.yyy.zzz.com:27017. The full response is { \"ok\" : 0.0, \"errmsg\" : \"cannot add session into the cache\", \"code\" : 261, \"codeName\" : \"TooManyLogicalSessions\" }When this error would occur, no further operations could be run against that shard until the mongod process was restarted. Clusters would not immediately exceed their logical session limit and it could take days or weeks for some clusters to reach this failure condition depending on their level of activity.After much investigation the issue boiled down to a confluence of the following scenarios: A Sharded Cluster was Live Migrated to MongoDB Atlas The original sharded cluster (correctly) had more than 1 chunk associated with the config.system.sessions collection When Live Migrate was finalizing, the config.system.sessions entry was removed from config.collections but only one (of many) chunks were removed from config.chunks… but why?Technical DetailsTo provide causal consistency, MongoDB 3.6 introduced client sessions. The underlying framework used by client sessions to support causal consistency (as well as retryable writes) are server sessions.Per the Driver Sessions Specification, starting with MongoDB 3.6, MongoDB Drivers associate all operations with a server session (with the exception of unacknowledged writes). The logic defined in this spec regarding “How to Check Whether a Deployment Supports Sessions” states that: If the TopologyDescription and connection type indicate that the driver is not connected to any servers, OR is not a direct connection AND is not connected to a data-bearing serverthen a driver must do a server selection for any server whose type is data-bearing. Server selection will either time out or result in a TopologyDescription that includes at least one connected, data-bearing server. Having verified in step 1 that the TopologyDescription includes at least one connected server a driver can now determine whether sessions are supported by inspecting the TopologyType and logicalSessionTimeoutMinutes property.With MongoDB 3.6, the hello command when targeting a sharded cluster will only return the logicalSessionTimeoutMinutes under two conditions: (Starting with MongoDB 3.6.0 via SERVER-31777) When the featureCompatibilityVersion is set to “3.6” AND (Starting with MongoDB 3.6.9 via SERVER-37631) The cluster has a valid system.sessions collectionStarting in MongoDB 4.0, an hello command when targeting a sharded cluster will always return logicalSessionTimeoutMinutes, as the featureCompatibilityVersion test has been removed (via SERVER-32460). The test for a valid system.sessions collections was not included in MongoDB 4.0.As a result, if the cluster contains a “broken” system.sessions collection the following can occur: On MongoDB 3.6, hello doesn’t return logicalSessionTimeoutMinutes which causes the driver to determine that sessions are not supported on this cluster, resulting in logical sessions not being used On MongoDB 4.0, hello always returns logicalSessionTimeoutMinutes, resulting in the driver enabling logical sessions. If a “broken” system.sessions collection exists, the sessions are not persisted/expired properly which can result in cluster failure once the maxSessions threshold (default: 1,000,000) is reached.Screenshot from a tool used to chart FTDC telemetryWhat is the system.sessions collectionThe system.sessions collection stores session records that are available to all members of the deployment.When a user creates a session on a mongod or mongos instance, the record of the session initially exists only in-memory on the instance. Periodically, the instance will sync its cached sessions to the system.sessions collection; at which time, they are visible to all members of the deployment.In a sharded cluster, the system.sessions collection is sharded. When adding a shard to the sharded cluster, if the shard to add already contains its own system.sessions collection, MongoDB drops the new shard’s system.sessions collection during the add process.What is a “broken” system.sessions collectionThe system.sessions collection is expected to be sharded, however in some cases, the system.sessions collection may be mistakenly created on the Config Servers as an unsharded collection. Each sharded node in a cluster expects to be able to write documents to the sessions collection, which is why it is necessary for the sessions collection to be sharded.When the system.sessions collection is “broken” the LogicalSessionCache* threads will emit log messages such as the following:// Primary Shard Logs2021-03-12T19:32:51.551+0000 I CONTROL [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Collection config.system.sessions is not sharded.2021-03-12T19:32:51.556+0000 I CONTROL [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Collection config.system.sessions is not sharded.// Config Server Logs2021-03-12T18:32:31.725+0000 I CONTROL [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Not primary while running findAndModify command on collection config.locks, will try again at the next refresh intervalHow does the system.sessions collection “break” in AtlasAt the time of investigation the working hypothesis was that there was a defect associated with the Live Migration process for Sharded Clusters. When the Live Migration tool was run the following was observed: When the config.system.sessions collection is migrated it is not initially sharded There are, however chunks associated with the config.system.sessions collection Reviewing the oplog for the Config Server replica set shows an entry where the config.system.sessions collection is removed from the config.collections collection along with an entry in the config.chunks collection: { \"ts\" : Timestamp(1616135732, 11), \"t\" : NumberLong(2), \"h\" : NumberLong(\"-2957928371374723467\"), \"v\" : 2, \"op\" : \"d\", \"ns\" : \"config.chunks\", \"ui\" : UUID(\"933eed1e-f9a6-4ad7-93e5-f799e0d41484\"), \"wall\" : ISODate(\"2021-03-19T06:35:32.598Z\"), \"o\" : { \"_id\" : \"config.`system.sessions`-_id_MinKey\" } }{ \"ts\" : Timestamp(1616135732, 12), \"t\" : NumberLong(2), \"h\" : NumberLong(\"7339349496984671905\"), \"v\" : 2, \"op\" : \"d\", \"ns\" : \"config.collections\", \"ui\" : UUID(\"703ea46b-44c6-435c-8dc3-91f9cf287c08\"), \"wall\" : ISODate(\"2021-03-19T06:35:32.607Z\"), \"o\" : { \"_id\" : \"config.`system.sessions`\" } } Based on this observation it appeared that during the live migration’s temporary data/metadata cleanup process (for a sharded cluster), the config.system.sessions collection was being removed from the config.collections collection which makes it appear to the cluster that config.system.sessions is unsharded.The LogicalSessionCacheRefresh thread should automatically recreate the collection as sharded in this case, however as there are still chunks associated with the collection this process failed and was retried indefinitely.Identification &amp; MitigationAny sharded cluster can be tested for an incorrectly (“broken”) configured config.system.sessions collection (caused by the suspected deficiency in Live Migrate or otherwise) by connecting (via the mongo or mongosh shell) and running:db.getSiblingDB('config').system.sessions.stats()['sharded']If the result of the above command is NOT true (either false or blank), performing the following actions on the cluster would address the issue:// Connect to the PRIMARY member of CSRS and ALL Shard PRIMARY members// and run the following commanddb.system.sessions.drop();// Connect to a single mongos configured for your cluster and run the followingdb.getSiblingDB('config').chunks.remove( {ns:'config.system.sessions'})db.adminCommand(\"flushRouterConfig\")Hopefully you’ll never find yourself in a situation such as the one described above, but if you do this guide may be useful for getting your cluster back up and running." }, { "title": "Cruise for a Corpse (Delphine Software International) - 1991", "url": "/blog/2023/01/12/cruise-for-a-corpse/", "categories": "Let's Adventure!", "tags": "adventure, Delphine Software International, CruisE, Erbe Software, Interplay Entertainment, U.S. Gold", "date": "2023-01-12 14:52:45 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Cruise for a Corpse is designed as a murder investigation. The player assumed the role of Raoul ...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Cruise for a Corpse is designed as a murder investigation. The player assumed the role of Raoul Dusentier, a French police inspector invited to spend some time on Niklos Karaboudjan’s boat. Quickly after arriving, Karaboudjan is murdered, and the investigation begins.The characters on the ship all seem to have their own motives and suspicions. Set over ten and a half hours, Raoul starts his investigation to find out who killed Niklos.As soon as you start playing this game you are presented with a beautifully rendered backgrounds. When you start moving around Raoul moves fluidly, as if he were rotoscoped. You can move Raoul around and interact with the game world by bringing up an action menu whenever the cursor changes from an arrow.Depending on what you’re interacting with you’ll get a different context menu. Seems pretty straightforward right?Certain interactions will give you a close up view of whatever you’ve clicked on. Again, this has all been beautifully drawn. Some close ups will also allow you to click around and interact with elements of whatever it is you’re looking at to learn more about it.Since this game is essentially a murder mystery, you have to go around speaking to the passengers and learning about them, their motives and their relationships. As you find items by searching the ship or uncover new talking points, you can go back and try those dialogue options out on other passengers to see if you learn anything new.Sounds like fun right? Ok … I gotta come clean here. Visually, this game looks great, but once you start playing it you’ll quickly realize it is not fun. I mean at all. Everything in this game is tedious.Most dialogue options are dead ends with most charactersNot only do you have to constantly backtrack every time you talk to someone and uncover more talking points, but if you happen to get lucky enough to find something to pick up, you then have to go ask everyone about that item to see if it advances the story.I hate this fucking clock“Advance the story” comes in the form of the in-game clock popping up to tell you some time has passed. This means a new story point can now be explored, which means go back over the ship again trying to figure out what’s different. Who’s moved to another room? Who’s left a room? Does searching something that was previously empty now have something in it you need?Anyone playing this game today that can solve this puzzle without a walkthrough is my new heroAny puzzles in the game are just item combination puzzles, and they generally don’t make sense. The game uses many references to French and Belgian pop culture … which as a Canadian player went right over my head … so I don’t know if these players would have an easier go here.The doll kinda creeped me outAll I can really promote about this game is the artwork. It really is beautifully drawn, and the overarching story is pretty good - but the game is just so boring to play. Thankfully you don’t have to physically walk to every location as you have a map you can use to quick travel, but this only makes the experience the tiniest bit less frustrating or repetitive.Thank god it’s finally overAfter what feels like an eternity you finally get to the endgame where you have to identify the killer. After doing so the end sequence plays out and you can exit out of this game once and for all, breathing a sigh of relief knowing you never have to revisit it again.The game engine had a lot of promise and could have made some great adventures, but unfortunately Cruise for a Corpse is all we got.Game Information Game Cruise for a Corpse Developer Delphine Software International Publisher Erbe Software, Interplay Entertainment, U.S. Gold Release Date 1991 Systems Amiga, Atari ST, DOS Game Engine CruisE Play Information How Long To Beat? 3 hours Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 8 Story (25) 10 Experience (15) 5 Impact (10) 4   39% Gallery" }, { "title": "My 2022 Reading List", "url": "/blog/2023/01/08/my-2022-reading-list/", "categories": "Personal", "tags": "books, audiobooks, reading", "date": "2023-01-08 20:18:16 -0500", "snippet": "I love to read, but I’ve found lately I have less time to actually sit down with a book. As a result I’ve been listening to a lot more audio books through Audible. Since I was able to get through m...", "content": "I love to read, but I’ve found lately I have less time to actually sit down with a book. As a result I’ve been listening to a lot more audio books through Audible. Since I was able to get through multiple books this year I figured I’d throw a quick summary together to cover what I’d “read”.The 4 Disciplines of ExecutionBy: Sean Covey, Chris McChesney, Jim Huling The 4 Disciplines of Execution is a simple, repeatable, and proven formula for executing your most important strategic priorities despite professional distractions. The authors suggest adopting the 4 Discipline. These disciplines are Focus on the Wildly Important; Act on Lead Measures; Keep a Compelling Scoreboard; Create a Cadence of Accountability. Through these disciplines, leaders can produce breakthrough results. These significant improvements require a commitment to these disciplines and effective integration into the wider team.I found this book really insightful when it came to reframing goals and defining them in ways that are easier to measureThis Is How They Tell Me the World EndsBy: Nicole Perlroth This Is How They Tell Me the World Ends (2021) takes a deep dive into the ongoing global cyberweapons arms race. It explains how the unregulated market for destructive weapons began, how nations are buying and using these weapons, and why they represent a threat to our immediate future.This one pulled back the curtain on the impact of zero day exploits as weapons.Sidney Crosby: The Rookie YearBy: Sidney Crosby In this Audible show Crosby describes what it was like trying to live up to the hype, stepping onto NHL ice for the first time, playing with legends like Mario Lemieux, and feeling the weight of a nation throughout his early career.If you’re a hockey fan this is worth the four hours it takes to listen to it. Crosby’s story is pretty interesting.Thinking in BetsBy: Annie Duke Professional poker player Annie Duke explores how we can all become better decision-makers in an uncertain and challenging world. She helps us understand how to disentangle the role of luck and skill in determining outcomes, ultimately helping us make better bets that lead to better outcomes and a better life.Since moving into Product Management I’ve been focusing on more books like this to get my head into the right “space”.The Network: The Battle for the AirwavesBy: Scott Woolley Woolley, a technology and business writer, traces the development of communications technology from the telegraph to the television to the first visions of the Internet. He frames these advances with the story of the complicated friendship between David Sarnoff, a media mogul who rose to the helm of the Radio Corporation of America (RCA), and Edwin Armstrong, prolific inventor who developed, among other game-changing technologies, the first amplifier to enable telegraph signal reception from greater distances.Having previously read “Empires of Light: Edison, Tesla, Westinghouse, and the Race to Electrify the World” I figured I’d find this story about the fight for dominance with early broadcast technology interesting … and I was right ;)The Storm Is Upon Us: How QAnon Became a Movement, Cult, and Conspiracy Theory of EverythingBy: Mike Rothschild On October 5th, 2017, President Trump made a cryptic extemporaneous remark in the State Dining Room. He called this gathering of top-ranking military officials, “the calm before the storm,” and refused to elaborate as journalist and politicos inquired further. But on the infamous message boards of 4chan, elaboration began all on its own. In the days that followed, an anonymous poster spun a yarn inspired by Trump’s remarks that rivalled Tom Clancy and satisfied the deepest desires of MAGA-America. Did any of it come to pass? No. Did that stop people from clinging to every word they were reading, expanding its mythology and promoting the theory for years? No. How did this happen, who are these followers, and how do adherents reconcile their worldview with the America they see around them? Mike Rothschild, a journalist specializing in conspiracy theories, explains all–taking readers from the earliest posts on 4Chan to its embrace by right-wing media, and the game that Donald Trump has played with its followers. As rabid adherents to the theory show no sign of calming—with Baby Boomers especially susceptible to its messaging—families are being torn apart and politicians are starting to openly espouse the ideology in their campaigns. It’s time to figure out what QAnon is, because QAnon explains everything you need to know about American politics and global fear after Trump.It’s hard to not be morbidly curious as to how all this MAGA stuff came to pass.The Filter Bubble: What the Internet is Hiding from YouBy: Eli Pariser We all know the internet is vast. Indeed, it’s a veritable universe of information – an exciting frontier open to all. Or at least that’s what we’re led to believe. In reality, though, our access to this digital cosmos is closely monitored, our every click subtly guided. We think we’re surveying the heavens, but, more often than not, we’re never able to see much farther than our own backyard. How did our view become so limited? Well, internet giants like Google, Facebook and YouTube have created an individualized web for us. By using personalization and filters, such companies ensure that we never get the full picture – that we’re always confined to our own digital bubble.I’ve heard about this book for a while now and never bothered to give it a shot as the premise seems pretty obvious. After giving it a listen I don’t know that I really got a whole lot more out of it than what I already knew going in.Algorithms optimize for engagement, so you end up only being exposed to a subset of content you’re likely to engage with - which filters out a lot.Product Management in PracticeBy: Matt LeMay Product management has become a critical function for modern organizations, from small startups to corporate enterprises. And yet, the day-to-day work of product management remains largely misunderstood. In theory, product managers are high-flying visionaries who build products that people love. In practice, they’re hard-working facilitators who bring clarity and focus to their teams. In this thoroughly revised and expanded edition, Matt LeMay provides real-world guidance for current and aspiring product managers. Updated for the era of remote and hybrid work, this book provides actionable answers to product management’s most persistent and confounding questions, starting with: What exactly am I supposed to do all day?If anyone wants to get into Product Management I’d highly recommend reading/listening to this book. Product management is a much harder role to define than I anticipated and the whole “if you don’t know who’s responsibility something is, it’s probably yours” statement was apt.Mindf*ck: Cambridge Analytica and the Plot to Break AmericaBy: Christopher Wylie Mindf*ck (2019), written by a whistleblower, tells the story of the largest data crime in history to date. On the eve of the 2016 United States presidential election, consulting firm Cambridge Analytica harvested the Facebook data from 87 million people and used it to conduct a mass disinformation campaign. Now, the full story has finally come to light.More Trump and MAGA shit to close out the year. After listening to this it really makes me feel bad for all the folks that bought into the MAGA movement as it seems “manufactured” by Bannon and Cambridge Analytica. Real people, their lives and families are being affected by getting caught up in this perpetual cycle of outrage …" }, { "title": "Nightshade (Beam Software) - 1992", "url": "/blog/2022/12/31/nightshade/", "categories": "Let's Adventure!", "tags": "adventure, Beam Software, Ultra Games", "date": "2022-12-31 11:56:32 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Nightshade, fully titled Nightshade Part 1: The Claws of Sutekh onscreen, is an action-adventure...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Nightshade, fully titled Nightshade Part 1: The Claws of Sutekh onscreen, is an action-adventure video game released in 1992 for the Nintendo Entertainment System. It was developed by Beam Software and published by Ultra Games. The game was meant to be the first part in a series, but no sequels were ever made; however, it served as the basis for Beam Software’s Super NES video game adaptation of Shadowrun. Though it’s not covered in this series, I’ve reviewed Shadowrun previously.Cool … a screwdriver …Your goal in Nightshade is to become a hero. You start off in the sewers under Metro City having been captured by Sutekh, who has taken over the city’s various crime factions and merged them all into one (under his control). You find yourself tied to a chair in the sewer with a bomb about the go off. Sutekh monologues for a moment then leaves, leaving you to try and escape.Once you do you go about your adventure, which involves collecting items and fighting off some baddies.A good deed is its own reward … but a wad of cash is a BETTER rewardAs you perform good deeds (such as saving citizens) or beating up bosses, your popularity will grow. Other than opening up new dialog options with certain NPCs I don’t know that this really has a huge impact on how you progress through the game.Enemy encounters give you direct control over Nightshade, allowing you to punch, kick and jump. Since you have limited health and no way to heal during a battle, if you get knocked around too quickly you’ll likely die .. so save often! The battles are actually pretty challenging and I found playing this game using an emulator made it a much less frustrating experience.Speaking of dying, I actually found the system they use in this game to continue after dying to be pretty innovative. You essentially have 3 “continues”, but in order to use them Nightshade needs to escape from a death trap that Sutekh has placed him in. These are overly elaborate and cartoony (like something out of the original Batman TV Series). After escaping the death trap you’re back in the sewers where the game started, but seeing as the game world is extremely small it doesn’t take too long to get back to where you were.The previous hero of Metro City was Vortex, who’s lair you’ll eventually find. There you can use a healing pod to recover your health a fixed number of times to help get you through the game. Since you don’t heal after a fight automatically you’ll find yourself going back to the lair often as you progress through the game.Be a hero they said! What’s the worst that could happen …If you’re wondering why Vortex doesn’t just kick Sutekh’s ass himself, well, once you go looking for the Rat King (one of the 5 crime bosses) you’ll find out.Each crime boss holds a scarab, which you’ll need to collect in order to open the path to the final encounter with Sutekh. I remember playing this on the original console back in the day and finding these boss battles extremely frustrating and difficult. I mean … REALLY difficult. Even playing today with an emulator and quick saves I found myself dying a lot.To get through some of these fights I found myself turning the emulation speed down to 50%. This gave me enough time to react to the enemies moves and setup an attack before getting my ass handed to me. Maybe I’m just getting old and my reaction time sucks now, but I had to resort to this more than once throughout this play through …Navigating the world is pretty straightforward, though interacting with it requires dropping down to a menu system from which you choose your options (Pick Up, Operate, Jump, Talk … etc). This can be pretty cumbersome, but seeing as it’s an NES game there really weren’t a lot of options for mapping these interactions to buttons.After collecting all 5 scarabs, you go to the graveyard and use that cumbersome menu system to select each scarab and put it into a slot in the statue. This opens up a door that leads you to a rematch with each of the 5 crime bosses. This really feels like busywork at this point, but after fighting them all again you find Sutekh, beat him and save Metro City.And that’s all she wrote … until next time!I’m likely going to score this game a little higher than it may deserve because I have a bit of a history with it. Back in 2005 I stumbled across Tool-assisted speed running and the TASVideos community. Back in those days I went by the handle Maximus, and one of my first published videos was “NES Nightshade by Maximus in 08:27.67”. Fun fact, a much younger me also wrote the original TAS Movie Editor, but this functionality was subsumed into Bizhawk eventually making the solution unnecessary.Game Information Game Nightshade Developer Beam Software Publisher Ultra Games Release Date January, 1992 Systems NES Game Engine   Play Information How Long To Beat? 1.5 hours Version Played NES via Bizhawk Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 13 Story (25) 16 Experience (15) 10 Impact (10) 7   66% Gallery" }, { "title": "Wizard and the Princess (On-Line Systems) - 1980", "url": "/blog/2022/12/30/wizard-and-the-princess/", "categories": "Let's Adventure!", "tags": "adventure, ADL, Sierra On-Line", "date": "2022-12-30 07:34:58 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Wizard and the Princess is a graphic adventure game written for the Apple II and published in 19...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Wizard and the Princess is a graphic adventure game written for the Apple II and published in 1980 by On-Line Systems. It was the second title released in the Hi-Res Adventures series after Mystery House (On-Line Systems). While Mystery House used monochrome drawings, Wizard and the Princess added color. Ports for the Atari 8-bit family and Commodore 64 were released in 1982 and 1984 respectively. The 1982 self-booting version for IBM PC compatibles was renamed Adventure in Serenia, and is the version I played.As with other ADL titles from On-Line Systems, you navigate the world by interacting with a text parser. The inputs you can provide are still pretty basic, but seeing as this is one of the first graphic adventure games from the early 80’s this can be forgiven.The color palette is a lot of green, brown and purple. I didn’t really think too much of this until reading through the Wikipedia entry and coming across the port notes that includes the quote “Roberta Williams reputedly referred to the colors on the IBM PC as ‘atrocious’ upon seeing the completed game running for the first time”.I took the above screenshot of the Commodore 64 version (running in VICE) just to compare the IBM PC version … and I can confirm the baby puke palette is an anomaly that really does make this port of the game look like shit.The game isn’t overly complex and can be finished in a short period of time. Some of the puzzles aren’t really intuitive, but the vast majority involve just using an item in the current room and it’ll deduce some bizarre outcome you wouldn’t have necessarily thought of (ex: USE KNIFE in the castle will pick the lock of the door so you can OPEN DOOR successfully).There are plenty of items to pick up along the way, and even a vendor at one point that you can use a coin you found on to buy a specific item from a table full of wares. This game really is all about picking up everything and trying out all items to see what item solves what puzzle (ex: USE VIAL to turn you into a bird … to fly one screen NORTH … you know … because of reasons).Eventually you make your way to the room in the castle with a frog, KISS FROG, reveal the princess, go up a flight of stairs, open a chest, get a pair of shoes (which you have to LOOK SHOES first to get a magic word), then WHOOSH back to the beginning of the game. Don’t forget to TALK to the princess to trigger the endgame or else you’ll just be standing there like a dope.Other than Mystery House that I have extremely fond memories of due to having played it as a kid these other early Sierra (On-Line Systems) games don’t really resonate with me. As a result the iterative improvements between each title, which ultimately set the stage for King’s Quest: Quest for the Crown and beyond, were lost on me.The Digital Antiquarian has a great write up on this game (Part 1, Part 2) that I’d highly recommend if you want to dive a little deeper into this title.Game Information Game Wizard and the Princess Developer On-Line Systems Publisher Online Systems Release Date 1980 Systems Apple II, Apple II Plus, Atari 8-bit, Commodore 64IBM PC, PCjr, FM-7, PC-88, PC-98 Game Engine ADL (Adventure Development Language) Play Information How Long To Beat? 26 minutes Version Played DOS via DOSBox-X Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 8 Story (25) 6 Experience (15) 3 Impact (10) 3   29% Gallery" }, { "title": "Hugo’s House of Horrors (Gray Design Associates) - 1990", "url": "/blog/2022/12/21/hugos-house-of-horrors/", "categories": "Let's Adventure!", "tags": "adventure, Gray Design Associates", "date": "2022-12-21 20:20:48 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Hugo’s House of Horrors is a parser-based adventure game designed by independent software develo...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Hugo’s House of Horrors is a parser-based adventure game designed by independent software developer David P. Gray and published as shareware by Gray Design Associates in 1990. The game follows the character Hugo as he searches for his girlfriend Penelope in a haunted house.This is another text parser driven game. The commands required to interact with the game world are pretty simple (and obvious … usually). Though I’ve been a huge advocate for playing as many of these adventure games as possible using ScummVM, I have to admit that I really did not like the text feedback being rendered using ScummVM’s modal subsystem. As functional as it may be, it’s a bit jarring and just feels out of place.The above screenshot was taken using DOSBox to run the game instead to compare the two interfaces.Aside from basic inventory management puzzles, there are also timing puzzles that involve moving Hugo to the right place in the room quickly enough to not get killed by the enemy(ies) on the current screen. This is most prominent in the basement when you figure out you need to walk between the boulders to advance to the next screen then immediately get killed by the bats. You have to blow a whistle to confuse the bats, but even after you do this if you were still standing in the wrong spot the bats will still kill you.The mummy on the next screen is a similar pain in the ass. You have to move through the room following a certain pattern to get the mummy trapped behind a rock so you can advance to the next screen.What I remember most about this game was that it was my first (pre-internet) experience getting so frustrated with a game I bought the “strategy guide”. This title was released as shareware (kids, ask your parents) - which I ended up buying the full version of … by sending David P Gray money in the mail and he sent a floppy disk back.When I got to the boat in the basement I had zero fucking clue how to plug the hole in the boat. I tried every possible combination I could think of and came up with nothing. I send David P Gray more money so he’d send me a strategy guide (which he did) only to learn I need to USE BUNG to fill the hole.This was likely a “U.K. English” thing as 10 year old Alex did not know what a fucking “bung” was - nor that I should use it on a hole in the bottom of a boat to fill a hole. Sigh.You find Penelope in a subsequent room, escape from this “house of horrors” and get married - the end.The game really isn’t bad for a text-parser game late in the life of text-parser games. David P Gray would go on to release two more titles in the Hugo trilogy (which I’ll get to eventually). You can still buy the games directly from the author from his website. I’d recommend going there just to bask in the glory of a webpage that still feels like it’s from 2001 :D.Game Information Game Hugo’s House of Horrors Developer Gray Design Associates Publisher Gray Design Associates Release Date 1990 Systems DOS, Windows Game Engine Hugo Play Information How Long To Beat? 1 hour Version Played DOS via ScummVM for Windows Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 9 Story (25) 10 Experience (15) 7 Impact (10) 3   41% Gallery" }, { "title": "Leisure Suit Larry in the Land of the Lounge Lizards (Sierra On-Line) - 1987", "url": "/blog/2022/12/19/leisure-suit-larry/", "categories": "Let's Adventure!", "tags": "adventure, Sierra Online, SCI, AGI", "date": "2022-12-19 06:25:05 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Leisure Suit Larry in the Land of the Lounge Lizards is the first installment in Sierra’s Leisur...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Leisure Suit Larry in the Land of the Lounge Lizards is the first installment in Sierra’s Leisure Suit Larry series. The game’s story follows a middle-aged male virgin named Larry Laffer as he desperately tries to “get lucky” in the fictional American city of Lost Wages. Land of the Lounge Lizards establishes several elements which recur in the later Leisure Suit Larry games, including Larry’s campy attire, perpetual bad luck with women, and penchant for double-entendres. The game’s overall plot and basic structure follow that of Softporn Adventure, Sierra’s own 1981 Apple II text adventure (that did not feature Larry as the main character).Unless you were born in the late 70’s or early 80’s you’re gonna need Google to answer these dated questionsThe game was originally released as a text parser game, but was later re-released as a point and click adventure with updated graphics. I chose to play the latter for this review to hopefully capture some … uh … better screenshots :PEverything in this game boils down to toilet humor (sometimes literally), dick and fart jokes and innuendo. Seeing as the game was written by Al Lowe, none of this should come as a surprise, but you may as well know what you’re getting into.I don’t think she likes meYour goal is to get laid. That’s it.Progressing through the game involves meeting women and trying to convince them they like you so they’ll hook up with you. As this is an adventure game you must travel to various locations (via a taxi) to get various items to try and help you accomplish your goal (flowers, chocolates, wine, condoms … etc). Seeing as this is a Sierra game there are numerous ways you can fuck up and die, but the death scenarios are typically pretty funny and add a bit of replayability to the game.Flushing the toilet is apparently deadlyWhen I first played this game I was probably 13 or 14 and the setting, scenario and jokes really appealed to me. Though I’m much older now, replaying this game still makes me chuckle. The jokes are mostly stupid, but you’re getting exactly what you expect and as long as you take it at face value, you’ll have a good time.Game Over … Good on you buddyGame Information Game Leisure Suit Larry in the Land of the Lounge Lizards Developer Sierra On-Line Publisher Sierra On-Line Release Date 1987 Systems DOS, Amiga, Apple II, Atari ST, Apple IIGS, TRS-80,Windows, OSX, Xbox Live Arcade, PlayStation Network, iOS, Android Game Engine Adventure Game Interpreter (AGI)/Sierra’s Creative Interpreter (SCI) Play Information How Long To Beat? 3 hours Version Played DOS (SCI Remake) via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 13 Story (25) 13 Experience (15) 9 Impact (10) 5   57% Gallery" }, { "title": "Ringworld: Revenge of the Patriarch (Tsunami Games) - 1992", "url": "/blog/2022/12/19/ringworld/", "categories": "Let's Adventure!", "tags": "adventure, Tsunami Games, TsAGE", "date": "2022-12-19 06:02:55 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Ringworld: Revenge of the Patriarch is a 1993 video game based on Larry Niven’s Ringworld novel ...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Ringworld: Revenge of the Patriarch is a 1993 video game based on Larry Niven’s Ringworld novel series. I was a huge fan of the books as a teenager and was really looking forward to this game when I first played it in the mid ’90s.The story of this game is that the Patriarch of the Kzinti empire has vowed revenge against the Puppeteer race for their genetic manipulation of the Kzinti. To achieve this, they have created an advanced prototype starship armed with an ancient Slaver weapon capable of destroying planets. The Patriarch first dispatches the ship with the goal of killing the family of the Kzin traitor Speaker-to-Animals, who has taken the name Chmeee. Once done, they will then seek out the home world of the Puppeteers and destroy it. The human Quinn is also travelling to meet with Chmeee to help find Louis Wu who has disappeared. Together, Quinn and Chmeee must travel to Ringworld to find Louis Wu and stop the Kzinti’s genocidal plot against the Puppeteers.The tie-in with the book is pretty loose, and though they mention some of the original characters (by name only), if you’ve never read the book you’re not going to be missing out on any story elements.Quinn and Chmee steal a ship and try to escape from the Kzinti empire, only to receive a communication from the Hindmost (a Puppeteer) who tells them they need to go to the Ringworld to collect some items in stasis boxes. If they don’t do this their coordinates will be shared with the Kzinti and that’ll be that … so off our heroes go.Character portraits look decentThe story progresses through character interactions, which can include simple decision trees. I don’t know that it really matters which option you choose when given a choice as this game is extremely linear.Once you escape from the Kzinti the engineer that was on board the ship you stole tries to sabotage you, which results in a fun bit of copy protection. The wires need to be arranged in a specific order according the page in the manual the game tells you they’ll be on.Who doesn’t love copy protection?After landing on the Ringworld you need to find the stasis boxes. The first group of natives you encounter you perform a “God Gambit” on to try and get them to do what you want. This involves Quinn engaging in Rishathra with the chief’s daughter (look it up … click that link …. you know you wanna …….). Seeing as I read these books as a teenager that term stuck in my head, so I appreciate that they worked it into the plot :PSomeone’s DTF …Most puzzles in the game are just fetch quests, and aren’t really that hard to complete. There are a couple of puzzles as well but these aren’t overly difficult to solve (such as the stool in the sky house or the final sliding tile puzzle to open the space craft).There are a number of items you collect throughout the game to complete the various fetch quests. These can be reviewed in your inventory at any time, and thankfully don’t involve any item combination puzzles.Once all stasis boxes are collected and you disable the stasis field on the ship in the first area, you solve a couple more item-based puzzles, put a melting down core reactor in a stasis box, fire that off at a Kzinti ship and remotely detonate it to win you freedom. The end!Honestly I look at this game with some rose colored classes due to having fond memories for the book. The game itself isn’t really “fun”, as it’s far too linear and the puzzles aren’t overly challenging.The background music involves short loops and can be annoying at times. The backgrounds and character portraits however are extremely well done for 1992 and the game does stand up visually.If you’re looking for a great science fiction experience go read the books :)The end … until Ringworld 2 that is …Game Information Game Ringworld: Revenge of the Patriarch Developer Tsunami Games Publisher Tsunami Games Release Date 1992 Systems DOS Game Engine TsAGE Play Information How Long To Beat? 3 hours Version Played DOS via ScummVM Notes Walkthrough, Manual ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 12 Story (25) 16 Experience (15) 11 Impact (10) 6   64% Gallery" }, { "title": "Grim Fandango (LucasArts) - 1998", "url": "/blog/2022/12/16/grim-fandango/", "categories": "Let's Adventure!", "tags": "adventure, Sierra Online, GrimE", "date": "2022-12-16 06:52:10 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Grim Fandango is a 1998 adventure game directed by Tim Schafer and developed and published by Lu...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Grim Fandango is a 1998 adventure game directed by Tim Schafer and developed and published by LucasArts for Microsoft Windows. It is the first adventure game by LucasArts to use 3D computer graphics overlaid on pre-rendered static backgrounds. As with other LucasArts adventure games, the player must converse with characters and examine, collect, and use objects to solve puzzles.Set in the Land of the Dead, through which recently departed souls, represented as calaca-like figures, travel before they reach their final destination. The story follows travel agent Manuel “Manny” Calavera as he attempts to save new arrival Mercedes “Meche” Colomar, a virtuous soul, on her journey. The game combines elements of the Aztec afterlife with film noir style.For a game late in the life of the golden age of point-and-click adventures games the graphics are as good as you’d expect. All characters are 3D models and the 2D backgrounds are beautifully rendered. The visual style is very different in that you’re following dead characters through the land of the dead and everything has a Mexican cultural twist to it. Honestly it looks really cool and is a breath of fresh air from the traditional fantasy settings of a lot of adventure games.As you would expect for a game of this genre the majority of your progress occurs via fetch quests, item combination and cut scenes. This game doesn’t have some of the “moon logic” older games contained (I’m looking at you “Monkey Wrench” puzzle from Monkey Island 2). There are very few items you need to pick up, and looking at the items in your inventory typically gives you enough of a clue to help clarify how/where/when the item should be used. The only exception is the Scythe which you have throughout the game and is used to solve multiple puzzles.Though story is well written, the voice acting is good and the game progresses logically, once huge strike against this game is the controls: Tank Controls to be precise. I fucking hate tank controls! Though you can’t accidentally get yourself killed or fall off anything, the controls are so awkward and it can take forever to properly align yourself to get Manny to look at an object to interact with or an item to pick up.I’m sure from a design point of view this made sense at the time as Manny moves his head to “face” hotspots you can interact with - but it really detracts from the game play after a while.As you progress through the game the save screen slowly reveals the map of the game to indicate your progress. I feel like these types of aesthetic touches really add to the game and really reinforce what great game designers LucasArts had at the time.Interacting with characters is completely menu driven. This feels a little “on rails” as there are a limited number of topics to choose from for each conversation and there are typically no “wrong” answers to anything that carry any negative repercussions. They serve their purpose though and convey tips, next steps and plot points appropriately. Since the voice acting is so well done I didn’t really mind that these interactions seemed a bit wooden and forced.Some of the story elements are actually extremely clever. The whole concept of characters being “sprouted” (killed) makes a lot of sense (in retrospect) as they’re dead to begin with … so how do you kill them off? The souls themselves can suffer death-within-death by being “sprouted”, the result of being shot with “sproutella”-filled darts that cause flowers to grow out through bones, rapidly feeding off the calcium of the soul’s skeleton. This just comes off as extremely metal ;)You eventually get to the final confrontation with Hector LeMans, complete a couple more fetch quests to setup the endgame and rid the world of a very bad dude. The souls of the departed are once again free to receive priority passage to the afterlife (folks that were good during their lives at least …).I was running this game using ScummVM and I stumbled upon Ticket #13139, which lead me down a bit of a debugging rabbit hole. The game’s scripted logic is written in Lua so I went on the hunt for a disassembler to see what the failing logic was doing (see the linked ticket above if you’re curious). This lead me to Bret Mogilefsky’s excellent post on “LUA IN GRIM FANDANGO” I’d also recommend.Grim Fandango doesn’t have a huge amount of replayability, but it is a lot of fun the first time you run through it. If you appreciate a good story though, this is definitely a game worth playing at least once.Game Information Game Grim Fandango Developer LucasArts Publisher LucasArts Release Date October 30, 1998 Systems Windows Game Engine GrimE Play Information How Long To Beat? 11 hours Version Played Windows via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 18 Story (25) 22 Experience (15) 13 Impact (10) 8   87% Gallery" }, { "title": "TableCheck: Empowering Restaurants with Best-in-Class Booking Tools Powered by MongoDB", "url": "/blog/2022/12/14/tablecheck-empowering-restaurants-with-best-in-class-booking-tools-powered-by-mongodb/", "categories": "MongoDB", "tags": "ruby, rails, mongodb, orm, odm, customer-success, cross-post", "date": "2022-12-14 14:49:43 -0500", "snippet": " Cross posted from the MongoDB Developer CenterTableCheck is the world’s premiere booking and guest platform. Headquartered in Tokyo, they empower restaurants with tools to elevate their guest exp...", "content": " Cross posted from the MongoDB Developer CenterTableCheck is the world’s premiere booking and guest platform. Headquartered in Tokyo, they empower restaurants with tools to elevate their guest experience and create guests for life with features like booking forms, surveys, marketing automation tools and an ecosystem of powerful solutions for restaurants to take their business forward.Architectural Overview of TableCheckLaunched in 2013, TableCheck began life as a Ruby on Rails monolith. Over time the solution has been expanded to include satellite microservices however one constant that has remained throughout this journey was MongoDB.Originally TableCheck managed their own MongoDB Enterprise clusters, however once MongoDB Atlas became available they migrated their data to a managed replica set running in AWS.According to CEO Johnny Shields, MongoDB was selected initially as the database of choice for TableCheck as it was “love at first sight”. Though MongoDB was a much different solution in 2013, even in the database product’s infancy it fit perfectly with their development workflow and allowed them to work with their data easily and quickly while building out their APIs and application.Ruby on Rails + MongoDBAny developer familiar with Ruby on Rails knows that the ORM layer (via Active Record) was designed to support relational databases. MongoDB’s Mongoid ODM acts as a veritable “drop-in” replacement for existing Active Record adapters so that MongoDB can be used seamlessly with Rails. The CRUD API is familiar to Ruby on Rails developers and makes working with MongoDB extremely easy.When asked if MongoDB and Ruby were a good fit, Johnny Shields replied: “Yes, I’d add the combo of MongoDB + Ruby + Rails + Mongoid is a match made in heaven. Particularly with the Mongoid ORM library, it is easy to get MongoDB data represented in native Ruby data structures, e.g. as nested arrays and objects”.This has allowed TableCheck to ensure MongoDB remains the “golden-source” of data for the entire platform. They currently replicate a subset of data to Elasticsearch for deep multi-field search functionality, however given the rising popularity and utility of Atlas Search this part of the stack may be further simplified.As MongoDB data changes within the TableCheck platform, these changes are broadcast over Apache Kafka via the MongoDB Kafka Connector to enable downstream services to consume it. Several of their microservices are built in Elixir, including a data analytics application. PostgreSQL is being used for these data analytics use cases as the only MongoDB Drivers for Elixir and managed by the community (such as elixir-mongo/mongodb or zookzook/elixir-mongodb-driver), however should an official Driver surface this decision may change.Benefits of the Mongoid ODM for Ruby on Rails DevelopmentThe “killer feature” for new users discovering Ruby on Rails is Active Record Migrations. This feature of Active Record provides a DSL that enables developers to manage their relational database’s schema without having to write a single line of SQL. Because MongoDB is a NoSQL database, migrations and schema management are unnecessary!Johnny Shields shares the following based on his experience working with MongoDB and Ruby on Rails: “You can add or remove data fields without any need to migrate your database. This alone is a “killer-feature” reason to choose MongoDB. You do still need to consider database indexes however, but MongoDB Atlas has a profiler which will monitor for slow queries and auto-suggest if any index is needed.”As the Mongoid ODM supports large portions of the Active Record API, another powerful productivity feature TableCheck was able to leverage is the use of Associations. Cross-collection referenced associations are available, however unlike relational databases embedded associations can be used to simplify the data model.Open Source and Community StrongBoth mongodb/mongoid and mongodb/mongo-ruby-driver are licensed under OSI approved licenses and MongoDB encourages the community to contribute feedback, issues and pull requests!Since 2013, the TableCheck team has contributed nearly 150 PRs to both projects. The majority tend to be quality-of-life improvements and bug fixes related to edge-case combinations of various methods/options. They’ve also helped improve the accuracy of documentation in many places, and have even helped the MongoDB Ruby team setup Github Actions so that it would be easier for outsiders to contribute.With so many contributions under their team’s belt, and clearly able to extend the Driver and ODM to fit any use case the MongoDB team may not have envisioned, when asked if there were any use-cases MongoDB couldn’t satisfy within a Ruby on Rails application the feedback was: “I have not encountered any use case where I’ve felt SQL would be a fundamentally better solution than MongoDB. On the contrary, we have several microservices which we’ve started in SQL and are moving to MongoDB now wherever we can.”The TableCheck team are vocal advocates for things like better changelogs, more discipline in following semantic versioning best practices. These have benefited the community greatly, and Johnny and team continue to advocate for things like adopting static code analysis (ex: via Rubocop) to improve overall code quality and consistency.Overall Thoughts on Working With MongoDB and Ruby on RailsTableCheck has been a long-time user of MongoDB via the Ruby driver and Mongoid ODM, and as a result has experienced some growing pains as the data platform matured. When asked about any challenges his team faced working with MongoDB over the years Johnny replied: “The biggest challenge was that in earlier MongoDB versions (3.x) there were a few random deadlock-type bugs in the server that bit us. These seemed to have gone away in newer versions (4.0+). MongoDB has clearly made an investment in core stability which we have benefitted from first-hand. Early on we were maintaining our own cluster, and from a few years ago we moved to Atlas and MongoDB now does much of the maintenance for us”.We at MongoDB continue to be impressed by the scope and scale of the solutions our users and customers like TableCheck continue to build. Ruby on Rails continues to be a viable framework for enterprise and best-in-class applications, and our team will continue to grow the product to meet the needs of the next generation of Ruby application developers.Johnny presented at MongoDB Day Singapore on November 23, 2022 (view presentation). His talk covered a number of topics, including his experiences working with MongoDB and Ruby." }, { "title": "Efficiently Identifying Duplicates using MongoDB", "url": "/blog/2022/11/29/efficiently-identifying-duplicates-using-mongodb/", "categories": "MongoDB", "tags": "javascript, mongodb, scripting, queries, indexing", "date": "2022-11-29 10:50:52 -0500", "snippet": "Efficiently Identifying Duplicates using MongoDBOne question that comes up time and again on Stack Overflow or the MongoDB Developer Forums is “how can I find duplicate X and get a list of Y that c...", "content": "Efficiently Identifying Duplicates using MongoDBOne question that comes up time and again on Stack Overflow or the MongoDB Developer Forums is “how can I find duplicate X and get a list of Y that contains these duplicates” (ex: “Query to find duplicate users (ip)”).Using MongoDB’s Aggregation Framework this can be done easily.function generate_random_data(size){ var chars = 'abcdefghijklmnopqrstuvwxyz'.split(''); var len = chars.length; var random_data = []; while (size--) { random_data.push(chars[Math.random()*len | 0]); } return random_data.join('');}function setup() { db.foo.drop(); db.foo.insertMany([ { parent_id: 1, user_id: 1, junk: generate_random_data(512) }, { parent_id: 1, user_id: 2, junk: generate_random_data(512) }, { parent_id: 2, user_id: 3, junk: generate_random_data(512) }, { parent_id: 3, user_id: 4, junk: generate_random_data(512) }, { parent_id: 4, user_id: 5, junk: generate_random_data(512) }, { parent_id: 3, user_id: 6, junk: generate_random_data(512) }, { parent_id: 2, user_id: 7, junk: generate_random_data(512) } ]);}setup();Given the above setup our collection will contain 7 documents. If we wanted to identify how many duplicate parent_id values there are and what the associated user_id values are we could do something like the following:db.foo.aggregate([ { $group: { _id: \"$parent_id\", used: { $sum: 1 }, user_ids: { $push: \"$user_id\" } } }, { $match: { used: { $gt: 1 } } }, { $project: { _id: 0, parent_id: \"$_id\", user_ids: \"$user_ids\"} }]);/* ** output **[ { \"parent_id\": 3, \"user_ids\": [ 4, 6 ] }, { \"parent_id\": 1, \"user_ids\": [ 1, 2 ] }, { \"parent_id\": 2, \"user_ids\": [ 3, 7 ] }]*/This will work pretty efficiently for our sample set of 7 documents, but what about millions (or billions)?Reviewing PerformanceBy generating Explain Results for the above operation we can better understand how this operation is performing:db.foo.explain(\"executionStats\").aggregate([ { $group: { _id: \"$parent_id\", used: { $sum: 1 }, user_ids: { $push: \"$user_id\" } } }, { $match: { used: { $gt: 1 } } }, { $project: { _id: 0, parent_id: \"$_id\", user_ids: \"$user_ids\"} }]);/* ** output **\"winningPlan\": { ... \"inputStage\": { \"stage\": \"COLLSCAN\", \"direction\": \"forward\" }},...\"executionStats\": { \"nReturned\": 7, \"totalKeysExamined\": 0, \"totalDocsExamined\": 7,*/According to the documentation we can improve our pipeline’s performance with indexes and document filters.No index was available for use and as a result a full collection scan was required.Adding an IndexWe know only 2 fields from our document are actually being used by the pipeline, so let’s try again using a purpose-built compound index and review the explain plan again:db.foo.createIndex({ parent_id: 1, user_id: 1 });db.foo.explain(\"executionStats\").aggregate([ { $group: { _id: \"$parent_id\", used: { $sum: 1 }, user_ids: { $push: \"$user_id\" } } }, { $match: { used: { $gt: 1 } } }, { $project: { _id: 0, parent_id: \"$_id\", user_ids: \"$user_ids\"} }]);/* ** output **\"winningPlan\": { ... \"inputStage\": { \"stage\": \"COLLSCAN\", \"direction\": \"forward\" }},...\"executionStats\": { \"nReturned\": 7, \"totalKeysExamined\": 0, \"totalDocsExamined\": 7,*/Even with what appears to be an ideal index a collection scan is being performed. What gives? Oh right …. even if following “ESR Guidance” for creating optimal indexes, an unfiltered $group must scan the entire collection anyway and wouldn’t benefit directly from an index ….. or would it?Adding a $sort before the $groupHaving a $sort stage prior to the $group will allow the pipeline take advantage of the index to group a sorted set.db.foo.explain(\"executionStats\").aggregate([ { $sort: { parent_id: 1 } }, { $group: { _id: \"$parent_id\", used: { $sum: 1 }, user_ids: { $push: \"$user_id\" } } }, { $match: { used: { $gt: 1 } } }, { $project: { _id: 0, parent_id: \"$_id\", user_ids: \"$user_ids\"} }]);/* ** output **\"winningPlan\": { ... \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"parent_id\": 1, \"user_id\": 1 }, ...\"executionStats\": { \"nReturned\": 7, \"totalKeysExamined\": 7, \"totalDocsExamined\": 0,*/The explain plan for the above operation shows not only that an index was used, but the entire operation was a covered query.ConclusionNow that we can identify duplicates further processing can be done with the results as needed. For example assume we wanted to remove all documents with a duplicate parent_id and only keep the first:db.foo.aggregate([ { $sort: { parent_id: 1 } }, { $group: { _id: \"$parent_id\", used: { $sum: 1 }, user_ids: { $push: \"$user_id\" } } }, { $match: { used: { $gt: 1 } } }, { $project: { _id: 0, parent_id: \"$_id\", user_ids: \"$user_ids\"} }]).forEach((d) =&gt; db.foo.deleteMany({ user_id: { $in: d.user_ids.slice(1, d.user_ids.length) } }));The above is taking the results of the aggregation pipeline and applying the following deleteMany command to each: (d) =&gt; db.foo.deleteMany({ user_id: { $in: d.user_ids.slice(1, d.user_ids.length) } }).Note this could be further optimized for larger delete workloads by instead writing all duplicate user_id values to a single array and deleting those all at once:var toDelete = [];db.foo.aggregate([ { $sort: { parent_id: 1 } }, { $group: { _id: \"$parent_id\", used: { $sum: 1 }, user_ids: { $push: \"$user_id\" } } }, { $match: { used: { $gt: 1 } } }, { $project: { _id: 0, parent_id: \"$_id\", user_ids: \"$user_ids\"} }]).forEach((d) =&gt; toDelete.push(d.user_ids.slice(1, d.user_ids.length)));db.foo.deleteMany({ user_id: { $in: toDelete.flat() } }); Be very careful whenever batch removing data and test in a lower environment first!Hopefully you found this helpful. If you did, feel free to drop a comment below ;)" }, { "title": "Just Finished - Earthbound", "url": "/blog/2022/11/22/just-finished-earthbound/", "categories": "Gaming, Just Finished", "tags": "jrpg", "date": "2022-11-22 06:48:37 -0500", "snippet": "When I was really getting into 16-bit JRPGs in the early to mid 90’s, Earthbound was actually one of the first games I ended up owning. Strangely enough what I remember most about it was the oversi...", "content": "When I was really getting into 16-bit JRPGs in the early to mid 90’s, Earthbound was actually one of the first games I ended up owning. Strangely enough what I remember most about it was the oversized box I couldn’t bring myself to get rid of, but couldn’t seem to find a place to put either.EarthBound takes place in the 1990s, several years after the events of Mother, in the fictional country of Eagleland (a parody of the United States). The player starts as a young boy named Ness as he investigates a nearby meteorite crash with his neighbor, Pokey to find his neighbor’s brother Picky. They find that an alien force, Giygas, has enveloped and consumed the world in hatred and consequently turned animals, humans, and objects into malicious creatures. A small, bee-like creature from the future named Buzz-Buzz instructs Ness to collect melodies in a Sound Stone from eight Sanctuaries to preemptively stop the force, but is killed shortly thereafter when Pokey and Picky’s mother mistakes him for a pest. On his journey to visit the sanctuaries, Ness visits the cultists of Happy Happy Village, where he saves Paula, and the zombie-infested Threed, where the two of them fall prey to a trap. After Paula telepathically instructs Jeff in a Winters boarding school to rescue them, they continue to Saturn Valley, a village filled with a species of creatures called Mr. Saturn, the city of Fourside, and the seaside resort Summers. Meanwhile, Poo, the prince of Dalaam, undergoes training called “Mu Training” before joining the party as well.Like most (J)RPGs of the time, interaction with the game world is mostly menu driven. When you want to interact with a character, object or use an item you have to bring up a menu and select the appropriate entry. You can access the talk/search option quickly by pressing a different button on your controller, but it’s still just picking a menu entry for you.As you explore the world you’ll see enemies on screen that will rush at you and trigger an encounter. These battles are also menu driver, however unlike most games at the time the backgrounds aren’t static images. “Funky” patterns are pulsating moving around all the time, enhancing the experience. I don’t think I’ve actually seen this type of battle presentation since Earthbound, and though it seemed weird at first it actually really suited this game.Battling enemies gives you experience, which allows you to level up. As you gain levels and your characters get stronger you’ll eventually be “too strong” for the enemies in a particular area of the game. When this happens, instead of running towards you, the enemies will run away from you. If you engage them in battle, instead of fighting you’ll just get an instant victory - including the EXP you would have gained by winning a battle.There is an element of strategy as to how you approach an enemy on the overworld. For example, if you surprise an enemy (from behind), the encounter will being with a “green swirl” which gives you the first move as an advantage. If the enemy surprises you however, a “red swirl” will be shown and the enemy will go first in the ensuing battle.You’ll encounter a large cast of characters as you explore the game world, and each will help progress the story forward. You’ll team up with three other characters throughout the game - each of which is proficient with a different skill.Jeff uses items and can fix various broken items you find to turn them into something useful (such as a weapon for himself). Paula has strong offensive magic, and Poo is a good fighter with a mix of offensive and defensive magic.As you explore you’ll find presents all over the place that contain items, weapons and armour. Inventory management is a bit of a pain in the ass in this game as each character has a limited number of inventory slots and once they’re used up you have to choose to drop and item every time a new one is found. There is a service you can call to store and retrieve your items (Escargo Express), however you need to have access to a phone booth for this.The story seems a little simplistic at the beginning, but as you progress it draws you in an keeps you engaged. I’ve replayed this game multiple times over the years and still find myself reading through the dialogue on each replay as the writing is very clever. The characters are all interesting and for a game you’d think was designed for younger kids it’s actually pretty “deep”.Ultimately your next door neighbour ends up being evil and aligns himself with the ultimate evil force, Giygas. You need to travel into your mind, through time and eventually transport your soul into a machine to go fight Pokey, then finally Giygas. Interestingly enough, the final battle isn’t all about wailing on the baddie, but requires to you do a little thinking (and a little praying).If you’ve never played Earthbound (aka. Mother 2) and are a fan of JRPGs from “golden age”, this is definitely one to try!" }, { "title": "MongoDB ORMs, ODMs, and Libraries", "url": "/blog/2022/11/02/mongodb-orms-odms-and-libraries/", "categories": "MongoDB", "tags": "ruby, rails, mongodb, orm, odm, cross-post", "date": "2022-11-02 06:28:40 -0400", "snippet": " Cross posted from the MongoDB Developer CenterThough developers have always been capable of manually writing complex queries to interact with a database, this approach can be tedious and error-pr...", "content": " Cross posted from the MongoDB Developer CenterThough developers have always been capable of manually writing complex queries to interact with a database, this approach can be tedious and error-prone. Object-Relational Mappers (or ORMs) improve the developer experience, as they accomplish multiple meaningful tasks: Facilitating interactions between the database and an application by abstracting away the need to write raw SQL or database query language. Managing serialization/deserialization of data to objects. Enforcement of schema.So, while it’s true that MongoDB offers Drivers with idiomatic APIs and helpers for most  programming languages, sometimes a higher level abstraction is desirable. Developers are used to interacting with data in a more declarative fashion (LINQ for C#, ActiveRecord for Ruby, etc.) and an ORM facilitates code maintainability and reuse by allowing developers to interact with data as objects.MongoDB provides a number of ORM-like libraries, and our community and partners have as well! These are sometimes referred to as ODMs (Object Document Mappers), as MongoDB is not a relational database management system. However, they exist to solve the same problem as ORMs do and the terminology can be used interchangeably.The following are some examples of the best MongoDB ORM or ODM libraries for a number of programming languages, including Ruby, Python, Java, Node.js, and PHP.BeanieBeanie is an Asynchronous Python object-document mapper (ODM) for MongoDB, based on Motor (an asynchronous MongoDB driver) and Pydantic.When using Beanie, each database collection has a corresponding document that is used to interact with that collection. In addition to retrieving data, Beanie allows you to add, update, and delete documents from the collection. Beanie saves you time by removing boilerplate code, and it helps you focus on the parts of your app that actually matter.See the Beanie documentation for more information.DoctrineDoctrine is a PHP MongoDB ORM, even though it’s referred to as an ODM. This library provides PHP object mapping functionality and transparent persistence for PHP objects to MongoDB, as well as a mechanism to map embedded or referenced documents. It can also create references between PHP documents in different databases and work with GridFS buckets.See the Doctrine MongoDB ODM documentation for more information.MongoidMost Ruby-based applications are built using the Ruby on Rails framework. As a result, Rails’ Active Record implementation, conventions, CRUD API, and callback mechanisms are second nature to Ruby developers. So, as far as a MongoDB ORM for Ruby, the Mongoid ODM provides API parity wherever possible to ensure developers working with a Rails application and using MongoDB can do so using methods and mechanics they’re already familiar with.See the Mongoid documentation for more information.MongooseIf you’re seeking an ORM for NodeJS and MongoDB, look no further than Mongoose. This Node.js-based Object Data Modeling (ODM) library for MongoDB is akin to an Object Relational Mapper (ORM) such as SQLAlchemy. The problem that Mongoose aims to solve is allowing developers to enforce a specific schema at the application layer. In addition to enforcing a schema, Mongoose also offers a variety of hooks, model validation, and other features aimed at making it easier to work with MongoDB.See the Mongoose documentation or MongoDB &amp; Mongoose: Compatibility and Comparison for more information.MongoEngineMongoEngine is a Python ORM for MongoDB. Branded as a Document-Object Mapper, it uses a simple declarative API, similar to the Django ORM.It was first released in 2015 as an open-source project, and the current version is built on top of PyMongo, the official Python Driver by MongoDB.See the MongoEngine documentation for more information.PrismaPrisma is a new kind of ORM for Node.js and Typescript that fundamentally differs from traditional ORMs. With Prisma, you define your models in the declarative Prisma schema, which serves as the single source of truth for your database schema and the models in your programming language. The Prisma Client will read and write data to your database in a type-safe manner, without the overhead of managing complex model instances. This makes the process of querying data a lot more natural as well as more predictable since Prisma Client always returns plain JavaScript objects.Support for MongoDB was one of the most requested features since the initial release of the Prisma ORM, and was added in version 3.12.See Prisma &amp; MongoDB for more information.Spring Data MongoDBIf you’re seeking a Java ORM for MongoDB, Spring Data for MongoDB is the most popular choice for Java developers. The Spring Data project provides a familiar and consistent Spring-based programming model for new datastores while retaining store-specific features and capabilities.Key functional areas of Spring Data MongoDB that Java developers will benefit from are a POJO centric model for interacting with a MongoDB DBCollection and easily writing a repository-style data access layer.See the Spring Data MongoDB documentation or the Spring Boot Integration with MongoDB Tutorial for more information.Go Build Something Awesome!Though not an exhaustive list of the available MongoDB ORM and ODM libraries available right now, the entries above should allow you to get started using MongoDB in your language of choice more naturally and efficiently.If you’re looking for assistance or have any feedback don’t hesitate to engage on our Community Forums." }, { "title": "From Engineering to Product Management", "url": "/blog/2022/10/06/from-engineering-to-product-management/", "categories": "Personal", "tags": "mongodb, ruby, product management", "date": "2022-10-06 03:13:27 -0400", "snippet": "I’ve spent my entire professional career as an engineering IC either churning out code, working on architectural challenges or helping to lead engineering teams (while still churning out code). Whe...", "content": "I’ve spent my entire professional career as an engineering IC either churning out code, working on architectural challenges or helping to lead engineering teams (while still churning out code). When I recently moved into a Product Management role at MongoDB I brought a significant amount of technical expertise about MongoDB’s products with me, however I’ve been unsure as to how much of an advantage this really was.Moving into Product Management is a career trajectory change, and though having an engineering background and product experience should be an advantage, it can also be a disadvantage if you bring along biases, unfounded expectations and a general sense of “knowing more about something based on only your experience with it”.Product Management is not only about knowing how the product works, but also about understanding the users, community, developers, competitive landscape, associated technologies, potential partners, existing customers, potential customers, the product’s legacy and history (warts and all) and how to enable all these different groups to succeed. I’ve also been reading Cracking the PM Career lately, which is helping fill in some gaps in my knowledge of this role.This post is meant to capture some of the lessons learned in the first 90 days on the job and hopefully help anyone else considering a similar career change.Don’t try to hit the ground runningHaving used MongoDB for nearly a decade, and having worked as a Technical Services Engineer for 3 years I understood the product suite quite well. This included the MongoDB Server, MongoDB Atlas, Atlas Realm and the ins and out of core concepts such as replication, sharding, query performance, query tuning, indexing, the aggregation pipeline, etc. I’ve also been building software using the MongoDB Ruby driver for a number of years, so taking over as the Product Manager for the driver seemed like a great fit.The problem however is that having this experience resulted in bias. My experience with the product is not necessarily our users/customers experience with the product. I put a lot of pressure on myself to start delivering quickly as I should “know how things work”, but new PMs on a team need to go through a discovery process - which I also ended up doing and found extremely valuable.I went through a process similar to the Discover -&gt; Define -&gt; Design -&gt; Develop -&gt; Deliver -&gt; Debrief -&gt; (REPEAT) cycle, which involved doing a bunch of user-focused research to try and better understand the current state of our users exposure to my product during their developer journey. I know how to use the Ruby driver - I know where to look for documentation - I know where the code is … but is that true for all of our users? How easy is it to get started? What do they find when they search for tutorials and getting started resources? Does the information they find actually help them?The answers to the above questions would help inform a strategy that we can plan to execute against.Understand what your product is and isn’tMongoDB Drivers (as a product) are a little different than traditional products. They exist to act as an interface between a developer and their database, but should largely be “out of the way”. A good driver will conform to the associated programming language’s conventions and best practices. A Ruby programmer working with the MongoDB Ruby driver shouldn’t have to adapt their codebase to work with our tools - our tools should fit neatly into their codebase.As most Ruby applications are Ruby on Rails applications, the Mongoid ODM should allow developers to interact with their data using similar APIs that any other Active Record provider they choose for the application would expose.I love the Ruby driver. I write most of my test cases and scenarios using the Ruby driver … but most of our users write Rails applications and would be using the Mongoid ODM as a result. Understanding how users interact with the product, what tools and frameworks they use alongside the product and what types of issues they face really helps to highlight where I should be focusing our efforts.Outcomes over outputDeveloping a strategy is a lot harder than I expected. I’m used to measuring progress (and success) based on basic telemetry. How many cases did I take? How many cases did I close? How many tickets did I take? How many user stories did I complete? Did I push my code to prod?The world of Product Management moves a lot slower - and for good reason. User outreach, user research, discovery, planning, interviews, writing specification, writing content, writing documentation - these all take time. Measuring the impact of the work being done also takes time, and this can be particularly challenging if you’re used to a faster feedback loop.Learn to measure incremental progress towards larger goals!Imposter syndrome comes for us allHaving been an engineer at MongoDB for a number of years, I assumed I’d understand how to succeed at my new role quickly. When it didn’t feel like that was the case it can be hard to reach out for help - but we’re all human and your colleagues want you to succeed as much as you do ;)Initially I found myself slipping back into doing more engineering-focused tasks as that’s what I was most comfortable with. Because I wasn’t sure how to measure success in a Product Management capacity, this felt “safe” and also allowed me to feel productive. This only happened periodically during the first few weeks of ramping up into the new role as I was comfortable enough to reach out to colleagues and ask questions and get advice.I’ve always felt like a good engineer, but I didn’t feel like a good PM right away (still don’t … but working on it).Learn where to leverage past experienceUnderstanding the product and having engineering experience is beneficial to a Product Manager as it allows you to more realistically estimate the complexity of a given task/feature/epic. Being able to talk to your developers intelligently about the contents of the current sprint, offering useful feedback during stand-ups, planning and code reviews helps establish clout.Don’t assign yourself arbitrary engineering tasks though. That’s not what you’re there for. I fell into this trap initially and would see tickets I could “just deal with”, but that’s not the value of a PM to the team.Currently I’ll take tickets only if they’ll help me understand the product, our users or the community better (such as documentation improvement or defect validation).Focus on the roleBeing a PM is a new experience for me. I’ve never done this job before and I don’t claim to be an expert at it. There is a lot to learn still and in order to do that time needs to be allocated to continuing education.I initially tried to “pick things up as I went”, but without proper focus and dedication you can get caught up in the whirlwind of the day. I block off an hour a day to just read, during which time I either focus on a particular book, scope documents, specifications, tickets, process documents, blog posts or other targeted content that will help me advance my understanding of the role.That’s it for now. If you’ve recently moved from Engineering to Product, or have a similar experience you want to share feel free to drop me a line and let me know how your journey is progressing." }, { "title": "Bug Hunting with the MongoDB Haskell Community", "url": "/blog/2022/09/21/bug-hunting-with-the-mongodb-haskell-community/", "categories": "MongoDB", "tags": "drivers, haskell, mongodb", "date": "2022-09-21 06:21:02 -0400", "snippet": "MongoDB currently maintains 10 programming language Drivers in-house, including a Ruby driver for which I’m presently the Product Manager. Additionally we also have a library of community maintaine...", "content": "MongoDB currently maintains 10 programming language Drivers in-house, including a Ruby driver for which I’m presently the Product Manager. Additionally we also have a library of community maintained drivers, built using the MongoDB Driver specifications our engineers maintain and publish.It was brought to my attention that one of these community drivers - the Haskell driver - was experiencing an issue whereby queries were no longer returning results from the MongoDB Atlas clusters their applications were connected to.Though I’ve never worked with Haskell, before joining the team I worked in Technical Services providing support for customers experiencing problems with their applications via our drivers. This seemed like an interesting problem we could hopefully solve for our developer community so I’d like to share the diagnostic journey that lead us to the issue and ultimately enabled a resolution.OverviewWhen Adrien first reported issue #131 on GitHub the initial assessment was that their application could successfully connect to a MongoDB Atlas cluster and write new content, but when trying to read those results back the result set was always empty. This had happened suddenly causing existing applications and workloads to break however no new code had been introduced which could potentially be the culprit.As I’m unfamiliar with Haskell Adrien kindly provided a Dockerized reproduction I could use to test this issue against my own Atlas clusters. This reproduction would write 3 documents to a collection, then try to read 3 documents back. To begin testing I setup an M10 cluster and ran the tests a few times.Failures: src/Lib.hs:101:33: 1) Reads Ensures reads work expected: 3 but got: 9Each time I ran the test it would fail, but the number of documents in the “Ensures reads work” that were received kept increasing. The cluster I was testing on was a dedicated cluster, however MongoDB Atlas also offers free and shared tier clusters so for completeness of testing I configured an M0 next and re-ran the tests.Failures: src/Lib.hs:101:33: 1) Reads Ensures reads work expected: 3 but got: 0No matter how many times I ran the tests against my M0 (also tested M2 and M5) the results were always 0.Just to make sure this wasn’t a larger issue I tested with a script that uses the Ruby driver against an M0 cluster to verify the behavior didn’t reproduce there:require 'bundler/inline'gemfile do source 'https://rubygems.org' gem 'mongo'endclient = Mongo::Client.new('mongodb+srv://....mongodb.net/test')collection = client[:foo]collection.dropputs \"Found #{collection.find.to_a.length} documents\"# =&gt; Found 0 documentscollection.insert_many([].fill({ \"bar\": \"baz\" },0,3))puts \"Found #{collection.find.to_a.length} documents\"# =&gt; Found 3 documentsThe script would produce the expected result, which further pointed to a potential issue on the Atlas side that was specific to free and shared tier clusters.MongoDB Atlas imposes some limitations on free and shared tier clusters, which in some cases are enforced by a proxy layer between the application and the underlying infrastructure backing the cluster.AnalysisNow that the issue was narrowed down, working with a Cloud Operations Engineer to create an isolated M2 cluster in a development environment, we increased the log verbosity for that cluster for QUERY and COMMAND log components.With this information, when we download logs for the node our test is targeting we should be able to get a lot more information as to what was being executed and where it might be failing.// Test #1{\"t\":{\"$date\":\"2022-08-18T11:19:16.985+00:00\"},\"s\":\"D2\", \"c\":\"COMMAND\", \"id\":5578800, \"ctx\":\"conn24194\",\"msg\":\"Deprecated operation requested. The client driver may require an upgrade in order to ensure compatibility with future server versions. For more details see https://dochub.mongodb.org/core/legacy-opcode-compatibility\",\"attr\":{\"op\":\"query\",\"clientInfo\":{\"driver\":{\"name\":\"mongo-go-driver\",\"version\":\"v1.7.2+prerelease\"},\"os\":{\"type\":\"linux\",\"architecture\":\"arm64\"},\"platform\":\"go1.18.2\",\"application\":{\"name\":\"Atlas Proxy v20220824.0.0.1660656950\"}}}}{\"t\":{\"$date\":\"2022-08-18T11:19:16.986+00:00\"},\"s\":\"D2\", \"c\":\"QUERY\", \"id\":20914, \"ctx\":\"conn24194\",\"msg\":\"Running query\",\"attr\":{\"query\":\"ns: 62fe1f7d37518e1c32149694_haskell.test123 query: { comment: { AtlasProxyAppName: \\\"\\\", AtlasProxyClientMetadata: {} } } sort: {} projection: {}\"}}{\"t\":{\"$date\":\"2022-08-18T11:19:16.986+00:00\"},\"s\":\"D5\", \"c\":\"QUERY\", \"id\":20917, \"ctx\":\"conn24194\",\"msg\":\"Not caching executor but returning results\",\"attr\":{\"numResults\":0}}Based on log analysis we could not only verify the issue existed, but why it was affecting these operations from the Haskell driver:A deprecated operation was being runMongoDB uses a wire protocol when sending/receiving messages internally and externally (via Drivers). Initially a number of opcodes existed, but starting with MongoDB 5.0 most of these were deprecated in favor of OP_MSG.Prior to MongoDB 3.6 when OP_MSG was introduced to subsume existing opcodes, query operations were executed via OP_QUERY, which the Haskell driver is apparently still using for query execution.Note that though OP_QUERY is deprecated, it would still be supported in the version of MongoDB we were testing (5.0) and as such is not the cause of this problem.The logs confirm no results are being returned by the queryAt the default level, the Database Profiler will only output queries to the mongod logs if they exceed the slow query threshold (slowms) of 100ms. The tests we were running would likely have completed in under 10ms, which prevented anything useful from being logged. {\"t\":{\"$date\":\"2022-08-18T11:19:16.986+00:00\"},\"s\":\"D5\", \"c\":\"QUERY\", \"id\":20917, \"ctx\":\"conn24194\",\"msg\":\"Not caching executor but returning results\",\"attr\":{\"numResults\":0}}Once the log level was increased it was apparent that the operation in question was being executed, but was not returning any results.The logs highlight an issue with the query itselfWith the log level increased however the QUERY component logs showed clearly that not only were no results being returned, but the query shape that was being sent to the server didn’t match what we expected: {\"t\":{\"$date\":\"2022-08-18T11:19:16.986+00:00\"},\"s\":\"D2\", \"c\":\"QUERY\", \"id\":20914, \"ctx\":\"conn24194\",\"msg\":\"Running query\",\"attr\":{\"query\":\"ns: 62fe1f7d37518e1c32149694_haskell.test123 query: { comment: { AtlasProxyAppName: \\\"\\\", AtlasProxyClientMetadata: {} } } sort: {} projection: {}\"}}It appeared that the query’s filter - which we expected to be empty - was in fact filtering for comment: { AtlasProxyAppName: \"\", AtlasProxyClientMetadata: {} }. Since none of the sample documents that were being created as part of this test matched these criteria, the query returned 0 results.FindingsFrom our log analysis it would appear our operations were being rewritten to append a filter criteria for a comment field with a value of { AtlasProxyAppName: \"\", AtlasProxyClientMetadata: {} }. As comment has a specific meaning within the context of MongoDB commands it was becoming apparent what the issue was and where it may have originated.Starting with MongoDB 4.4, a comment option was added to all database commands (see SERVER-29794).This was not be confused with the $comment meta operator that has been available since MongoDB 2.0 for propagating metadata to query logs.The Atlas team introduced a feature (released 2022-06-22) that would utilize these comments to improve the currentOp output in free/shared clusters. As all “official” MongoDB Drivers communicate with modern MongoDB clusters using OP_MSG, when this feature was being tested there were no issues.Unfortunately, drivers that still use OP_QUERY to make queries were negatively impacted as a result of the metadata comment injection occurring in the filter instead of one level above as is the case for OP_MSG.Now that the issue could be verified, additional logic was introduced to use the $comment meta operator if an OP_QUERY was detected instead of improperly applying a comment option.OutcomeWith the assistance of the Haskell community we were able to identify and address a deficiency in the free and shared tiers of MongoDB Atlas. The fix for this was released in version 8ed75a4810@v20220914 on 2022-09-21, and any Haskell application using the community maintained Haskell driver should have started working as expected without the need for additional intervention.We truly appreciate the investment our developer communities make when they put time and effort into building something as powerful as a MongoDB driver and want to ensure we do what we can to offer assistance if possible. Cross posted to DEV" }, { "title": "Solving a mongorestore failure due to 'Values in v:2 index key pattern cannot be of type object.'", "url": "/blog/2022/09/15/createindex-error-values-in-v-2-index-key-pattern-cannot-be-of-type-object/", "categories": "MongoDB", "tags": "mongodb, indexing, mongorestore, mongodump", "date": "2022-09-15 06:47:26 -0400", "snippet": "Starting with MongoDB 4.4, the MongoDB Database Tools are now released separately from the MongoDB Server and use their own versioning, with an initial version of 100.0.0. Previously, these tools w...", "content": "Starting with MongoDB 4.4, the MongoDB Database Tools are now released separately from the MongoDB Server and use their own versioning, with an initial version of 100.0.0. Previously, these tools were released alongside the MongoDB Server and used matching versioning.If you use mongodump to backup your data using these newer versions, if you try to use an older (pre-4.4) version of mongorestore you’ll likely get an error such as the following:$(m bin 4.0.28-ent)/mongorestore2022-09-15T07:09:28.775-0400\tusing default 'dump' directory2022-09-15T07:09:28.775-0400\tpreparing collections to restore from2022-09-15T07:09:28.777-0400\treading metadata for test.foo from dump/test/foo.metadata.json2022-09-15T07:09:28.777-0400\trestoring test.foo from dump/test/foo.bson2022-09-15T07:09:28.780-0400\trestoring indexes for collection test.foo from metadata2022-09-15T07:09:28.780-0400\tFailed: test.foo: error creating indexes for test.foo: createIndex error: Error in specification { ns: \"test.foo\", name: \"baz_1\", key: { baz: { $numberDouble: \"1.0\" } } } :: caused by :: Values in v:2 index key pattern cannot be of type object. Only numbers &gt; 0, numbers &lt; 0, and strings are allowed.In the example above we used the m version manager to try and restore a backup taken using a mongodump from a newer version of the database tools. If you see this error, it just means you need to use a newer version of mongorestore.To verify what version of the tools you’re using, run them with a --version parameter:$ $(m bin 4.0.28-ent)/mongorestore --versionmongorestore version: r4.0.28git version: af1a9dc12adcfa83cc19571cb3faba26eeddac92Go version: go1.11.13 os: darwin arch: amd64 compiler: gc$ mongorestore --versionmongorestore version: 100.6.0git version: 1d46e6e7021f2f5668763dba624e34bb39208cb0Go version: go1.17.10 os: darwin arch: amd64 compiler: gc" }, { "title": "Performance Profiling a Mongoid Issue Using AppProfiler", "url": "/blog/2022/09/09/performance-profiling-a-mongoid-issue-using-appprofiler/", "categories": "Ruby", "tags": "mongoid, ruby, rails, mongodb", "date": "2022-09-09 06:41:01 -0400", "snippet": "In MONGOID-4889 the claim was made that assignment of a large number of embedded documents to an instance of a model will take increasingly longer as the size of the list of documents to embed grow...", "content": "In MONGOID-4889 the claim was made that assignment of a large number of embedded documents to an instance of a model will take increasingly longer as the size of the list of documents to embed grows. This is notable as no database operations are being performed during this process, which points to potential issues with the library itself. The ticket author identified Mongoid::Association::Embedded::EmbedsMany::Proxy#object_already_related? as the likely source of this performance issue, however I wanted to see how best to validate this.While doing research on Ruby profiling I found Shopify’s blog post on “How to Fix Slow Code in Ruby”. Though the entire post was extremely insightful, it lead me to Shopify’s app_profiler library, which can be used to automatically profile code and redirect the output to a local instance of speedscope. Having worked previously with Flame Graphs of CPU stack traces collected using perf.Adapting the sample code in the Jira ticket results in the following:require 'bundler/inline'gemfile do source 'https://rubygems.org' gem 'mongoid' gem 'app_profiler'endclass Foo include Mongoid::Document embeds_many :barsendclass Bar include Mongoid::Document embedded_in :fooend# AppProfiler forms the output filename using Time.zone.nowrequire 'active_support/core_ext/time/zones'Time.zone = 'Pacific Time (US &amp; Canada)'AppProfiler.root = Pathname.new(__dir__)AppProfiler.profile_root = Pathname.new(__dir__)arr = Array.new(2000) { Bar.new }report = AppProfiler.run(mode: :cpu) do Foo.new.bars = arrendreport.viewRunning the above code doesn’t require a MongoDB connection string or an active cluster, but will attempt to embed 2000 new instances of Bar into the newly created instance of Foo. Once completed, the following chart is produced that reinforces the initial suspicion that the calls to object_already_related? are a likely candidate for further investigation.AppProfiler was designed to be injected into a Rails application, however as can be seen above, it can easily be adapted to work on a standalone Ruby script as well. Cross posted to DEV" }, { "title": "Why Use MongoDB with Ruby", "url": "/blog/2022/08/18/why-use-mongodb-with-ruby/", "categories": "MongoDB", "tags": "ruby, rails, mongodb, cross-post", "date": "2022-08-18 11:23:27 -0400", "snippet": " Cross posted from the MongoDB Developer CenterBefore discovering Ruby and Ruby on Rails I was a .NET developer. At that time I’d make ad-hoc changes to my development database, export my table/fu...", "content": " Cross posted from the MongoDB Developer CenterBefore discovering Ruby and Ruby on Rails I was a .NET developer. At that time I’d make ad-hoc changes to my development database, export my table/function/stored procedure/view definitions to text files and check them into source control with any code changes. Using diff functionality I’d compare the schema changes that the DBAs needed to apply to production and we’d script that out separately.I’m sure better tools existed (and I eventually started using some of RedGate’s tools), but I was looking for a change. At that time, the real magic of Ruby on Rails for me was the Active Record Migrations which made working with my database fit with my programming workflow. Schema management became less of a chore and there were rake tasks for anything I needed (applying migrations, rolling back changes, seeding a test database).Schema versioning and management with Rails was leaps and bounds better than what I was used to, and I didn’t think this could get any better - but then I found MongoDB.When working with MongoDB there’s no need to CREATE TABLE foo (id integer, bar varchar(255), ...); if a collection (or associated database) doesn’t exist, inserting a new document will automatically create it for you. This means Active Record migrations are no longer needed as this level of schema change management was no longer necessary.Having the flexibility to define my data model directly within the code without needing to resort to the intermediary management that Active Record had facilitated just sort of made sense to me. I could now persist object state to my database directly, embed related model details and easily form queries around these structures to quickly retrieve my data.Flexible SchemaData in MongoDB has a flexible schema as collections do not enforce a strict document structure or schema by default. This flexibility gives you data-modeling choices to match your application and its performance requirements, which aligns perfectly with Ruby’s focus on simplicity and productivity.Let’s Try It OutWe can easily demonstrate how to quickly get started using the MongoDB Ruby Driver using the following simple Ruby script that will connect to a cluster, insert a document and read it back:require 'bundler/inline'gemfile do source 'https://rubygems.org' gem 'mongo'endclient = Mongo::Client.new('mongodb+srv://username:password@mycluster.mongodb.net/test')collection = client[:foo]collection.insert_one({ bar: \"baz\" })puts collection.find.first# =&gt; {\"_id\"=&gt;BSON::ObjectId('62d83d9dceb023b20aff228a'), \"bar\"=&gt;\"baz\"}When the document above is inserted an _id value of BSON::ObjectId('62d83d9dceb023b20aff228a') is created. All documents must have an _id field, however if not provided a default _id of type ObjectId will be generated. When running the above you will get a different value for _id, or you may choose to explicitly set it to any value you like!Feel free to give the above example a spin using your existing MongoDB cluster or MongoDB Atlas cluster. If you don’t have a MongoDB Atlas cluster, sign up for an always free Free Tier cluster to get started.InstallationThe MongoDB Ruby Driver is hosted at RubyGems, or if you’d like to explore the source code it can be found on GitHub.To simplify the example above we used bundler/inline to provide a single-file solution using Bundler, however the mongo gem can be just as easily added to an existing Gemfile or installed via gem install mongo.Basic CRUD operationsOur sample above demonstrated how to quickly create and read a document. Updating and deleting documents are just as painless as shown below:# set a new field 'counter' to 1collection.update_one({ _id: BSON::ObjectId('62d83d9dceb023b20aff228a')}, :\"$set\" =&gt; { counter: 1 })puts collection.find.first# =&gt; {\"_id\"=&gt;BSON::ObjectId('62d83d9dceb023b20aff228a'), \"bar\"=&gt;\"baz\", \"counter\"=&gt;1}# increment the field 'counter' by onecollection.update_one({ _id: BSON::ObjectId('62d83d9dceb023b20aff228a')}, :\"$inc\" =&gt; { counter: 1 })puts collection.find.first# =&gt; {\"_id\"=&gt;BSON::ObjectId('62d83d9dceb023b20aff228a'), \"bar\"=&gt;\"baz\", \"counter\"=&gt;2}# remove the test documentcollection.delete_one({ _id: BSON::ObjectId('62d83d9dceb023b20aff228a') })Object Document MapperThough all interaction with your Atlas cluster can be done directly using the MongoDB Ruby Driver, most developers prefer a layer of abstraction such as an ORM or ODM. Ruby developers can use the Mongoid ODM to easily model MongoDB collections in their code and simplify interaction using a fluid API akin to Active Record’s Query Interface.The following example adapts the previous example to use Mongoid:require 'bundler/inline'gemfile do source 'https://rubygems.org' gem 'mongoid'endMongoid.configure do |config| config.clients.default = { uri: \"mongodb+srv://username:password@mycluster.mongodb.net/test\" }endclass Foo include Mongoid::Document field :bar, type: String field :counter, type: Integer, default: 1end# create a new instance of 'Foo', which will assign a default value of 1 to the 'counter' fieldfoo = Foo.create bar: \"baz\"puts foo.inspect# =&gt; &lt;Foo _id: 62d84be3ceb023b76a48df90, bar: \"baz\", counter: 1&gt;# interact with the instance variable 'foo' and modify fields programmaticallyfoo.counter += 1# save the instance of the model, persisting changes back to MongoDBfoo.save!puts foo.inspect# =&gt; &lt;Foo _id: 62d84be3ceb023b76a48df90, bar: \"baz\", counter: 2&gt;SummaryWhether you’re using Ruby/Rails to build a script/automation tool, a new web application or even the next Coinbase MongoDB has you covered with both a Driver that simplifies interaction with your data or an ODM that seamlessly integrates your data model with your application code.ConclusionInteracting with your MongoDB data via Ruby - either using the Driver or the ODM - is straightforward, but you can also directly interface with your data from MongoDB Atlas using the built in Data Explorer. Depending on your preferences though, there are options: MongoDB for Visual Studio Code allows you to connect to your MongoDB instance and enables you to interact in a way that fits into your native workflow and development tools. You can navigate and browse your MongoDB databases and collections, and prototype queries and aggregations for use in your applications. MongoDB Compass is an interactive tool for querying, optimizing, and analyzing your MongoDB data. Get key insights, drag and drop to build pipelines, and more. Studio 3T is an extremely easy to use 3rd party GUI for interacting with your MongoDB data. MongoDB Atlas Data API lets you read and write data in Atlas with standard HTTPS requests. To use the Data API, all you need is an HTTPS client and a valid API key. Ruby was recently added as a language export option to both MongoDB Compass and the MongoDB VS Code Extension. Using this integration you can easily convert an aggregation pipeline from either tool into code you can copy/paste into your Ruby application." }, { "title": "My Blog is FINALLY PROFITABLE!", "url": "/blog/2022/08/04/my-blog-is-finally-profitable/", "categories": "Personal", "tags": "blog", "date": "2022-08-04 08:42:24 -0400", "snippet": "I signed up for Google AdSense in January of 2015 in hopes of subsidizing some of the time I spent writing with ad-based revenue. Without doing any research I quickly formed some baseless expectati...", "content": "I signed up for Google AdSense in January of 2015 in hopes of subsidizing some of the time I spent writing with ad-based revenue. Without doing any research I quickly formed some baseless expectations of success and profitability that would see my sparsely populated, personal blog be a vehicle to financial freedom!Well humble reader … I DID IT! Ok, not really - AdSense has a payment threshold of $100, so until you earn at least that much nothing will ever be paid out. Fast forward 7 years and I finally drove enough traffic to be able to cash out for the first time.At this rate I’m only earing about $1.19 a month so I guess I won’t be quitting my day job … but hey, money’s money ;)" }, { "title": "Hello Product Management", "url": "/blog/2022/07/04/hello-product-management/", "categories": "Personal", "tags": "mongodb, career", "date": "2022-07-04 11:44:20 -0400", "snippet": "PastMy MongoDB Career Journey began almost 4 years ago, and I’ve enjoyed my time as a Technical Services Engineer immensely. During my tenure as part of the organization I had the opportunity to wo...", "content": "PastMy MongoDB Career Journey began almost 4 years ago, and I’ve enjoyed my time as a Technical Services Engineer immensely. During my tenure as part of the organization I had the opportunity to work with a number of high profile clients on some extremely challenging scenarios. These gave me a chance to write about some interesting aspects of the product such as FTDC internals, change stream resume performance, the impact of retryable writes on the oplog and replica set priority takeover mechanics.While I advanced from a TSE to a Senior TSE and finally a Lead TSE I had the opportunity to contribute back to the core server and related products by adding tickets to the backlog. These were predominantly identified via reproductions initiated as a result of a customer-facing issue, and gave me a great deal of personal satisfaction to report. For anyone interested in the types of issues we can help identify, among the 100+ product tickets I’ve opened in this time some of the most interesting were: SERVER-36870: Replication fails if server date exceeds January 19th, 2038 SERVER-44617: $regexFind crash when one of the capture group doesn’t match the input but pattern matches SERVER-44891: collStats will fail if resulting BSON size &gt; 16MB SERVER-57851: Optimize usersInfo calls from mongos to CSRS PRIMARY for Authz User Role resolution SERVER-59754: Incorrect logging of queryHash/planCacheKey for operations that share the same $lookup shape SERVER-62310: collMod command not sent to all shards for a sharded collection if no chunks have been receivedIf the type of digital detective work that goes into finding and reporting these types of issues is appealing to you then Technical Services is always on the lookout for new engineers ;) Have a look at the available opportunities in your region and tell them Alex sent you :)PresentAs much as I’ve loved my time in Technical Services, as of July 5th my journey moves in a different direction as I step into a new role as Product Manager, Developer Experience focusing on the Ruby Driver and Ruby Language Ecosystem.First of all, what is a Product Manager? According to Atlassian, a product manager is the person who identifies the customer need and the larger business objectives that a product or feature will fulfill, articulates what success looks like for a product, and rallies a team to turn that vision into a reality.Since this isn’t another engineering role I will be stepping out of my comfort zone, but feel I am extremely well positioned internally at MongoDB to be successful with a role that advocates for Ruby developers! Since joining the Technical Services team in 2018 I quickly began focusing on MongoDB Drivers focused cases, especially the MongoDB Ruby Driver and the Mongoid ODM.I’ve been working as a Ruby developer for more than a decade at this point, both as a Software Engineer building commercial applications and as a Technical Services Engineer helping MongoDB customers address production issues within their solutions. Heck, I’ve even written a book covering plugin development for a Ruby on Rails based project management suite.I fell in love with the language and found it adopted and incorporated a lot of the best practices, ideas and design patterns of other languages. My feelings towards the language and community somewhat echo what Matz (the author of the Ruby language) stated more succinctly in The Ruby Programming Language: I knew many languages before I created Ruby, but I was never fully satisfied with them. They were uglier, tougher, more complex, or more simple than I expected. I wanted to create my own language that satisfied me, as a programmer. I knew a lot about the language’s target audience: myself. To my surprise, many programmers all over the world feel very much like I do. They feel happy when they discover and program in Ruby. Throughout the development of the Ruby language, I’ve focused my energies on making programming faster and easier. All features in Ruby, including object-oriented features, are designed to work as ordinary programmers (e.g., me) expect them to work. Most programmers feel it is elegant, easy to use, and a pleasure to program.… Ruby is designed to make programmers happy._ FutureThis new role really feels like the culmination of a much longer journey that began when I started getting my feet wet with MongoDB in 2012, learned about Ruby drivers for MongoDB and was actively troubleshooting Mongoid issues as early as 2014.My love of Ruby and my love of MongoDB should help give me a leg up in this new role, however the real work now begins! My focus will be to help drive adoption of MongoDB within the Ruby developer community, though how I can move the needle here remains to be seen.I plan on sharing a lot more as this journey continues and I welcome any feedback you may have along the way." }, { "title": "Just Finished - Terranigma", "url": "/blog/2022/06/26/just-finished-terranigma/", "categories": "Gaming, Just Finished", "tags": "jrpg", "date": "2022-06-26 19:42:59 -0400", "snippet": "This has been a long time coming. I took a run at Terranigma in 2015 and failed miserably, but I was such a huge fan of the other Quintet titles for the SNES I just couldn’t leave this unfinished.T...", "content": "This has been a long time coming. I took a run at Terranigma in 2015 and failed miserably, but I was such a huge fan of the other Quintet titles for the SNES I just couldn’t leave this unfinished.The game tells the story of the Earth’s resurrection by the hands of a boy named Ark, and its progress from the evolution of life to the present day.It was published by Enix in Japan before Nintendo localized the game and released English, German, French and Spanish versions in Europe and Australia. The game has never been officially released in North America.Terranigma starts off with the protagonist Ark meeting a friendly demon named Yomi by opening Pandora’s Box, causing the citizens of the world to freeze. To unfreeze people he needs to resurrect the world. To do this he needs to travel through 5 “chapters” that see him resurrecting everything from plants, animals, people and even the wind.Mode 7 is used extensively in the overworld to great effect, which is not surprising for a game made this late in the SNES’ existence. This effect greets you as soon as you exit the first town and head for one of the 5 towers you need to beat in order to restore the continents.Once completed you have to jump into a chasm that spits you out on the “light side”, where you progress across the world (in a very linear fashion) clearing monster infested areas to keep reviving parts of the world. To progress past an “area” of the game you have to fight a screen-filling monster who typically has a fixed attack pattern and isn’t all that hard to beat.This brings us back to why I quit on this game in the first place. In my last playthrough I made it as far as Bloody Mary (or as one writer aptly put it: Bloody Fucking Mary) and was so under-levelled there was nothing but an ass kicking waiting for me every time I tried to take her on.The difficulty of this game doesn’t increase all that much as you progress through the first half, and as long as you buy the best weapons and armour you can find there’s little challenge. Bloody Mary ratcheted this up to 11 by requiring a minimum of level 25 to stand a chance. Playing this game “normally” up to this point would have you at level 18 at best, and grinding is a huge time sink.I was able to beat Bloody Mary (very under-levelled) by using save states and a lot of patience, but if playing on a console the challenge may prove impossible to overcome.As you advance through the game the beautiful graphics and soundtrack greet you at every turn. Eventually you make your way to the point where the world has been resurrected and you’ve dealt with all the “lightside / darkside plot twists” that may have been surprising at first but end up being over played when repeated too often …Being a 16-bit era JRPG the game is full of fetch quests and overly complicated puzzles. When the time comes to go on the Starstone fetch quest for example, one puzzle requires you to buy an extra flower from a girl in one city to give to a random penguin in Antarctica based on an extremely cryptic clue. Sure, if you’re paying attention you might remember this when the time comes … but it’s highly unlikely.Eventually you make your way to the end, fight Dark Gaia’s 2 forms and win. This again is an area that very much requires you to grind up to level 35 to deal with properly. I … didn’t make it that far so again resorted to save states and patience. Honestly some of these fights were tedious and repetitive, but to make them easier you need to grind a lot … which itself is tedious and repetitive.This was par for the course with 90’s JRPGs so I can’t really fault the developers for these tropes, but going back and playing these games now I would rather invest my time enjoying the experience instead of blindly killing enemies over and over.Overall I thoroughly enjoyed this game and found the story to be extremely well told. For anyone that enjoys these old games and grew up in North America, this is one you’ve likely never played and will not want to miss!" }, { "title": "MongoDB Driver Monitoring", "url": "/blog/2022/05/15/mongodb-driver-monitoring/", "categories": "MongoDB", "tags": "mongodb, drivers", "date": "2022-05-15 08:27:09 -0400", "snippet": "One of the great things about MongoDB Drivers is that they are all built around a common set of specifications. Though these specifications exist to facilitate the development of new language drive...", "content": "One of the great things about MongoDB Drivers is that they are all built around a common set of specifications. Though these specifications exist to facilitate the development of new language drivers or to consistently implement new features across drivers, being aware of them can help when it comes to troubleshooting issues.One of the more prominent specifications is the Server Discovery and Monitoring (SDAM), which defines a set of behaviour in the drivers for providing runtime information about server discovery and monitoring events. These events are further codified in the associated SDAM Monitoring Specification.Examples exist within the MongoDB documentation (ex: Node.js 3.6.x, Node.js 4.5.x, Java 4.3.x, C# 2.15.x), however I wanted to collect as many as possible in one place.Troubleshooting driver-level issues can be challenging, and the DRIVERS-1204: Easier debugging with standardized logging initiative exists to improve this however for the time being instrumenting your code is the best form of introspection.Node.jsStarting with version 2.1.10 of the Node.js driver SDAM Monitoring can be done by subscribing to various SDAM events. With version 3.2 and newer of the driver the Unified Topology Design was introduced, which is why the useUnifiedTopology flag is enabled in the sample below.Note that with version 4.0 of the Node.js driver all legacy topologies were removed and only the unified topology remains. For more examples see the Cluster Monitoring documentation.const { MongoClient } = require('mongodb');const uri = 'mongodb+srv://......';const client = new MongoClient(uri, { useUnifiedTopology: true});​// For debugging commandsclient.on('commandStarted', (event) =&gt; { // Will want to log the event somewhere here, ex: console.log('commandStarted', event)});client.on('commandFailed', (event) =&gt; { // Will want to log the event somewhere here.});client.on('commandSucceeded', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('serverDescriptionChanged', (event) =&gt; { // Will want to log the event somewhere here.});​// For debugging SDAM eventsclient.on('serverHeartbeatFailed', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('serverOpening', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('serverClosed', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('topologyOpening', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('topologyClosed', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('topologyDescriptionChanged', (event) =&gt; { // Will want to log the event somewhere here.});​// For debugging CMAP eventsclient.on('connectionCheckOutStarted', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('connectionCheckOutFailed', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('connectionCheckedIn', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('connectionPoolCleared', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('connectionClosed', (event) =&gt; { // Will want to log the event somewhere here.});​client.on('connectionPoolClosed', (event) =&gt; { // Will want to log the event somewhere here.});​await client.connect(uri);​​// Please modify each event listener above with the appropriate logging logic that will record the event name and result (ex: console.log('commandStarted', event)).JavaThe driver documentation covers Monitoring in detail, including examples such as the following:class CommandCounter implements CommandListener { private Map&lt;String, Integer&gt; commands = new HashMap&lt;String, Integer&gt;(); @Override public synchronized void commandSucceeded(final CommandSucceededEvent event) { String commandName = event.getCommandName(); int count = commands.containsKey(commandName) ? commands.get(commandName) : 0; commands.put(commandName, count + 1); System.out.println(commands.toString()); } @Override public void commandFailed(final CommandFailedEvent event) { System.out.println(String.format(\"Failed execution of command '%s' with id %s\", event.getCommandName(), event.getRequestId())); }}class CommandCounter implements CommandListener { private Map&lt;String, Integer&gt; commands = new HashMap&lt;String, Integer&gt;(); @Override public synchronized void commandSucceeded(final CommandSucceededEvent event) { String commandName = event.getCommandName(); int count = commands.containsKey(commandName) ? commands.get(commandName) : 0; commands.put(commandName, count + 1); System.out.println(commands.toString()); } @Override public void commandFailed(final CommandFailedEvent event) { System.out.println(String.format(\"Failed execution of command '%s' with id %s\", event.getCommandName(), event.getRequestId())); }}My colleague Jorge has an example of this at jorge-imperial/MongoTopologyMonitor.GoThe sample program below shows how you can use the CommandMonitor, PoolMonitor and ServerMonitor. The post “MongoDB Cluster Monitoring in Go” offers additional examples you may find useful.// go run poc.go 2&gt;&amp;1 | tee go_test_$(date +%s).logpackage mainimport (\t\"context\"\t\"encoding/json\"\t\"fmt\"\t\"os\"\t\"strings\"\t\"time\"\t\"github.com/google/uuid\"\t\"go.mongodb.org/mongo-driver/bson\"\t\"go.mongodb.org/mongo-driver/bson/primitive\"\t\"go.mongodb.org/mongo-driver/event\"\t\"go.mongodb.org/mongo-driver/mongo\"\t\"go.mongodb.org/mongo-driver/mongo/options\"\t\"go.mongodb.org/mongo-driver/mongo/readpref\")func pp(i interface{}) {\ts, _ := json.Marshal(i)\tfmt.Printf(\"%T: %s\\n\", i, string(s))}func main() {\tfmt.Println(\"starting\") // Your connection string here uri := \"mongodb+srv://...\"\t////////////////////////////// ctx := context.Background()\tcommand_monitor := &amp;event.CommandMonitor{\t\tStarted: func(ctx context.Context, evt *event.CommandStartedEvent) { pp(evt) },\t\tSucceeded: func(ctx context.Context, evt *event.CommandSucceededEvent) { pp(evt) },\t\tFailed: func(ctx context.Context, evt *event.CommandFailedEvent) { pp(evt) },\t}\tpool_monitor := &amp;event.PoolMonitor{\t\tEvent: func(evt *event.PoolEvent) {\t\t\tif strings.Contains(string(evt.Type), \"Failed\") {\t\t\t\tpp(evt)\t\t\t}\t\t},\t}\tserver_monitor := &amp;event.ServerMonitor{\t\tServerDescriptionChanged: func(evt *event.ServerDescriptionChangedEvent) { pp(evt) },\t\tServerOpening: func(evt *event.ServerOpeningEvent) { pp(evt) },\t\tServerClosed: func(evt *event.ServerClosedEvent) { pp(evt) },\t\tTopologyDescriptionChanged: func(evt *event.TopologyDescriptionChangedEvent) { pp(evt) },\t\tTopologyOpening: func(evt *event.TopologyOpeningEvent) { pp(evt) },\t\tTopologyClosed: func(evt *event.TopologyClosedEvent) { pp(evt) },\t\tServerHeartbeatStarted: func(evt *event.ServerHeartbeatStartedEvent) { pp(evt) },\t\tServerHeartbeatSucceeded: func(evt *event.ServerHeartbeatSucceededEvent) { pp(evt) },\t\tServerHeartbeatFailed: func(evt *event.ServerHeartbeatFailedEvent) { pp(evt) },\t}\tclient, err := mongo.NewClient(\t\toptions.Client().\t\t\tApplyURI(uri).\t\t\tSetPoolMonitor(pool_monitor).\t\t\tSetServerMonitor(server_monitor).\t\t\tSetMonitor(command_monitor))\tif err != nil {\t\tfmt.Printf(\"error creating client: %v\\n\", err)\t\tos.Exit(1)\t}\terr = client.Connect(ctx)\tif err != nil {\t\tfmt.Printf(\"error connecting: %v\\n\", err)\t\tos.Exit(1)\t}\terr = client.Ping(ctx, readpref.Primary())\tif err != nil {\t\tfmt.Printf(\"error pinging: %v\\n\", err)\t\tos.Exit(1)\t}\tfmt.Println(\"connected\")\tfor {\t\ttime.Sleep(2 * time.Second)\t\tfmt.Println(\"\")\t\tfmt.Println(time.Now().String())\t\tcollection := client.Database(\"test\").Collection(\"items\")\t\tid := uuid.New().String()\t\t_, err := collection.InsertOne(ctx, bson.D{\t\t\t{Key: \"uuid\", Value: id},\t\t})\t\tif err != nil {\t\t\tfmt.Printf(\"Insert error - %v\\n\", err)\t\t\tcontinue\t\t}\t\tfindResult := collection.FindOne(ctx, bson.D{\t\t\t{Key: \"uuid\", Value: id},\t\t})\t\tif findResult.Err() != nil {\t\t\tfmt.Printf(\"Find error: %v\\n\", err)\t\t\tcontinue\t\t}\t\tresult := struct {\t\t\tID primitive.ObjectID `bson:\"_id\"`\t\t\tUUID string `bson:\"uuid\"`\t\t}{}\t\terr = findResult.Decode(&amp;result)\t\tif err != nil {\t\t\tfmt.Printf(\"Error decoding: %v\\n\", err)\t\t}\t\tfmt.Printf(\"Got the result: %s\\n\", result.UUID)\t}}RubyThe Ruby Driver’s Monitoring documentation outlines how to create event subscribers in great detail. These are summarized in the sample application below.# ruby poc.rb 2&gt;&amp;1 | tee ruby_test_$(date +%s).logrequire 'bundler/inline'require 'securerandom'gemfile do source 'https://rubygems.org' gem 'mongo'endclass SDAMLogSubscriber include Mongo::Loggable def succeeded(event) log_debug(format_message(event.inspect)) end private def logger Mongo::Logger.logger end def format_message(message) format(\"SDAM | %s\".freeze, message) endendMongo::Logger.logger.level = Logger::DEBUGMongo::Monitoring::Global.subscribe(Mongo::Monitoring::CONNECTION_POOL, Mongo::Monitoring::CmapLogSubscriber.new)Mongo::Monitoring::Global.subscribe(Mongo::Monitoring::TOPOLOGY_OPENING, SDAMLogSubscriber.new)Mongo::Monitoring::Global.subscribe(Mongo::Monitoring::SERVER_OPENING, SDAMLogSubscriber.new)Mongo::Monitoring::Global.subscribe(Mongo::Monitoring::SERVER_DESCRIPTION_CHANGED, SDAMLogSubscriber.new)Mongo::Monitoring::Global.subscribe(Mongo::Monitoring::TOPOLOGY_CHANGED, SDAMLogSubscriber.new)Mongo::Monitoring::Global.subscribe(Mongo::Monitoring::SERVER_CLOSED, SDAMLogSubscriber.new)Mongo::Monitoring::Global.subscribe(Mongo::Monitoring::TOPOLOGY_CLOSED, SDAMLogSubscriber.new)# Your connection string hereclient = Mongo::Client.new('mongodb+srv://...')#############################loop do sleep 2 collection = client[:items] id = SecureRandom.uuid doc = { Key: \"uuid\", Value: id } begin collection.insert_one(doc) rescue =&gt; ex puts \"Insert error - #{ex}\\n\" end begin response = collection.find(doc).first puts \"Got the result: #{response}\\n\" rescue =&gt; ex puts \"Find error - #{ex}\\n\" endendC#/.NETThe C# driver allows the use of a MongoClientSettings.SdamLogFilename property to be set which will write most SDAM events to a log file without further configuration.In the example below we use the IEventSubscriber interface instead to build a custom event subscriber that can be used to emit event details to STDOUT.using MongoDB.Bson;using MongoDB.Driver;using MongoDB.Driver.Core.Events;using System;using System.Web.Script.Serialization;namespace ConsoleApp1{ internal class Program { private static void Main(string[] args) { var settings = MongoClientSettings.FromConnectionString(\"mongodb://localhost:27017/test\"); settings.ClusterConfigurator = builder =&gt; { builder.Subscribe(new CustomEventSubscriber()); }; var client = new MongoClient(settings); var database = client.GetDatabase(\"test\"); var collection = database.GetCollection&lt;BsonDocument&gt;(\"foo\"); var filter = Builders&lt;BsonDocument&gt;.Filter.Eq(\"bar\", 1); var output = collection.CountDocuments(filter); Console.WriteLine(output); // 1 Console.ReadKey(); } } public class CustomEventSubscriber : IEventSubscriber { private readonly IEventSubscriber _subscriber; private readonly JavaScriptSerializer _serializer; public CustomEventSubscriber() { _subscriber = new ReflectionEventSubscriber(this); _serializer = new JavaScriptSerializer(); } public bool TryGetEventHandler&lt;TEvent&gt;(out Action&lt;TEvent&gt; handler) { return _subscriber.TryGetEventHandler(out handler); } public void Handle(ClusterAddedServerEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterAddingServerEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterClosedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterClosingEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterDescriptionChangedEvent e) { Console.WriteLine($\"{e.GetType().Name}: Old = {e.OldDescription} / New = {e.NewDescription}\"); } public void Handle(ClusterOpenedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterOpeningEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterRemovedServerEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterRemovingServerEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterSelectedServerEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterSelectingServerEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ClusterSelectingServerFailedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(CommandFailedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionClosedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionClosingEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionCreatedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionFailedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionOpenedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionOpeningEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionOpeningFailedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolAddedConnectionEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolAddingConnectionEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolCheckedInConnectionEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolCheckedOutConnectionEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolCheckingInConnectionEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolCheckingOutConnectionEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolCheckingOutConnectionFailedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolClearedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolClearingEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolClosedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolClosingEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolOpenedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolOpeningEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolRemovedConnectionEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ConnectionPoolRemovingConnectionEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } //public void Handle(ConnectionReceivedMessageEvent e) //{ // Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); //} //public void Handle(ConnectionReceivingMessageEvent e) //{ // Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); //} public void Handle(ConnectionReceivingMessageFailedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } //public void Handle(ConnectionSendingMessagesEvent e) //{ // Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); //} public void Handle(ConnectionSendingMessagesFailedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } //public void Handle(ConnectionSentMessagesEvent e) //{ // Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); //} public void Handle(SdamInformationEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ServerClosedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ServerClosingEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ServerDescriptionChangedEvent e) { Console.WriteLine($\"{e.GetType().Name}: Old = {e.OldDescription} / New = {e.NewDescription}\"); } public void Handle(ServerHeartbeatFailedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {e.ConnectionId}\"); } public void Handle(ServerOpenedEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } public void Handle(ServerOpeningEvent e) { Console.WriteLine($\"{e.GetType().Name}: {_serializer.Serialize(e)}\"); } }}C++This example requires a little more setup as the necessary versions of bot the C Driver and C++ Driver need to be compiled and installed.# Setup environment for C/C++ driversexport WORKDIR=$(pwd)export CDRIVER_VERSION=1.17.4export CPPDRIVER_VERSION=3.6.2export LD_LIBRARY_PATH=/usr/local/libsudo apt-get update &amp;&amp; sudo apt-get install -y build-essential wget cmake git pkg-config libssl-dev libsasl2-devmkdir -p ${WORKDIR} &amp;&amp; cd ${WORKDIR}wget https://github.com/mongodb/mongo-c-driver/releases/download/${CDRIVER_VERSION}/mongo-c-driver-${CDRIVER_VERSION}.tar.gz &amp;&amp; \\ tar xzf mongo-c-driver-${CDRIVER_VERSION}.tar.gzcd ${WORKDIR}/mongo-c-driver-${CDRIVER_VERSION} &amp;&amp; \\ mkdir cmake-build &amp;&amp; \\ cd cmake-build &amp;&amp; \\ cmake -DENABLE_AUTOMATIC_INIT_AND_CLEANUP=OFF .. &amp;&amp; \\ make &amp;&amp; sudo make installcd ${WORKDIR}wget https://github.com/mongodb/mongo-cxx-driver/archive/r${CPPDRIVER_VERSION}.tar.gz &amp;&amp; \\ tar -xzf r${CPPDRIVER_VERSION}.tar.gzcd ${WORKDIR}/mongo-cxx-driver-r${CPPDRIVER_VERSION}/build &amp;&amp; \\ echo $CPPDRIVER_VERSION &gt; VERSION_CURRENT &amp;&amp; \\ cmake -DCMAKE_BUILD_TYPE=Release -DBSONCXX_POLY_USE_BOOST=1 -DENABLE_UNINSTALL=ON \\ -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_PREFIX_PATH=/usr/local .. &amp;&amp; \\ sudo cmake --build . --target installOnce the environment is setup, the following sample application can be used to monitor SDAM events. Note only the ServerDescriptionChanged is monitored, however this list can be expanded as needed (see mongocxx::options::apm Class Reference documentation for more information).// save as test.cpp and compile as follows:// c++ --std=c++11 test.cpp -o test $(pkg-config --cflags --libs libmongocxx)#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;bsoncxx/builder/stream/document.hpp&gt;#include &lt;bsoncxx/json.hpp&gt;#include &lt;mongocxx/client.hpp&gt;#include &lt;mongocxx/instance.hpp&gt;#include &lt;mongocxx/pool.hpp&gt;void work(mongocxx::pool&amp; pool) { auto conn = pool.acquire(); auto collection = (*conn)[\"test\"][\"foo\"];/*void work(const mongocxx::client&amp; conn) { bsoncxx::builder::stream::document document{}; auto collection = conn[\"test\"][\"foo\"];*/ bsoncxx::builder::stream::document document{}; document &lt;&lt; \"foo\" &lt;&lt; \"bar\"; document &lt;&lt; \"t\" &lt;&lt; bsoncxx::types::b_date(std::chrono::system_clock::now()); try { //collection.insert_one(document.view()); bsoncxx::builder::stream::document order{}; order &lt;&lt; \"_id\" &lt;&lt; -1; auto opts = mongocxx::options::find{}; opts.sort(order.view()); auto result = collection.find_one({}, opts); std::cout &lt;&lt; \"Last: \" &lt;&lt; bsoncxx::to_json(*result) &lt;&lt; std::endl; } catch (std::exception&amp; e) { std::cout &lt;&lt; \"Error: \" &lt;&lt; e.what() &lt;&lt; std::endl; }}int main(int, char**) { mongocxx::options::apm apm_opts; mongocxx::options::client client_opts; // http://mongocxx.org/api/current/classmongocxx_1_1options_1_1apm.html for other options apm_opts.on_server_changed([&amp;](const mongocxx::events::server_changed_event&amp; event) { std::cout &lt;&lt; \"ServerDescriptionChanged \" &lt;&lt; bsoncxx::to_json(event.new_description().is_master()) &lt;&lt; std::endl; }); client_opts.apm_opts(apm_opts); mongocxx::instance inst{}; mongocxx::uri uri{\"mongodb://...\"}; mongocxx::pool pool{uri, client_opts}; // mongocxx::client conn{uri, client_opts}; int n = 2; // wait 2 seconds between loops int milli_seconds = n * 1000; time_t start, end; start = time(0); std::cout &lt;&lt; \"Starting. New document will be inserted every \" &lt;&lt; n &lt;&lt; \" second(s).\" &lt;&lt; std::endl; while (1) { if (time(0) - start == n) { work(pool); start = start + n; std::cout &lt;&lt; \"Tick: \" &lt;&lt; start &lt;&lt; std::endl; } }}PHPTODO https://gist.github.com/jmikola/dfcad9bc4e512b22dbb04beed4dc0a99 https://www.php.net/manual/en/mongodb.tutorial.apm.phpThe goal will be to collect more examples over time and post them here. If you have anything you’d like to share that I haven’t covered, please feel free to comment below ;)Happy Coding!" }, { "title": "Blue Force (Tsunami Games) - 1993", "url": "/blog/2022/03/31/blue-force/", "categories": "Let's Adventure!", "tags": "adventure, Tsunami Games, TsAGE", "date": "2022-03-31 06:02:55 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Since I just finished playing Police Quest 1 I figured I’ll stick with the “police work simulato...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Since I just finished playing Police Quest 1 I figured I’ll stick with the “police work simulator” category and pick up Blue Force next.Blue Force was also designed by Police Quest designer Jim Walls so there are some thematic similarities between the titles. You have to use proper radio codes while on your bike when calling things in, and if you don’t call for backup appropriately you’ll likely get yourself killed.The story follows Jake Ryan, a rookie officer who’s parents were killed when he was a child and the murderer never caught. The game sees you investigating a number of incidents that seem to surround a national guard armory break in. These result in evidence surfacing that ties into Jake’s parents murder and the murderer being involved in the gun smuggling ring.A point-and-click interface is used to interact with the game world. Common actions such as Move, Interact, Inspect and Speak are present as icons in a circular menu that is raised when you right click. I’m not sure if Tsunami was the first adventure game company to adopt this style, but it does appear in other company’s games later on (such as Full Throttle from LucasArts).Navigating between locations is done by getting on your motorcycle and selecting a location from the game map. As you move the cursor about and hover over locations a tool tip will occasionally indicate this is somewhere you can travel. Other than the Marina which is easy to identify, figuring out where you need to go can be frustrating and unintuitive. I actually took the screenshot above as I kept forgetting where Jamison &amp; Ryan was and found myself blindly moving the mouse around trying to find it each time …The story progresses through various traffic stops and scripted events that your dispatcher sends you to. Proper police protocol needs to be followed when making an arrest or detaining a suspect, however this is extremely repetitive (talk, talk, talk, cuff, search). Luckily this isn’t a very long game so you only have to do this …. like 8 times.Interacting with other characters overlays their floating heads without a border or frame above the text area, which is actually a lot nicer than Sierra’s approach. The character designs are fairly detailed and the story advances quickly through these interactions.Your dad’s old partner, who’s been in your life since your parents murder is still running a PI firm. As you surface new evidence that ties into your parents murderer you being to work with him more to try and track down the gun smugglers.These investigations eventually lead to a stash of stolen automatic weapons, which you alert the ATF about. As you suspected, one of the main gun runners is the same guy that killed your parents, and the game ends with you finally bringing him to justice.Overall the story is pretty tight and interesting, and the game progresses smoothly. The plot guides you appropriately to the handful of locations on the map you need to explore, but since this is an adventure game there are no shortage of puzzles …. frustrating puzzles.Need to get a perp out of their locked house boat? Stick a rag, diesel fuel and a flare in the air intake to smoke him out. Want to investigate a locked houseboat? Bring a rare coin from your house to the boat rental shop and give it to the man there so he goes to the back room to get a price guide, which gives you an opportunity to steal the keys…Having played adventure games my whole life these types of puzzles aren’t uncommon or unexpected, but it never hurts to have a walkthrough handy just in case ;)The graphics are pretty good for this time period. Character portraits are well done, and your character on screen is generally large enough to appear smoothly animated and expressive. The background music and sound effects are a bit repetitive though, and can get annoying at times.Blue Force came out at the height of Sierra Online’s dominance of the adventure game market, and Sierra’s influence on this game’s design can definitely be seen. Though the game is fairly linear, I enjoyed the story and the overall game play experience.I’m looking forward to playing other Tsunami games as this series progresses.Game Information Game Blue Force Developer Tsunami Games Publisher Tsunami Games Release Date 1993 Systems DOS Game Engine TsAGE Play Information Time to Completion 2 hours How Long To Beat? 4 hours Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 16 Story (25) 18 Experience (15) 13 Impact (10) 7   77% Gallery" }, { "title": "Police Quest: In Pursuit of the Death Angel (Sierra On-Line) - 1987", "url": "/blog/2022/03/15/police-quest-1/", "categories": "Let's Adventure!", "tags": "adventure, Sierra Online", "date": "2022-03-15 06:39:40 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Police Quest: In Pursuit of the Death Angel is an adventure game (and police simulation) produce...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Police Quest: In Pursuit of the Death Angel is an adventure game (and police simulation) produced by Jim Walls for Sierra On-Line, and originally released in 1987 built on their AGI engine. It was remade in 1992 using 256-color VGA graphics and the SCI engine, which dramatically improved the appearance and audio of the game, and replaced the command line interface with point and click.Part of the goal of this series of reviews is to give me a chance to replay a bunch of games that meant a lot to me as a kid, and Police Quest was one of those. I loved the story, the characters, the text parser, having to rely on the manual for radio codes and booking codes … all of it.It’s been a good 20+ years since I played this last, so I figured I should start with the version I remembered. I think I got about 20 minutes into my play through of the AGI version before I gave up as the driving segments were … painful.I decided to switch over to the SCI version which has numerous quality of life improvements such as a point and click interface, simplified driving interface, ability to inspect buildings while driving so you know where you’re going … and less ways to die on the road. If you don’t stop at a stop sign you just back up and try again without needing to restore from a save.The plot follows officer Sonny Bonds, who is a traffic cop in Lytton, California. You start out by sitting through a briefing, getting in your squad car and patrolling the streets. You investigate a car crash, pull over and arrest a drunk driver, issue a speeding ticket and stop for coffee with a fellow officer.When you arrest someone and bring them in for booking, you have to enter the right booking codes … and there are generally multiple. I remember enjoying this long ago, but now I found it to be a bit tedious. Unfortunately, tedium was a running theme throughout this game.Every time you change your clothes (street clothes to uniform and back) you have to: enter your locker combo, take a towel, walk to the shower, walk back, enter your locker combo, put the towel back. EVERY TIME. And you change your clothes many times.Since you’re a traffic cop, you have to drive around looking for offenses (which you are alerted to after “a while” via radio), but in the meantime you’re just aimlessly driving around. A big part of this game is just driving back and forth between locations which feels like “busy work”.Don’t worry though, you’re only a traffic cop for about half the game as you also have an opportunity to apply for the narcotics unit. Apparently there’s a big drug problem in Lytton, and it turns out one of your fellow officer’s daughters gets caught up in this. I actually found this part of the story to be a lot more impactful replaying this as an adult (with kids of my own now) and give Sierra lot of credit for tackling something like this in a game.There is a new drug kingpin in town going by the name “The Death Angel” (hence the title of the game :P), and as you start to get closer to him after arresting one of his dealers you get a temporary promotion to narcotics. You get to go on a stakeout, arrest another dealer and do more “police stuff” (getting a judge to issue a no bail order for the dealer). A prostitute you meet in a bar turns out to be an old girlfriend of your from high school and she agrees to help you when you go undercover to try and get closer to the Death Angel.You dye your hair, go to the Hotel Delphoria (yup, more driving), have a few drinks at the bar and get invited to a high stakes poker game. After winning enough hands (or just clicking “No, But Win Anyway” when asked to start playing) you end up finding out that one of the players was Jesse Bains … aka the Death Angel.Bains finds out you’re a cop and pulls a gun. Assuming you remember to radio in the hotel room you’re in, backup will arrive and shoot Bains (or he’ll shoot you) and thus ends the first installment of the Police Quest series.I’ve got to be honest - it was not as much fun as I remember it being. The SCI version is exponentially more playable than the original AGI version, and though the story is compelling getting through this game feels like “work”. Sierra really tried hard to make this an accurate police operations and process simulator, but that results in long segments just being boring.I enjoyed the updated graphics such as the interior of the car while driving, but you’re stuck staring at that view for way too much of the game. The music and sound effects are forgettable, but still an improvement over the original. Since I have fond memories of all 4 police quest games (I don’t count Police Quest: SWAT). I’m still looking forward to replaying them, but I hope the rose coloured glasses I seem to have had on all these years for this game doesn’t apply to the whole series …Police Quest 1 was reported to have been used as a training tool for police officers, which can explain a lot of the rigidity. Unfortunately this hurts the overall play experience.Game Information Game Police Quest: In Pursuit of the Death Angel Developer Sierra On-Line Publisher Sierra On-Line Release Date 1987 Systems DOS, Amiga, Atari ST, Apple II, Apple IIGS Game Engine Adventure Game Interpreter (AGI)/Sierra’s Creative Interpreter (SCI) Play Information Time to Completion 3.5 hours How Long To Beat? 5 hours Version Played DOS (SCI Remake) via ScummVM Notes Walkthrough, Map ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 14 Story (25) 18 Experience (15) 9 Impact (10) 6   67% Gallery" }, { "title": "Performance Analysis of Resuming a MongoDB Change Stream", "url": "/blog/2022/03/02/performance-analysis-of-resuming-a-mongodb-change-stream/", "categories": "MongoDB", "tags": "mongodb, changestream, performance", "date": "2022-03-02 06:50:39 -0500", "snippet": "Change Streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog. Applications can use change streams to subscribe to all data changes on a si...", "content": "Change Streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog. Applications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them.For applications that rely on change streams, ensuring continuity on process restart can be accomplished by specifying a resume token to resume the change stream. Depending on how many events have been recorded in the oplog after the resume token, the time taken to resume the stream can take longer than expected.This article will attempt to explain the default behavior of change streams when resumed and how performance can potentially improved. Note that all scripts used in this article are shared in the Reproduction section at the end.Capturing Our First Change EventThe nodejs-capture-first-event.js script uses the MongoDB Node Driver to connect to the cluster we just created and listen for changes. First ensure you have Node.js installed, then using npm install the mongo package and run the script:npm install mongonode capture-first-event.jsOnce the script is watching the collection connect to the cluster and run the setupEnvironment() function to setup the cluster and insert a test document:mongo --quiet --eval \"load('shell-configure-test.js'); setupEnvironment();\"Once this document is inserted, the nodejs-capture-first-event.js we started previously should produce a result similar to the following, then exit:2022-02-15T12:13:26.173Z Change received: { _id: { _data: '82620B98E5000000022B022C0100296E5A1004437FB549CFDD45269DD59B9BF0EB354746645F69640064620B98E564DA118651C642000004' }, operationType: 'insert', clusterTime: new Timestamp({ t: 1644927205, i: 2 }), fullDocument: { _id: new ObjectId(\"620b98e564da118651c64200\"), msg: 'We expect our filter to match this' }, ns: { db: 'test', coll: 'foo' }, documentKey: { _id: new ObjectId(\"620b98e564da118651c64200\") }}Note the _id value of the change event as this will be used to resume the change stream later.Seeding the CollectionThe nodejs-capture-first-event.js script should have terminated after the change event was detected and printed. Next we want to fill the collection with content prior to attempting to resume processing, which can be done using our setup script from a mongo/mongosh shell:mongo --quiet --eval \"load('shell-configure-test.js'); seedCollection();\"The function above will push 3000 “junk” documents (~ 6MB in size) to the collection along with a few additional documents mixed in that should match our initial change stream filter. Once the collection is seeded, we can use the $collStats aggregation stage to get an idea as to how much data we’ve just generated. This should be run from a mongo/mongosh shell connected to the cluster:$ mongosh --quiet --eval \"db.foo.aggregate([ { $collStats: { storageStats: {} }}, { $project: { 'storageStats.wiredTiger': 0, 'storageStats.indexDetails': 0 } }]).pretty();\"[ { ns: 'test.foo', host: 'Alexs-MacBook-Pro.local:27018', localTime: ISODate(\"2022-02-15T21:36:13.752Z\"), storageStats: { size: Long(\"16993324327\"), // 16.99GB count: 2711, avgObjSize: 6268286, // 6.26MB storageSize: Long(\"17022169088\"), freeStorageSize: 17760256, capped: false, nindexes: 1, indexBuilds: [], totalIndexSize: 114688, totalSize: Long(\"17022283776\"), indexSizes: { _id_: 114688 }, scaleFactor: 1 } }]Resuming a Change Stream with a ResumeTokenChange streams can be resumed by using a ResumeToken. To resumeAfter you use the _id value of the last change stream event as this acts as the resumeToken. This can be inspected via the mongosh shell using the resumetoken snippet (see mongodb-js/mongodb-resumetoken-decoder)Enterprise replset [direct: primary] test&gt; decodeResumeToken('82620B98E5000000022B022C0100296E5A1004437FB549CFDD45269DD59B9BF0EB354746645F69640064620B98E564DA118651C642000004'){ timestamp: new Timestamp({ t: 1644927205, i: 2 }), version: 1, tokenType: 128, txnOpIndex: 0, fromInvalidate: false, uuid: new UUID(\"437fb549-cfdd-4526-9dd5-9b9bf0eb3547\"), documentKey: { _id: new ObjectId(\"620b98e564da118651c64200\") }}To use the resume token, the nodejs-resume-changestream.js script can be adjusted to use our token we found earlier.Performance Using Cursor DefaultsWhen the above script is run, the output should be similar to the following:$ node capture-first-event.js2022-03-02T11:38:37.014Z Resuming Change Stream ...2022-03-02T11:38:49.888Z Change received: \"This document will be 1Kb\" (token: 82621F54B3000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54B33284546A99670EFD0004)2022-03-02T11:38:49.888Z Change received: \"100 6MB documents, then another 1Kb document\" (token: 82621F54C4000000062B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54C43284546A99670F620004)2022-03-02T11:38:49.889Z Change received: \"And another 100 6MB documents, then another 1Kb document\" (token: 82621F54D4000000072B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54D43284546A99670FC70004)2022-03-02T11:38:49.889Z Change received: \"... followed immediately by a 1MB document\" (token: 82621F54D5000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54D53284546A99670FC80004)2022-03-02T11:38:49.889Z Change received: \"100 6MB documents preceded this 3MB document\" (token: 82621F54E8000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54E83284546A9967102D0004)2022-03-02T11:38:49.889Z Change received: \"... followed by another 1MB document\" (token: 82621F54E9000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54E93284546A9967102E0004)2022-03-02T11:38:49.890Z Change received: \"500 6MB documents added\" (token: 82621F5531000000072B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F55313284546A996712230004)2022-03-02T11:38:49.890Z Change received: \"200 6MB documents added\" (token: 82621F554D000000052B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712EC0004)2022-03-02T11:38:49.890Z Change received: \"Adding 2000 more 6MB documents...\" (token: 82621F554D000000062B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712ED0004)2022-03-02T11:38:49.891Z Change received: \"This is the last document we'd expect\" (token: 82621F5724000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F57243284546A99671ABE0004)Note that ~12 seconds elapse between the cursor being opened and the results all being returned apparently at once.Reviewing the logs for this operation show that a single aggregate command was executed that returned about 5MB of data (reslen) in 10 documents (nreturned). 3060 documents were scanned to identify these results and that required 19GB (bytesRead) to be read from disk into cache. {“t”:{“$date”:”2022-03-02T06:38:47.825-05:00”},”s”:”I”, “c”:”COMMAND”, “id”:51803, “ctx”:”conn53”,”msg”:”Slow query”,”attr”:{“type”:”command”,”ns”:”test.foo”,”command”:{\"aggregate\":\"foo\",”pipeline”:[{“$changeStream”:{“fullDocument”:”default”,”resumeAfter”:{“_data”:”82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004”}}},{“$match”:{“fullDocument.msg”:{“$exists”:true}}},{“$project”:{“fullDocument”:1}}],”cursor”:{},”lsid”:{“id”:{“$uuid”:”14c4f916-957f-4245-8106-b617b17fa603”}},”$clusterTime”:{“clusterTime”:{“$timestamp”:{“t”:1646221107,”i”:1}},”signature”:{“hash”:{“$binary”:{“base64”:”AAAAAAAAAAAAAAAAAAAAAAAAAAA=”,”subType”:”0”}},”keyId”:0}},”$db”:”test”},”planSummary”:”COLLSCAN”,”cursorid”:8835322801609070020,”keysExamined”:0,”docsExamined”:3039,”numYields”:636,\"nreturned\":10,”queryHash”:”7C2ADF3A”,\"reslen\":5248534,”locks”:{“Global”:{“acquireCount”:{“r”:640}},”Mutex”:{“acquireCount”:{“r”:4}}},”readConcern”:{“level”:”majority”},”writeConcern”:{“w”:”majority”,”wtimeout”:0,”provenance”:”implicitDefault”},”storage”:{“data”:{\"bytesRead\":19528768718,”timeReadingMicros”:9979872},”timeWaitingMicros”:{“cache”:4991}},”remote”:”192.168.2.100:61953”,”protocol”:”op_msg”,”durationMillis”:12356}}The entire operation took 12.36 seconds (durationMillis) to complete.Performance Using a Smaller Cursor batchSizeAccording to the previous log entry, all outstanding results following the resume token were returned in a single cursor batch. By default, find() and aggregate() operations have an initial batch size of 101 documents. Subsequent getMore operations issued against the resulting cursor have no default batch size, so they are limited only by the 16 megabyte message size (the BSON Max Size).Let’s try adjusting nodejs-resume-changestream.js with a cursor.batchSize() of 1, as this should return documents as they’re found.var changeStream = collection.watch([ { $match: { \"fullDocument.msg\": { $exists: true } } }, { $project: { fullDocument: 1 } } ], { resumeAfter: resumeToken, batchSize: 1 }); // &lt;-- add `batchSize: 1`2022-03-02T11:47:26.743Z Resuming Change Stream ...2022-03-02T11:47:27.952Z Change received: \"This document will be 1Kb\" (token: 82621F54B3000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54B33284546A99670EFD0004)2022-03-02T11:47:27.958Z Change received: \"100 6MB documents, then another 1Kb document\" (token: 82621F54C4000000062B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54C43284546A99670F620004)2022-03-02T11:47:27.962Z Change received: \"And another 100 6MB documents, then another 1Kb document\" (token: 82621F54D4000000072B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54D43284546A99670FC70004)2022-03-02T11:47:28.097Z Change received: \"... followed immediately by a 1MB document\" (token: 82621F54D5000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54D53284546A99670FC80004)2022-03-02T11:47:28.364Z Change received: \"100 6MB documents preceded this 3MB document\" (token: 82621F54E8000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54E83284546A9967102D0004)2022-03-02T11:47:28.463Z Change received: \"... followed by another 1MB document\" (token: 82621F54E9000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54E93284546A9967102E0004)2022-03-02T11:47:30.474Z Change received: \"500 6MB documents added\" (token: 82621F5531000000072B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F55313284546A996712230004)2022-03-02T11:47:31.234Z Change received: \"200 6MB documents added\" (token: 82621F554D000000052B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712EC0004)2022-03-02T11:47:31.239Z Change received: \"Adding 2000 more 6MB documents...\" (token: 82621F554D000000062B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712ED0004)2022-03-02T11:47:39.334Z Change received: \"This is the last document we'd expect\" (token: 82621F5724000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F57243284546A99671ABE0004)Unlike the first example that returned all results as they fit into the initial batch size, now a getMore is being issued for each result returned from the cursor. Checking the logs again we can verify this as we expect there to be 10 log entries associated with the change stream’s cursor id (8620417026431980160 in this case):{\"t\":{\"$date\":\"2022-03-02T06:47:26.367-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":309,\"numYields\":59,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":1618,\"locks\":{\"Global\":{\"acquireCount\":{\"r\":61}},\"Mutex\":{\"acquireCount\":{\"r\":2}}},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"storage\":{\"data\":{\"bytesRead\":1942084556,\"timeReadingMicros\":951431},\"timeWaitingMicros\":{\"cache\":3195}},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":1181}}{\"t\":{\"$date\":\"2022-03-02T06:47:26.375-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":0,\"numYields\":0,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":1636,\"locks\":{},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":0}}{\"t\":{\"$date\":\"2022-03-02T06:47:26.380-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":0,\"numYields\":0,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":1648,\"locks\":{},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":0}}{\"t\":{\"$date\":\"2022-03-02T06:47:26.385-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":0,\"numYields\":0,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":1049186,\"locks\":{},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":0}}{\"t\":{\"$date\":\"2022-03-02T06:47:26.521-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":0,\"numYields\":0,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":3146340,\"locks\":{},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":2}}{\"t\":{\"$date\":\"2022-03-02T06:47:26.790-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":1,\"numYields\":0,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":1049180,\"locks\":{\"Global\":{\"acquireCount\":{\"r\":1}},\"Mutex\":{\"acquireCount\":{\"r\":1}}},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"storage\":{\"data\":{\"bytesRead\":1048977,\"timeReadingMicros\":1974}},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":3}}{\"t\":{\"$date\":\"2022-03-02T06:47:28.885-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":501,\"numYields\":104,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":583,\"locks\":{\"Global\":{\"acquireCount\":{\"r\":105}},\"Mutex\":{\"acquireCount\":{\"r\":1}}},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"storage\":{\"data\":{\"bytesRead\":3246578426,\"timeReadingMicros\":1613240},\"timeWaitingMicros\":{\"cache\":2390}},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":2000}}{\"t\":{\"$date\":\"2022-03-02T06:47:29.651-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221647,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":201,\"numYields\":38,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":583,\"locks\":{\"Global\":{\"acquireCount\":{\"r\":39}},\"Mutex\":{\"acquireCount\":{\"r\":1}}},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"storage\":{\"data\":{\"bytesRead\":1289823601,\"timeReadingMicros\":625733},\"timeWaitingMicros\":{\"cache\":15}},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":754}}{\"t\":{\"$date\":\"2022-03-02T06:47:29.656-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221647,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":1,\"numYields\":0,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":593,\"locks\":{\"Global\":{\"acquireCount\":{\"r\":1}},\"Mutex\":{\"acquireCount\":{\"r\":1}}},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"storage\":{},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":0}}{\"t\":{\"$date\":\"2022-03-02T06:47:37.751-05:00\"},\"s\":\"I\", \"c\":\"COMMAND\", \"id\":51803, \"ctx\":\"conn66\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"test.foo\",\"command\":{\"getMore\":8620417026431980160,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221647,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"originatingCommand\":{\"aggregate\":\"foo\",\"pipeline\":[{\"$changeStream\":{\"fullDocument\":\"default\",\"resumeAfter\":{\"_data\":\"82621F5485000000022B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F54853284546A99670EFC0004\"}}},{\"$match\":{\"fullDocument.msg\":{\"$exists\":true}}},{\"$project\":{\"fullDocument\":1}}],\"cursor\":{\"batchSize\":1},\"lsid\":{\"id\":{\"$uuid\":\"4a230956-9cbe-48a9-9093-8c2fae2b659f\"}},\"$clusterTime\":{\"clusterTime\":{\"$timestamp\":{\"t\":1646221637,\"i\":1}},\"signature\":{\"hash\":{\"$binary\":{\"base64\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"subType\":\"0\"}},\"keyId\":0}},\"$db\":\"test\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":8620417026431980160,\"keysExamined\":0,\"docsExamined\":2024,\"numYields\":411,\"nreturned\":1,\"queryHash\":\"7C2ADF3A\",\"reslen\":597,\"locks\":{\"Global\":{\"acquireCount\":{\"r\":412}},\"Mutex\":{\"acquireCount\":{\"r\":1}}},\"readConcern\":{\"level\":\"majority\"},\"writeConcern\":{\"w\":\"majority\",\"wtimeout\":0,\"provenance\":\"implicitDefault\"},\"storage\":{\"data\":{\"bytesRead\":13017774068,\"timeReadingMicros\":6513189},\"timeWaitingMicros\":{\"cache\":19677}},\"remote\":\"192.168.2.100:53792\",\"protocol\":\"op_msg\",\"durationMillis\":8090}}From start to finish both approaches will take approximately the same amount of time, however from an application responsiveness point of view processing events as they’re found compared to waiting for a batch is likely a better user experience.How Internal Aggregation Batching Logic Affects batchSizeExpanding on this further, let’s say we wanted to resume after the third last entry (“200 6MB documents added”):2022-03-02T11:47:31.234Z Change received: \"200 6MB documents added\" (token: 82621F554D000000052B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712EC0004)2022-03-02T11:47:31.239Z Change received: \"Adding 2000 more 6MB documents...\" (token: 82621F554D000000062B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712ED0004)2022-03-02T11:47:39.334Z Change received: \"This is the last document we'd expect\" (token: 82621F5724000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F57243284546A99671ABE0004)To do this we’d supply the resumeToken (82621F554D000000052B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712EC0004 as seen above) to the code sample and run again. The expectation in this case would be that “Adding 2000 more 6MB documents…“ would almost immediately return, followed after a brief delay by “This is the last document we’d expect”, however when we run the code … that’s not what we see. Instead after 6+ seconds both documents are returned:2022-03-02T12:15:49.134Z Resuming Change Stream ...2022-03-02T12:15:55.831Z Change received: \"Adding 2000 more 6MB documents...\" (token: 82621F554D000000062B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712ED0004)2022-03-02T12:15:55.838Z Change received: \"This is the last document we'd expect\" (token: 82621F5724000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F57243284546A99671ABE0004)As an optimization, MongoDB’s query engine internally caches data from a cursor before pipeline processing. This is controlled by the internalDocumentSourceCursorBatchSizeBytes query execution knob which defaults to 4MB (lowered from 16MB in MongoDB 3.4.2 via SERVER-27406).We can verify this tuneable is in fact affecting the behavior of our change stream by lowering the value from 4194304 to 128 (via the mongosh shell):db.adminCommand({ setParameter: 1, internalDocumentSourceCursorBatchSizeBytes: 128});After making this change, resuming our change stream returns “Adding 2000 more 6MB documents…“ almost instantly whereas “This is the last document we’d expect” returns 7 seconds later.2022-03-02T14:11:41.416Z Resuming Change Stream ...2022-03-02T14:11:41.445Z Change received: \"Adding 2000 more 6MB documents...\" (token: 82621F554D000000062B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F554D3284546A996712ED0004)2022-03-02T14:11:48.054Z Change received: \"This is the last document we'd expect\" (token: 82621F5724000000012B022C0100296E5A1004D9EC8991B42F4F71BA61FC5BA26E2DED46645F69640064621F57243284546A99671ABE0004) Though this improves the performance of our isolated test, this batching behavior is in place for a reason (one example outlined in SERVER-27829). Changing internalDocumentSourceCursorBatchSizeBytes in production may adversely affect other workloads and would not be advisable.SummaryIf you’re using MongoDB Change Streams and filtering for events that occur infrequently (compared to other activity within the oplog) resuming the change stream may appear “sluggish” using the defaults. Consider specifying a custom batchSize based on your workload to potentially improve the time to returning the first event.Let me know in the comments below if you found this article helpful :)ReproductionThe scripts used in this article can be found below. I configured a local environment running a MongoDB 5.2.0 3 node replica set using the m version manager as well as mtools.m 5.2.0-entmlaunch init --replicaset --nodes 3 --bind_ip_all --binarypath $(m bin 5.2.0-ent)To use the scripts below save them locally and execute the associated commands from the same directory.shell-configure-test.jsThis script for the mongo/mongosh shell contains the helper functions used in this article to configure the environment and generate the write load.// filename: shell-configure-test.js//function setupEnvironment() { // set the oplog to at least 20GB (20480MB) so our workload doesn't roll out db.adminCommand({ replSetResizeOplog: 1, size: 20480 }); // insert one document and observe the result from the change stream cursor db.getSiblingDB(\"test\").foo.insertOne({ msg: \"We expect our filter to match this\" });}function randomString(length) { var result = ''; var characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'; var charactersLength = characters.length; for ( var i = 0; i &lt; length; i++ ) { result += characters.charAt(Math.floor(Math.random() * charactersLength)); } return result;}function writeJunk(count, stringLength) { print(\"Pushing \" + count + \" junk docs of size \" + stringLength); var data = []; // doesn't matter what the string is so just reuse it var string = randomString(stringLength); for (var i = 0; i &lt; count; i++) { data.push({ i: i, s: string }); } db.getSiblingDB(\"test\").foo.insertMany(data);}function seedCollection() { db.foo.insertOne({ msg: \"This document will be 1Kb\", s: randomString(1024) }); writeJunk(100, 1048576 * 6); db.foo.insertOne({ msg: \"100 6MB documents, then another 1Kb document\", s: randomString(1024) }); writeJunk(100, 1048576 * 6); db.foo.insertOne({ msg: \"And another 100 6MB documents, then another 1Kb document\", s: randomString(1024) }); db.foo.insertOne({ msg: \"... followed immediately by a 1MB document\", s: randomString(1024 * 1024) }); writeJunk(100, 1048576 * 6); db.foo.insertOne({ msg: \"100 6MB documents preceded this 3MB document\", s: randomString(1024 * 1024 * 3) }); db.foo.insertOne({ msg: \"... followed by another 1MB document\", s: randomString(1024 * 1024) }); writeJunk(500, 1048576 * 6); db.foo.insertOne({ msg: \"500 6MB documents added\" }); writeJunk(200, 1048576 * 6); db.foo.insertOne({ msg: \"200 6MB documents added\" }); db.foo.insertOne({ msg: \"Adding 2000 more 6MB documents...\" }); writeJunk(2000, 1048576 * 6); db.foo.insertOne({ msg: \"This is the last document we'd expect\" });}nodejs-capture-first-event.jsThis Node.js script should be used to listen for the first change event from which we’ll extract a resume token for later use.// file: nodejs-capture-first-event.js//// Configure the `MongoClient` with connection details appropriate to your environmentconst { MongoClient } = require(\"mongodb\");const client = new MongoClient(\"mongodb://localhost:27017/test?replicaSet=replset\");async function run() { await client.connect(); const database = client.db(\"test\"); const collection = database.collection(\"foo\"); var changeStream = collection.watch([ { $match: { \"fullDocument.msg\": { $exists: true } } }]); changeStream.on(\"change\", next =&gt; { console.log(`${new Date().toISOString()} Change received: `, next); process.exit(0); });}run().catch(console.dir);nodejs-resume-changestream.jsThe resume token found when running nodejs-capture-first-event.js can be plugged into this script to capture matching events following the token.// file: nodejs-resume-changestream.js//// Configure the `MongoClient` with connection details appropriate to your environmentconst { MongoClient } = require(\"mongodb\");const client = new MongoClient(\"mongodb://localhost:27017/test?replicaSet=replset\");async function run() { await client.connect(); const database = client.db(\"test\"); const collection = database.collection(\"foo\"); // replace the resume token with the value from your own tests var resumeToken = { _data: '82620B98E5000000022B022C0100296E5A1004437FB549CFDD45269DD59B9BF0EB354746645F69640064620B98E564DA118651C642000004' } console.log(`${new Date().toISOString()} Resuming Change Stream ...`); var changeStream = collection.watch([ { $match: { \"fullDocument.msg\": { $exists: true } } }, { $project: { fullDocument: 1 } } ], { resumeAfter: resumeToken }); changeStream.on(\"change\", next =&gt; { console.log(`${new Date().toISOString()} Change received: ${next.fullDocument.msg} (token: ${next._id._data})`); });}run().catch(console.dir);" }, { "title": "Cleaning Up ETL Results in MongoDB by Transposing Multiple Arrays", "url": "/blog/2022/02/07/cleaning-up-etl-results-in-mongodb-by-transposing-multiple-arrays/", "categories": "MongoDB", "tags": "mongodb, aggregation, etl", "date": "2022-02-07 09:05:00 -0500", "snippet": "When performing an ETL from a normalized relational dataset there’s a good chance a 1:1 conversion won’t produce the desired results on the first pass. For example, if the goal is to Model One-to-M...", "content": "When performing an ETL from a normalized relational dataset there’s a good chance a 1:1 conversion won’t produce the desired results on the first pass. For example, if the goal is to Model One-to-Many Relationships with Embedded Documents but the dataset contains a number of relationships mapped to individual fields as arrays of scalar values, you’ll likely want to convert these to subdocuments to facilitate access and interaction from your applications.In this example, our data has been imported from a legacy system with the above design, and has produced documents in a punch_cards collection with the following schema:{ \"date\": \"December 1, 2020\", \"category\": \"AM\", \"events\": { \"employee\": [ \"Alex\", \"Max\", \"Will\", \"Sara\" ], \"action\": [ \"Punched In\", \"Punched In\", \"Punched Out\", \"Punched In\" ], \"timestamp\": [ \"2020/12/01 08:01\", \"2020/12/01 07:58\", \"2020/12/01 09:03\", \"2020/12/01 09:59\"] }},{ \"date\": \"December 1, 2020\", \"category\": \"PM\", \"events\": { \"employee\": [ \"Alex\", \"Max\", \"Sara\", \"Will\" ], \"action\": [ \"Punched Out\", \"Punched Out\", \"Punched Out\", \"Punched In\" ], \"timestamp\": [ \"2020/12/01 16:00\", \"2020/12/01 16:30\", \"2020/12/01 20:00\", \"2020/12/01 23:58\"] }} The initial schema is a result of limitations with the initial import strategy. The goals of this article are to showcase how these limitations an be overcome once the initial ETL from source system to MongoDB has been completed.The desired end state is a document with all events mapped to an array of subdocuments:{ \"events\" : [ { \"employee\" : \"Alex\", \"action\" : \"Punched Out\", \"timestamp\" : \"2020/12/01 16:00\" }, { \"employee\" : \"Max\", \"action\" : \"Punched Out\", \"timestamp\" : \"2020/12/01 16:30\" }, { \"employee\" : \"Sara\", \"action\" : \"Punched Out\", \"timestamp\" : \"2020/12/01 20:00\" }, { \"employee\" : \"Will\", \"action\" : \"Punched In\", \"timestamp\" : \"2020/12/01 23:58\" } ]}Using MongoDB’s Aggregation functionality there are multiple ways to produce the desired results, two of which I’d like to share below.The “Easy” WayStarting in MongoDB 3.4 the $zip operator was introduced, which could be used to transpose an array of input arrays so that the first element of the output array would be an array containing, the first element of the first input array, the first element of the second input array, etc. If only $zip is used the resulting documents would appear as an array of arrays:db.punch_cards.aggregate([{ $project: { events: { $zip: { inputs: [ \"$events.employee\", \"$events.action\", \"$events.timestamp\" ]} }}}]);// output{ \"events\" : [ [ \"Alex\", \"Punched Out\", \"2020/12/01 16:00\" ], [ \"Max\", \"Punched Out\", \"2020/12/01 16:30\" ], [ \"Sara\", \"Punched Out\", \"2020/12/01 20:00\" ], [ \"Will\", \"Punched In\", \"2020/12/01 23:58\" ] ]}By providing the output of the $zip as the input to a $map the results can be easily rewritten to match our desired schema:db.punch_cards.aggregate([{ $project: { events: { $map: { input: { $zip: { inputs: [ \"$events.employee\", \"$events.action\", \"$events.timestamp\" ]} }, as: \"zipped\", in: { employee: { $arrayElemAt: [ \"$$zipped\", 0 ] }, action: { $arrayElemAt: [ \"$$zipped\", 1 ] }, timestamp: { $arrayElemAt: [ \"$$zipped\", 2 ] } } } }}}]); These pipeline examples only project the events field. To include additional fields (ex: date, category) these would have to be included in the $project stage explicitly.&lt;/p&gt;The “Hard” WayAssuming you’re running MongoDB 3.2 or earlier (which is highly unlikely) and don’t have access to the $zip operator, a more complex aggregation pipeline can be created that $unwinds each array, then tags each document emitted with a field indicating if all results are from the same array index for each document, then filters out matches and re-$groups them:db.punch_cards.aggregate([{ $unwind: { path: \"$events.employee\", includeArrayIndex: \"idx01\" } },{ $unwind: { path: \"$events.action\", includeArrayIndex: \"idx02\" } },{ $unwind: { path: \"$events.timestamp\", includeArrayIndex: \"idx03\" } },{ $project: { events: 1, keep: { $cond: { if: { $and: [ { $eq: [\"$idx01\", \"$idx02\" ] }, { $eq: [\"$idx02\", \"$idx03\" ] } , { $eq: [\"$idx03\", \"$idx01\" ] } ] }, then: true, else: false } }}},{ $match: { keep: true } },{ $group: { _id: \"$_id\", events: { $push: \"$events\" }}},]);I’ve included two variations of the pipeline to illustrate the different approaches you can take to solve the same problem. Depending on your use case the “hard” way may be more appropriate, however the “easy” way requires far less processing and should be more performant as a result.Updating the DataThe pipeline examples above don’t acctually writing any changes back to disk. This is by design to ensure no copy/paste errors result in unanticipated data loss as a result.Once you are satisfied with the transformations and are ready to write the results, either an $out or $merge stage can be added as the final stage in the pipeline." }, { "title": "Generate MongoDB Index Utilization Report", "url": "/blog/2022/01/24/generate-mongodb-index-utilization-report/", "categories": "MongoDB", "tags": "mongodb, queries, indexing", "date": "2022-01-24 09:49:50 -0500", "snippet": "When MongoDB 3.2 introduced the $indexStats aggregation pipeline stage accesses details were suddenly accessible to users. As a result, scripts could now be written to better understand how frequen...", "content": "When MongoDB 3.2 introduced the $indexStats aggregation pipeline stage accesses details were suddenly accessible to users. As a result, scripts could now be written to better understand how frequently indexes were being accessed by operations.The following script will cycle through all databases and collections (omitting admin, local and config) to produce a delimited report of index utilization:var DELIMITER = '\\t';var IGNORE = [\"admin\", \"local\", \"config\"];print([\"Namespace\", \"Index Name\", \"Usage Count\", \"Last Used\", \"Index Size (bytes)\", \"Index Specification\"].join(DELIMITER));db.getMongo().getDBNames().forEach(function (dbname) { if (IGNORE.indexOf(dbname) &lt; 0) { db.getSiblingDB(dbname).getCollectionNames().forEach(function (cname) { if (!cname.includes(\"system.\")) { var coll = db.getSiblingDB(dbname).getCollection(cname); var stats = coll.stats(); // make sure stats ran successfully (if it's a view it won't) if (stats.ok == 1) { coll.aggregate([{ $indexStats: {} }]).forEach(function (ix) { var ixname = ix.name; var ns = dbname + \".\" + cname; var ixsize = stats.indexSizes[ixname]; var ops = ix.accesses.ops; var since = ix.accesses.since; print([ns, ixname, ops, since, ixsize, JSON.stringify(ix.spec)].join(DELIMITER)); }); } } }); }});For example, I ran the above against a test cluster I have in MongoDB Atlas with a DELIMITER set as a pipe character (|) to facilitate the generation of a Markdown table such as the following: Namespace Index Name Usage Count Last Used Index Size (bytes) Index Specification data.users age_1_address.state_1_name_1 NumberLong(0) Tue Jan 18 2022 14:05:40 GMT-0500 (Eastern Standard Time) 48164864 {“v”:2,”key”:{“age”:1,”address.state”:1,”name”:1},”name”:”age_1_address.state_1_name_1”,”ns”:”data.users”} data.users id NumberLong(0) Tue Jan 18 2022 14:05:40 GMT-0500 (Eastern Standard Time) 34496512 {“v”:2,”key”:{“id”:1},”name”:”_id”,”ns”:”data.users”} data.users address.state_1_name_1_age_1 NumberLong(0) Tue Jan 18 2022 14:05:40 GMT-0500 (Eastern Standard Time) 43565056 {“v”:2,”key”:{“address.state”:1,”name”:1,”age”:1},”name”:”address.state_1_name_1_age_1”,”ns”:”data.users”} data.users age_1 NumberLong(0) Tue Jan 18 2022 14:05:40 GMT-0500 (Eastern Standard Time) 13721600 {“v”:2,”key”:{“age”:1},”name”:”age_1”,”ns”:”data.users”} encryption.__keyVault id NumberLong(0) Tue Jan 18 2022 14:05:40 GMT-0500 (Eastern Standard Time) 36864 {“v”:2,”key”:{“id”:1},”name”:”_id”} medicalRecords.patients id NumberLong(0) Tue Jan 18 2022 14:05:40 GMT-0500 (Eastern Standard Time) 36864 {“v”:2,”key”:{“id”:1},”name”:”_id”} As indexes aren’t free (see “Indexing Strategies”) dropping unused indexes will allow you to reclaim some disk space and potentially improve write throughput. In a replica set the output would be for the current node you’re connected to (likely the PRIMARY). Before dropping indexes ensure you review the output above for all data bearing nodes as some workloads may only target SECONDARY members, which would result in (likely) lower usage statistics on the PRIMARY.Note that the Last Used values are reset when a mongod is restarted. If the Usage Count is 0, the Last Used value will indicate the time the process was started; not when that index was actually last used. The output of the Last Values above will be in your local timezone. For more information see the MDN Docs for Date.Let me know if you find this script useful in the comments below ;)" }, { "title": "What Versions of MongoDB Has This Node Run?", "url": "/blog/2022/01/04/what-versions-of-mongodb-has-this-node-run/", "categories": "MongoDB", "tags": "mongodb", "date": "2022-01-04 15:33:44 -0500", "snippet": "Ever wanted to know how many different versions of MongoDB the current node has run under? Assuming the node’s local database hasn’t been reset (for example via an initial sync), it will contain a ...", "content": "Ever wanted to know how many different versions of MongoDB the current node has run under? Assuming the node’s local database hasn’t been reset (for example via an initial sync), it will contain a startup_log collection that holds this information. On startup, each mongod instance inserts a document into the startup_log capped collection (capped at 10MB) with diagnostic information about the mongod instance itself and host information.This collection can be used to generate a report using this diagnostic information using the following script run from a mongo or mongosh shell connected to your cluster:var lastVersion = null;print(\"Process Last Started\\tMongoDB Version\\tCommand Line Options\");db.getSiblingDB(\"local\").startup_log.find({}).sort({ startTime: -1 }).forEach(function(d) { if (d.buildinfo.version != lastVersion) { lastVersion = d.buildinfo.version; print([d.startTime.toUTCString(), lastVersion, JSON.stringify(d.cmdLine)].join('\\t')); }});The output above will return tab-delimited results, however these could be easily updated to produce a CSV or Markdown table similar to the following: Process Last Started MongoDB Version Command Line Options Fri, 10 Dec 2021 19:18:30 GMT 4.4.10 { “config”: … } Thu, 14 Oct 2021 18:08:53 GMT 4.4.9 { “config”: … } Wed, 08 Sep 2021 18:22:50 GMT 4.4.8 { “config”: … } Sat, 10 Jul 2021 13:33:24 GMT 4.4.6 { “config”: … } Thu, 06 May 2021 18:12:19 GMT 4.4.5 { “config”: … } Sat, 27 Mar 2021 18:10:53 GMT 4.4.4 { “config”: … } Thu, 18 Mar 2021 18:47:19 GMT 4.2.12 { “config”: … } Thu, 21 Jan 2021 20:47:22 GMT 4.2.11 { “config”: … } Sat, 21 Nov 2020 15:41:39 GMT 4.2.10 { “config”: … } Wed, 16 Sep 2020 09:58:36 GMT 4.2.9 { “config”: … } Sun, 16 Aug 2020 10:52:48 GMT 4.2.8 { “config”: … } Sat, 30 May 2020 18:13:48 GMT 4.2.6 { “config”: … } Thu, 28 May 2020 18:23:28 GMT 4.2.7 { “config”: … } Thu, 21 May 2020 18:15:00 GMT 4.2.6 { “config”: … } Thu, 02 Apr 2020 18:27:14 GMT 4.2.5 { “config”: … } Thu, 02 Apr 2020 11:50:29 GMT 4.2.3 { “config”: … } Thu, 02 Apr 2020 11:47:16 GMT 4.0.16 { “config”: … } Wed, 01 Apr 2020 19:26:17 GMT 3.6.17 { “config”: … } Mon, 06 Jan 2020 19:19:03 GMT 3.6.16 { “config”: … } Tue, 26 Nov 2019 19:01:22 GMT 3.6.15 { “config”: … } Fri, 08 Nov 2019 19:30:40 GMT 3.6.14 { “config”: … } Fri, 26 Jul 2019 19:12:13 GMT 3.6.13 { “config”: … } Tue, 04 Jun 2019 19:19:59 GMT 3.6.12 { “config”: … } Thu, 28 Mar 2019 19:21:07 GMT 3.6.11 { “config”: … } I ran this against one of my development MongoDB Atlas clusters to show how version information persists regardless of the order of upgrade or major/minor version used." }, { "title": "Troubleshooting 'MongoDB Cursor xxxxxx not found' Errors", "url": "/blog/2021/12/29/troubleshooting-mongodb-cursor-xxxxxx-not-found-errors/", "categories": "MongoDB", "tags": "mongodb, queries", "date": "2021-12-29 09:26:53 -0500", "snippet": "Read operations that return multiple documents do not immediately return all values matching the query. Because a query can potentially match very large sets of documents, these operations rely upo...", "content": "Read operations that return multiple documents do not immediately return all values matching the query. Because a query can potentially match very large sets of documents, these operations rely upon an object called a cursor. A cursor fetches documents in batches to reduce both memory consumption and network bandwidth usage.One category of issue you may observe occasionally in your application logs is a CursorNotFound entry such as the following (from an application using the MongoDB Java Driver): com.mongodb.MongoCursorNotFoundException: Query failed with error code -5 and error message 'Cursor 4865637895305205821 not found on server prod-shard-00-00.xxxxx.mongodb.net:27017' on server prod-shard-00-00.xxxxx.mongodb.net:27017Depending on whether the version of MongoDB your cluster is using is greater than 4.4.7 the “cursor id xxxxxx not found” can refer to two possible timeouts.(1) cursorTimeoutMillis being exceededThe cursorTimeoutMillis server parameter sets the expiration threshold (in milliseconds) for idle cursors before MongoDB removes them. The default value for cursorTimeoutMillis is 600000, or 10 minutes. Idle cursors are timed out using the ClientCursorMonitor background job, whose thread is identified in the mongod logs as clientcursormon.The ClientCursorMonitor identifies and reaps idle cursors every 4 seconds (the default value of clientCursorMonitorFrequencySecs).When a cursor timeout is identified these can be found in the log with entries similar to the following:{\"t\":{\"$date\":\"2021-12-29T09:22:41.937-05:00\"},\"s\":\"I\",\"c\":\"COMMAND\",\"id\":51803,\"ctx\":\"conn3\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"admin.foo\",\"appName\":\"MongoDB Shell\",\"command\":{\"find\":\"foo\",\"filter\":{},\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"824bd767-4a7d-4240-a8cd-8f4c83c8cf99\"}},\"$db\":\"admin\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":4225966264683133400,\"keysExamined\":0,\"docsExamined\":1,\"numYields\":0,\"nreturned\":1,\"reslen\":123,\"locks\":{\"ReplicationStateTransition\":{\"acquireCount\":{\"w\":1}},\"Global\":{\"acquireCount\":{\"r\":1}},\"Database\":{\"acquireCount\":{\"r\":1}},\"Collection\":{\"acquireCount\":{\"r\":1}},\"Mutex\":{\"acquireCount\":{\"r\":1}}},\"storage\":{},\"protocol\":\"op_msg\",\"durationMillis\":0}}{\"t\":{\"$date\":\"2021-12-29T09:22:44.096-05:00\"},\"s\":\"I\",\"c\":\"QUERY\",\"id\":20529,\"ctx\":\"clientcursormon\",\"msg\":\"Cursor timed out\",\"attr\":{\"cursorId\":4225966264683133400,\"idleSince\":{\"$date\":\"2021-12-29T14:22:41.937Z\"}}}{\"t\":{\"$date\":\"2021-12-29T09:22:48.031-05:00\"},\"s\":\"I\",\"c\":\"COMMAND\",\"id\":51803,\"ctx\":\"conn3\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"admin.$cmd\",\"appName\":\"MongoDB Shell\",\"command\":{\"getMore\":4225966264683133400,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"824bd767-4a7d-4240-a8cd-8f4c83c8cf99\"}},\"$db\":\"admin\"},\"cursorid\":4225966264683133400,\"numYields\":0,\"ok\":0,\"errMsg\":\"cursor id 4225966264683133185 not found\",\"errName\":\"CursorNotFound\",\"errCode\":43,\"reslen\":129,\"locks\":{},\"protocol\":\"op_msg\",\"durationMillis\":0}}The log messages above have had their log component verbosity increased and were generated using the following script:# bashrm -rf data &amp;&amp; mkdir data$(m bin 4.4.7-ent)/mongod --dbpath data --bind_ip_all// mongo shelldb.runCommand({ setParameter: 1, cursorTimeoutMillis: 1000 });db.runCommand({ setParameter: 1, clientCursorMonitorFrequencySecs: 2 });db.foo.drop();db.foo.insertMany([ {}, {} ]);db.setLogLevel(4, 'command')db.foo.find({}).batchSize(1).forEach(function(d) { printjson(d); sleep(1000 * 6);});db.setLogLevel(-1, 'command')Note that the log contains a …”s”:”I”,”c”:”QUERY”,”id”:20529,”ctx”:”clientcursormon”,”msg”:”Cursor timed out…“ entry, which is at the default log verbosity. If this message is present the cursor timed out as a result of being idle longer than cursorTimeoutMillis and would have returned an error such as:Error: command failed: {\t\"ok\" : 0,\t\"errmsg\" : \"cursor id 4225966264683133185 not found\",\t\"code\" : 43,\t\"codeName\" : \"CursorNotFound\"} with original command request: {\t\"getMore\" : NumberLong(\"4225966264683133185\"),\t\"collection\" : \"foo\",\t\"batchSize\" : 1,\t\"lsid\" : {\t\t\"id\" : UUID(\"824bd767-4a7d-4240-a8cd-8f4c83c8cf99\")\t}}(2) localLogicalSessionTimeoutMinutes being exceededStarting with MongoDB 3.6 server sessions, or logical sessions, are the underlying framework used by client sessions to support Causal Consistency and retryable writes. When using a MongoDB Driver that is 3.6+ compatible implicit sessions are used (per the Drivers Sessions specification). As such, if your application is using a 3.6+ compatible driver, you are using sessions.The default value of localLogicalSessionTimeoutMinutes is 30 minutes and controls the time (in minutes) that a session remains active after its most recent use. Sessions that have not received a new read/write operation from the client or been refreshed with refreshSessions within this threshold are cleared from the cache.Starting with MongoDB 4.4.8 (via SERVER-6036) when a cursor is opened as part of a session, its lifetime will be tied to that session and as a result closing or timing out of a session will kill all associated cursors. This results in cursorTimeoutMillis/clientcursormon not being used to control cursor timeouts for any cursor with a session id.When a cursor times out as a result of the session being reaped the error appears as follows:{\"t\":{\"$date\":\"2021-12-29T08:02:04.942-05:00\"},\"s\":\"I\",\"c\":\"COMMAND\",\"id\":51803,\"ctx\":\"conn1\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"admin.foo\",\"appName\":\"MongoDB Shell\",\"command\":{\"find\":\"foo\",\"filter\":{},\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"824bd767-4a7d-4240-a8cd-8f4c83c8cf99\"}},\"$db\":\"admin\"},\"planSummary\":\"COLLSCAN\",\"cursorid\":63169428846689080,\"keysExamined\":0,\"docsExamined\":1,\"numYields\":0,\"nreturned\":1,\"reslen\":123,\"locks\":{\"ReplicationStateTransition\":{\"acquireCount\":{\"w\":1}},\"Global\":{\"acquireCount\":{\"r\":1}},\"Database\":{\"acquireCount\":{\"r\":1}},\"Collection\":{\"acquireCount\":{\"r\":1}},\"Mutex\":{\"acquireCount\":{\"r\":1}}},\"storage\":{},\"protocol\":\"op_msg\",\"durationMillis\":0}}{\"t\":{\"$date\":\"2021-12-29T08:03:53.030-05:00\"},\"s\":\"I\",\"c\":\"QUERY\",\"id\":20528,\"ctx\":\"LogicalSessionCacheRefresh\",\"msg\":\"Killing cursor as part of killing session(s)\",\"attr\":{\"cursorId\":63169428846689080}}{\"t\":{\"$date\":\"2021-12-29T08:04:04.968-05:00\"},\"s\":\"I\",\"c\":\"COMMAND\",\"id\":51803,\"ctx\":\"conn1\",\"msg\":\"Slow query\",\"attr\":{\"type\":\"command\",\"ns\":\"admin.$cmd\",\"appName\":\"MongoDB Shell\",\"command\":{\"getMore\":63169428846689080,\"collection\":\"foo\",\"batchSize\":1,\"lsid\":{\"id\":{\"$uuid\":\"824bd767-4a7d-4240-a8cd-8f4c83c8cf99\"}},\"$db\":\"admin\"},\"cursorid\":63169428846689080,\"numYields\":0,\"ok\":0,\"errMsg\":\"cursor id 63169428846689082 not found\",\"errName\":\"CursorNotFound\",\"errCode\":43,\"reslen\":127,\"locks\":{},\"protocol\":\"op_msg\",\"durationMillis\":0}}These log entries were generated by adjusting our previous example as seen below:# bashrm -rf data &amp;&amp; mkdir data$(m bin 4.4.11-ent)/mongod --dbpath data --bind_ip_all --setParameter logicalSessionRefreshMillis=1000 --setParameter localLogicalSessionTimeoutMinutes=1db.foo.drop();db.foo.insertMany([ {}, {} ]);db.setLogLevel(4, 'command')db.foo.find({}).batchSize(1).forEach(function(d) { printjson(d); sleep(1000 * 120);});db.setLogLevel(-1, 'command')Now the log entry that controls timing out the cursor is generated by the LogicalSessionCacheRefresh thread. Note that as the end result is the same (idle cursor is timed out) the error returned to the application would appear to be the same as well:uncaught exception: Error: command failed: {\t\"ok\" : 0,\t\"errmsg\" : \"cursor id 63169428846689082 not found\",\t\"code\" : 43,\t\"codeName\" : \"CursorNotFound\"} with original command request: {\t\"getMore\" : NumberLong(\"63169428846689082\"),\t\"collection\" : \"foo\",\t\"batchSize\" : 1,\t\"lsid\" : {\t\t\"id\" : UUID(\"824bd767-4a7d-4240-a8cd-8f4c83c8cf99\")\t}}SummaryRegardless of which MongoDB 3.6+ version is being used an idle cursor can time out with the failure bubbling up to the application with an error message such as “cursor id 63169428846689082 not found”, which is the same as if the cursor were timed out.Note that setting the noCursorTimeout cursor option in a MongoDB 3.6+ cluster can still result in a cursor being closed as Session Idle Timeout Overrides noCursorTimeout.For operations that return a cursor, if the cursor may be idle for longer than localLogicalSessionTimeoutMinutes minutes, issue the operation within an explicit session using Mongo.startSession() and periodically refresh the session using the refreshSessions command. For example:var session = db.getMongo().startSession()var sessionId = session.getSessionId().idvar cursor = session.getDatabase(\"examples\").getCollection(\"data\").find().noCursorTimeout()var refreshTimestamp = new Date() // take note of time at operation startwhile (cursor.hasNext()) { // Check if more than 5 minutes have passed since the last refresh if ( (new Date()-refreshTimestamp)/1000 &gt; 300 ) { print(\"refreshing session\") db.adminCommand({\"refreshSessions\" : [sessionId]}) refreshTimestamp = new Date() } // process cursor normally}Refreshing explicit sessions is one way to work around these timeouts, however this is only one approach. If there is a need to keep cursors idle for long periods of time these should be evaluated on a case by case basis to ensure refactoring isn’t a better solution.All examples in this article use the mongo shell however the logic could be adapted to your preferred language and used with the appropriate MongoDB Driver.Have any questions or comments? Post them below ;)" }, { "title": "MongoDB Driver Specifications", "url": "/blog/2021/12/23/mongodb-driver-specifications/", "categories": "MongoDB", "tags": "mongodb, drivers", "date": "2021-12-23 10:08:19 -0500", "snippet": "To ensure official MongoDB Drivers are developed with consistent functionality and APIs MongoDB maintains a set of public specifications (see GitHub Repository) that driver engineers can reference ...", "content": "To ensure official MongoDB Drivers are developed with consistent functionality and APIs MongoDB maintains a set of public specifications (see GitHub Repository) that driver engineers can reference while implementing functionality. These specifications can (and should) be used by community engineers building a community library or driver that will communicate with a MongoDB cluster.I will be diving deeper into some of these specifications in upcoming posts but to begin I’d like to summarize the existing specifications below.Current SpecificationsAuthenticationMongoDB supports various authentication strategies across various versions. When authentication is turned on in the database, a driver must authenticate before it is allowed to communicate with the server. This spec defines when and how a driver performs authentication with a MongoDB server.Performance BenchmarkingThis document describes a standard benchmarking suite for MongoDB drivers.BSON CorpusThe official BSON specification does not include test data, so this pseudo-specification describes tests for BSON encoding and decoding. It also includes tests for MongoDB's \"Extended JSON\" specification (hereafter abbreviated as extjson).BSON Decimal128 Type HandlingMongoDB 3.4 introduces a new BSON type representing high precision decimal (\"\\x13\"), known as Decimal128. 3.4 compatible drivers must support this type by creating a Value Object for it, possibly with accessor functions for retrieving its value in data types supported by the respective languages.Round-tripping Decimal128 types between driver and server MUST not change its value or representation in any way. Conversion to and from native language types is complicated and there are many pitfalls to represent Decimal128 precisely in all languages.Change StreamsAs of version 3.6 of the MongoDB server a new $changeStream pipeline stage is supported in the aggregation framework. Specifying this stage first in an aggregation pipeline allows users to request that notifications are sent for all changes to a particular collection. This specification defines the means for creating change streams in drivers, as well as behavior during failure scenarios.Client Side EncryptionMongoDB 4.2 introduced support for client side encryption, guaranteeing that sensitive data can only be encrypted and decrypted with access to both MongoDB and a separate key management provider (supporting AWS, Azure, GCP, a local provider, and KMIP). Once enabled, data can be seamlessly encrypted and decrypted with minimal application code changes.CollationAs of MongoDB server version 3.4 (maxWireVersion 5), a collation option is supported by the query system for matching and sorting on language strings in a locale-aware fashion.Command MonitoringThe performance monitoring specification defines a set of behaviour in the drivers for providing runtime information about commands to any 3rd party APM library as well internal driver use, such as logging.Wire CompressionThis specification describes how to implement Wire Protocol Compression between MongoDB drivers and mongod/mongos.Compression is achieved through a new bi-directional wire protocol opcode, referred to as OP_COMPRESSED.Server side compressor support is checked during the initial MongoDB Handshake, and is compatible with all historical versions of MongoDB. If a client detects a compatible compressor it will use the compressor for all valid requests.Connection Monitoring and PoolingDrivers currently support a variety of options that allow users to configure connection pooling behavior. Users are confused by drivers supporting different subsets of these options. Additionally, drivers implement their connection pools differently, making it difficult to design cross-driver pool functionality. By unifying and codifying pooling options and behavior across all drivers, we will increase user comprehension and code base maintainability.Connection StringThe purpose of the Connection String is to provide a machine readable way of configuring a MongoClient, allowing users to configure and change the connection to their MongoDB system without requiring any application code changes.This specification defines how the connection string is constructed and parsed. The aim is not to list all of connection string options and their semantics. Rather it defines the syntax of the connection string, including rules for parsing, naming conventions for options, and standard data types.CRUD APIThe CRUD API defines a set of related methods and structures defining a driver's API. As not all languages/frameworks have the same abilities, parts of the spec may or may not apply. These sections have been called out.GridFSGridFS is a convention drivers use to store and retrieve BSON binary data (type \"\\x05\") that exceeds MongoDB’s BSON-document size limit of 16 MiB. When this data, called a user file, is written to the system, GridFS divides the file into chunks that are stored as distinct documents in a chunks collection. To retrieve a stored file, GridFS locates and returns all of its component chunks. Internally, GridFS creates a files collection document for each stored file. Files collection documents hold information about stored files, and they are stored in a files collection.This spec defines a basic API for GridFS. This spec also outlines advanced GridFS features that drivers can choose to support in their implementations. Additionally, this document attempts to clarify the meaning and purpose of all fields in the GridFS data model, disambiguate GridFS terminology, and document configuration options that were previously unspecified.Initial DNS Seedlist DiscoveryPresently, seeding a driver with an initial list of ReplicaSet or MongoS addresses is somewhat cumbersome, requiring a comma-delimited list of host names to attempt connections to. A standardized answer to this problem exists in the form of SRV records, which allow administrators to configure a single domain to return a list of host names. Supporting this feature would assist our users by decreasing maintenance load, primarily by removing the need to maintain seed lists at an application level.This specification builds on the Connection String specification. It adds a new protocol scheme and modifies how the Host Information is interpreted.Load Balancer SupportThis specification defines driver behaviour when connected to MongoDB services through a load balancer.Max StalenessRead preference gains a new option, \"maxStalenessSeconds\".A client (driver or mongos) MUST estimate the staleness of each secondary, based on lastWriteDate values provided in server hello responses, and select only those secondaries whose staleness is less than or equal to maxStalenessSeconds.Most of the implementation of the maxStalenessSeconds option is specified in the Server Discovery And Monitoring Spec and the Server Selection Spec. This document supplements those specs by collecting information specifically about maxStalenessSeconds.OP_MSGOP_MSG is a bi-directional wire protocol opcode introduced in MongoDB 3.6 with the goal of replacing most existing opcodes, merging their use into one extendable opcode.HandshakeMongoDB 3.4 has the ability to annotate connections with metadata provided by the connecting client. The intent of this metadata is to be able to identify client level information about the connection, such as application name, driver name and version. The provided information will be logged through the mongod/mongos.log and the profile logs; this should enable sysadmins to easily backtrack log entries the offending application. The active connection data will also be queryable through aggregation pipeline, to enable collecting and analyzing driver trends.After connecting to a MongoDB node a hello command (if Versioned API is requested) or a legacy hello command is issued, followed by authentication, if appropriate. This specification augments this handshake and defines certain arguments that clients provide as part of the handshake.OCSP SupportThis specification is about the ability for drivers to to support OCSP—Online Certificate Status Protocol (RFC 6960)—and two of its related extensions: OCSP stapling (RFC 6066) and Must-Staple (RFC 7633).Polling SRV Records for mongos DiscoveryCurrently the Initial DNS Seedlist Discovery functionality provides a static seedlist when a MongoClient is constructed. Periodically polling the DNS SRV records would allow for the mongos proxy list to be updated without having to change client configuration.This specification builds on top of the original Initial DNS Seedlist Discovery specification, and modifies the Server Discovery and Monitoring specification's definition of monitoring a set of mongos servers in a Sharded TopologyType.Read and Write ConcernA driver must support configuring and sending read concern and write concerns to a server. This specification defines the API drivers must implement as well as how that API is translated into messages for communication with the server.Retryable ReadsThis specification is about the ability for drivers to automatically retry any read operation that has not yet received any results—due to a transient network error, a \"not writable primary\" error after a replica set failover, etc.—exactly once.Retryable WritesMongoDB 3.6 will implement support for server sessions, which are shared resources within a cluster identified by a session ID. Drivers compatible with MongoDB 3.6 will also implement support for client sessions, which are always associated with a server session and will allow for certain commands to be executed within the context of a server session.Additionally, MongoDB 3.6 will utilize server sessions to allow some write commands to specify a transaction ID to enforce at-most-once semantics for the write operation(s) and allow for retrying the operation if the driver fails to obtain a write result (e.g. network error or \"not writable primary\" error after a replica set failover). This specification will outline how an API for retryable write operations will be implemented in drivers. The specification will define an option to enable retryable writes for an application and describe how a transaction ID will be provided to write commands executed therein.Server Discovery and MonitoringThis spec defines how a MongoDB client discovers and monitors one or more servers. It covers monitoring a single server, a set of mongoses, or a replica set. How does the client determine what type of servers they are? How does it keep this information up to date? How does the client find an entire replica set from a seed list, and how does it respond to a stepdown, election, reconfiguration, or network error?All drivers must answer these questions the same. Or, where platforms' limitations require differences among drivers, there must be as few answers as possible and each must be clearly explained in this spec. Even in cases where several answers seem equally good, drivers must agree on one way to do it.Server MonitoringThis spec defines how a driver monitors a MongoDB server. In summary, the client monitors each server in the topology. The scope of server monitoring is to provide the topology with updated ServerDescriptions based on hello or legacy hello command responses.Server SelectionMongoDB deployments may offer more than one server that can service an operation. This specification describes how MongoDB drivers and mongos shall select a server for either read or write operations. It includes the definition of a \"read preference\" document, configuration options, and algorithms for selecting a server for different deployment topologies.Driver SessionsVersion 3.6 of the server introduces the concept of logical sessions for clients. A session is an abstract concept that represents a set of sequential operations executed by an application that are related in some way. This specification is limited to how applications start and end sessions. Other specifications define various ways in which sessions are used (e.g. causally consistent reads, retryable writes, or transactions).This specification also discusses how drivers participate in distributing the cluster time throughout a deployment, a process known as \"gossipping the cluster time\". While gossipping the cluster time is somewhat orthogonal to sessions, any driver that implements sessions MUST also implement gossipping the cluster time, so it is included in this specification.Snapshot ReadsVersion 5.0 of the server introduces support for read concern level \"snapshot\" (non-speculative) for read commands outside of transactions, including on secondaries. This spec builds upon the Sessions Specification to define how an application requests \"snapshot\" level read concern and how a driver interacts with the server to implement snapshot reads.SOCKS5 SupportSOCKS5 is a standardized protocol for connecting to network services through a separate proxy server. It can be used for connecting to hosts that would otherwise be unreachable from the local network by connecting to a proxy server, which receives the intended target host’s address from the client and then connects to that address.Convenient API for TransactionsReliably committing a transaction in the face of errors can be a complicated endeavor using the MongoDB 4.0 drivers API. This specification introduces a withTransaction method on the ClientSession object that allows application logic to be executed within a transaction. This method is capable of retrying either the commit operation or entire transaction as needed (and when the error permits) to better ensure that the transaction can complete successfully.TransactionsVersion 4.0 of the server introduces multi-statement transactions. This spec builds upon the Driver Sessions Specification to define how an application uses transactions and how a driver interacts with the server to implement transactions.The API for transactions must be specified to ensure that all drivers and the mongo shell are consistent with each other, and to provide a natural interface for application developers and DBAs who use multi-statement transactions.Unified Test FormatThis project defines a unified schema for YAML and JSON specification tests, which run operations against a MongoDB deployment. By conforming various spec tests to a single schema, drivers can implement a single test runner to execute acceptance tests for multiple specifications, thereby reducing maintenance of existing specs and implementation time for new specifications.URI OptionsHistorically, URI options have been defined in individual specs, and drivers have defined any additional options independently of one another. Because of the frustration due to there not being a single place where all of the URI options are defined, this spec aims to do just that—namely, provide a canonical list of URI options that each driver defines.Versioned APIAs MongoDB moves toward more frequent releases (a.k.a. continuous delivery), we want to enable users to take advantage of our rapidly released features, without exposing applications to incompatible server changes due to automatic server upgrades. A versioned API will help accomplish that goal.Older SpecificationsThere are also some older specifications in the repository that are there for historic context. If you’re writing a new driver this information is still useful though and should be reviewed.Handling of DBRefsDBRefs are a convention for expressing a reference to another document as an embedded document (i.e. BSON type 0x03). Several drivers provide a model class for encoding and/or decoding DBRef documents. This specification will both define the structure of a DBRef and provide guidance for implementing model classes in drivers that choose to do so.Bulk APIDriver support for Bulk write operationsEnumerating CollectionsA driver can contain a feature to enumerate all collections belonging to a database. This specification defines how collections should be enumerated.Enumerating DatabasesA driver can provide functionality to enumerate all databases on a server. This specification defines several methods for enumerating databases.Enumerating IndexesA driver can contain a feature to enumerate all indexes belonging to a collection. This specification defines how indexes should be enumerated.Extended JSONMongoDB Extended JSON is a string format for representing BSON documents. This specification defines the canonical format for representing each BSON type in the Extended JSON format. Thus, a tool that implements Extended JSON will be able to parse the output of any tool that emits Canonical Extended JSON. It also defines a Relaxed Extended JSON format that improves readability at the expense of type information preservation.Find, getMore and killCursors commandsThe Find, GetMore and KillCursors commands in MongoDB 3.2 or later replace the use of the legacy OP_QUERY, OP_GET_MORE and OP_KILL_CURSORS wire protocol messages. This specification lays out how drivers should interact with the new commands when compared to the legacy wire protocol level messages.Index ManagementThe index management spec defines a set of behaviour in the drivers for creating, removing and viewing indexes in a collection. It defines implementation details when required but also provides flexibility in the driver in that one or both of 2 unique APIs can be chosen to be implemented.ObjectID FormatThis specification documents the format and data contents of ObjectID BSON values that the drivers and the server generate when no field values have been specified (e.g. creating an ObjectID BSON value when no _id field is present in a document). It is primarily aimed to provide an alternative to the historical use of the MD5 hashing algorithm for the machine information field of the ObjectID, which is problematic when providing a FIPS compliant implementation. It also documents existing best practices for the timestamp and counter fields.Write CommandsMethod to do writes (insert/update/delete) that declares the write concern up front and support for batch operationsHandling of Native UUID TypesThe Java, C#, and Python drivers natively support platform types for UUID, all of which by default encode them to and decode them from BSON binary subtype 3. However, each encode the bytes in a different order from the others. To improve interoperability, BSON binary subtype 4 was introduced and defined the byte order according to RFC 4122, and a mechanism to configure each driver to encode UUIDs this way was added to each driver. The legacy representation remained as the default for each driver.Server Wire version and Feature ListThe 'WireVersion' captures all \"protocol events\" the write protocol went through. A protocol event is a change in the syntax of messages on the wire or the semantics of existing messages. We may also add \"logical\" entries for releases, although that's not mandatory.We use the wire version to determine if two agents (a driver, a mongos, or a mongod) can interact. Each agent carries two versions, a 'max' and a 'min' one. If the two agents are on the same 'max' number, they strictly speak the same wire protocol and it is safe to allow them to communicate. If two agents' ranges do not intersect, they should not beallowed to communicate.If two agents have at least one version in common they can communicate, but one of the sides has to be ready to compensate for not being on its partner version.What’s NextThe MongoDB JIRA project for specifications (SPEC) is not publicly available, however the DRIVERS project is. As new features are proposed that will impact all drivers, tickets are added to the DRIVERS project which anyone can review.More often than not these DRIVERS tickets influence specification changes and are a good source of information as to what changes may be coming.Interested in any particular specification and want a deeper dive? Let me know in the comments." }, { "title": "Sanitarium (DreamForge Intertainment) - 1998", "url": "/blog/2021/12/22/sanitarium/", "categories": "Let's Adventure!", "tags": "adventure, DreamForge, ASC Games", "date": "2021-12-22 06:08:00 -0500", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.I took a little break from this series to work on other writing projects, but considering ScummV...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.I took a little break from this series to work on other writing projects, but considering ScummVM announced support for this title officially I figured it was time to finally get back in the saddle and run through some adventure games again :)Sanitarium is a psychological horror point-and-click adventure video game that was originally released for Microsoft Windows. It was developed by DreamForge Intertainment and published by ASC Games in 1998. I have a long history with this game so that’s likely going to influence the rating somewhat, but for anyone that hasn’t played this game before the story is what steals the show.After a car accident knocks him unconscious, a man awakens from a coma, his face fully bandaged, to find that he has been admitted to a dilapidated sanitarium and that he cannot remember who he is or where he came from, or how he came to be there, though his fellow inmates seem to know him simply as “Max”. As he delves deeper into the asylum’s corridors in search of answers, Max finds himself transported to various obscure and otherworldly locations: a small town inhabited only by malformed children and overseen by a malevolent alien entity known only as “Mother”, a demented circus surrounded by an endless ocean and terrorized by a squid-like individual, an alien hive overrun by cyborg insects, and an Aztec village devastated by the return of the god Quetzalcoatl. Between each episode, Max returns to the asylum grounds, blending real and unreal, each time closer to regaining his memory and unraveling the truth surrounding the mysterious Dr. Morgan, head of the asylum. He remembers the death of his younger sister Sarah years ago and the real reason behind his institutionalization.Sanitarium is a point and click adventure that is split into 9 chapters. Chapter progression involves walking around, talking to the characters to uncover more of the story and reading various background pieces such as books, notes, diaries and tombstones. Everything you read further uncovers bits of the story that have been obscured due to the main character’s mental state.As you move around you’ll find items that can be used to solve various puzzles. These are done in typical fetch-quest style whereby (typically) a character will provider you with a clue as to the location of an item that you then need to go find and bring back in order to progress or to unlock some piece of the chapter. The inventory items are accessed via a ring menu around the character, and all items (aside from one literal “red herring”) are required to complete the game. Items are not carried over from chapter to chapter so everything you find will need to be used within the chapter you found it.There are multiple puzzles scattered throughout the game. If you pay attention as you play the puzzles aren’t overly difficult as clues to how to solve them are typically found fairly close to each puzzle. Puzzles in the first couple chapters are very straightforward (reattach the cables to a VCR, play tic-tac-toe) however as you approach the end of the game these do get a bit more complex.Interacting with NPCs is done using a keyword menu. As words are selected the characters fill in more of the story and selected words change colours. Certain keywords aren’t available until after you interact with other characters, complete subquests or find certain items. As a result you have to do some backtracking on occasion to ensure you’re able to advance the story.Depending on the chapter you’re in the main character (Max) can take the form of a Cyclops, an Aztec god, or his sister Sarah. When you start a chapter as one of these characters you remain as this character for the duration of the chapter. The only variation is in chapter 8 where you can alternate between each character to help solve certain puzzles.Cinematic cutscenes persist throughout the game to help advance the story. As you slowly uncover who you are, why you were in the asylum to start with and who Dr. Morgan is these videos add additional depth and colour to the story. Once you reach the final chapter you solve a final puzzle to escape from the prison of your mind and awake in a hospital with your family waiting.It’s been a long time since I actually played through this game, and though I may have fond memories of it due to the time spent trying to reverse engineer the game logic I found I still thoroughly enjoyed playing it again. The dialog and voice acting is a little over the top, but the writing is excellent and there’s a fair amount of comedy sprinkled throughout.The darker setting and theme resonates with me and for anyone that enjoys a good, solid story I’d highly recommend giving this game a play.Game Information Game Sanitarium Developer DreamForge Intertainment Publisher ASC Games Release Date April 28, 1998 Systems Windows Game Engine   Play Information Time to Completion 7 hours Version Played Windows via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 18 Story (25) 22 Experience (15) 13 Impact (10) 7   86% Gallery" }, { "title": "Just Finished - Sword of Hope", "url": "/blog/2021/12/18/just-finished-sword-of-hope/", "categories": "Gaming, Just Finished", "tags": "gb, rpg", "date": "2021-12-18 12:30:23 -0500", "snippet": "The Sword of Hope was released for the Nintendo Game Boy in 1991 by Kemko/Seika.The story involves a dragon having gained control over the king of Riccar, manipulating him into removing the Sword o...", "content": "The Sword of Hope was released for the Nintendo Game Boy in 1991 by Kemko/Seika.The story involves a dragon having gained control over the king of Riccar, manipulating him into removing the Sword of Hope from a painting and releasing its curse. Summoning the dark power mammon the dragon turned the people of Riccar into trees. When Prince Theo is born, the king tries to murder him, but Theo is saved by the brave knight Pascal who flees with him into a forest where three magicians live who conceal the king’s castle underground to prevent the evil from spreading. After living in the forest for 15 years, Prince Theo is finally strong enough to try and stop the dragon, using the Sword of Hope one of the magicians had retrieved. (Description from Moby Games)Sword of Hope combines adventure and RPG elements. The interface allows the prince to select commands such as LOOK, OPEN, MAGIC, HIT, and USE to interact with the game world, talk to NPCs or solve puzzles. When he encounters an enemy, the game switches to a turn based battle mode in which enemies have to be fought in typical console style RPG battles.The first thing that jumped out at me was the screen transitions (and their sound effects) were pulled from Shadowgate, which would make sense as the port of that title to the NES was also published by Kemko/Seika.Progressing through the game is fairly straightforward: the game world is split into areas that are behind gates that require keys. You find the magician in the area you have access to, who tells you where you need to go to find the key to the next area, which you do by slogging through a bunch of random encounters until you find said key.The game teaches you a lot of spells, which are learned from scrolls or by levelling up. Your MP levels (which deplete as you cast spells) can be restored using herbs you can find along the way or by buying them from the shopkeepers. I found myself grinding a bit early on to get money together to buy extra herbs so I didn’t have to keep using the TELEPORT spell back to the shaman to heal up when I ran low on HP/MP.You can also upgrade your weapon and armour along the way. Shopkeepers sell armour upgrades and as you complete the quests for the magicians they’ll power up your sword. You’ll need the extra stats as you progress as the random encounters occur VERY frequently and a bad grouping of enemies when you’re under-levelled will result in you getting your ass kicked back to the king.Once you get near the endgame you’ll start fighting Druids, who cast strong attack magic or sap your HP/MP. You can’t run from these enemies, so you’ll have to stay and fight, but their stamina is so high you do very little damage … and they have 100HP. This is one of those scenarios where I’m glad I was playing on an emulator as you could fudge the outcome of the battles by saving, waiting, taking and action and if you didn’t like the result reload and try again (the RNG appears to be time based).Eventually you work your way through the castle into the mirror world and find the painting of the dragon. When you do you have to fight the king (tough fight) then immediately fight the dragon (even tougher fight). I was only at level 19 when I made a run for the endgame, and the walkthroughs recommend level 25 or higher … and for good reason. These last 2 enemies are incredibly hard, but if you’re not playing this on a console and have save states you can manipulate luck enough to even the odds in your favour.I loved my Game Boy when I was a kid, and remember playing this title back then. I hated the game because it was too hard and I didn’t have the patience to grind, but now (many decades later) I can appreciate the simple story, linear progression and level of difficulty. If you want to play this game as the developers intended you’ll find yourself grinding A LOT near the end to make surviving the random encounters easier.One of the main drawbacks for me was having to face multiple multi-enemy grouping random encounters per screen as you try to make your way through the mirror world. It just seemed unnecessary and it became tedious and boring. I knew I was near the end so wanted to push through, but I can honestly say I’ll never pick this one up again as I can’t imagine fighting another Druid again …" }, { "title": "Just Finished - Brain Lord", "url": "/blog/2021/10/27/just-finished-brain-lord/", "categories": "Gaming, Just Finished", "tags": "snes, jrpg", "date": "2021-10-27 14:46:38 -0400", "snippet": "Brain Lord is an action RPG from 1994 for the Super Nintendo published by Enix. The story follows Remeer (you), who’s father being the last of the dragon warriors was sent on a quest to find the la...", "content": "Brain Lord is an action RPG from 1994 for the Super Nintendo published by Enix. The story follows Remeer (you), who’s father being the last of the dragon warriors was sent on a quest to find the last of the dragons terrorizing the village. Your father never returned from this quest and the game opens with you starting out on your own journey to find your father.Remeer is joined by his four friends: Kashian (a bounty hunter), Barness (a spiritual guru), Rein (a warrior), and Ferris (a witch). These characters help advance the plot, which is pretty standard JRPG fare (dragons, towers, save the world) and provide hints and clues along the way.Fun fact, Remeer (or its equivalent ‘Lemele’) appears in two more video games developed by Produce! and published by Enix, The 7th Saga and Mystic Ark. Double fun fact, I’ve been trying to beat both of those for years now and may eventually write them up in a “Just Gave Up On” entry …The game is broken down into 2 cities (Arcs, Toronto) and 5 dungeons (The Tower of Light, Ancient Ruins, the Ice Castle, Droog Volcano, and Platinum).Cities contain NPCs that nudge you in the right direction to keep the story progressing, as well as provide shops for equipment and items. Both cities have an inn you can recover at and save your progress.Once you’re out of the city you’re either on your way to a dungeon, or in a dungeon. These areas are filled with monsters that you engage with in an Action-RPG direct combat fashion. You have a number of weapons you’ll collect over the course of the game; the ranged weapons weaker (boomerang, bow) and the melee weapons stronger (swords, axes, morning stars).Weapons, armor and items can be found in treasure chests in the dungeons and are stored in your inventory. You have 3 inventory pages, after which you’ll have to either sell or drop items to make room for more.Aside from moving about the various floors of the dungeons and fighting random enemies, you’ll also have to solve puzzles. These puzzles all seem to come in the form of “press these switches either in a certain order, or all at once with rocks or balls”.When you’re not fighting or pushing rocks, you’re probably crossing chasms by jumping. Jumping puzzles suck in RPGs. I don’t really have anything to add to this other than complaints … and a deep appreciation for having played this game on an emulator with save states …You’re going to fall …. a lot. When you do you’re returned to the start of the room and you lose one life unit. If you were trying to solve a puzzle, leaving the room and returning will reset everything so you can try again, but falling right before you were about to solve a time consuming (usually tedious) puzzle is a pain in the ass.You can collect heart containers to increase your health, but you don’t level up while fighting. You do gain money and occasionally monsters drop orbs. These orbs can be collected by your Jade’s to level them up and make them stronger.These Jades are either bought in town or found in dungeons and provide various skills such as lighting dark rooms, healing you or attacking enemies. You can only equip two at a time, but I found I generally just kept two attack Jades at all times (unless fast forwarding the emulator with a healing Jade).The dungeons are made up of multiple floors with many, many locked doors. You collect keys throughout to unlock these and progress higher (or lower).Navigating the dungeons can be difficult, but you do get an item early on that can be used to show a minimap.Honestly I’d have been pretty lost without this as you forget where you are pretty quickly as you go, especially in the later dungeons. This would have been a great thing to be able to bind to one of the unused controller keys so you don’t have to open the inventory and select it each time, but seeing as this is an earlier RPG I’ll cut it some slack.Once you reach the end there is a boss that needs killing, which generally isn’t too difficult. These bosses are much larger and generally are multi phase battles.You make your way through the dungeons, kill the bosses, kill the Goblin king and you get a very basic ending screen for all your efforts.Overall the game is enjoyable and the puzzles can be interesting at first. Unfortunately there aren’t too many ways to spice up the “push rock, depress button” recipe so after a while this starts to get repetitive.The later dungeons include a lot of back tracking, and the dark room puzzles where you can’t use your light Jade and have to just “trial and error” your path through an invisible maze is frustrating.I did enjoy the journey and found the game to be pretty good for a lesser known SNES JRPG. I guess I shouldn’t be surprised that the precursor to 7th Saga would be frustrating to play :P" }, { "title": "Ensuring a MongoDB Replica Set Member's Priority Takeover Succeeds", "url": "/blog/2021/10/21/ensuring-replica-set-priority-takeover-success/", "categories": "MongoDB", "tags": "mongodb, replication", "date": "2021-10-21 07:11:18 -0400", "snippet": "High availability implies a system has been designed for durability, redundancy, and automatic failover such that the applications supported by the system can operate continuously and without downt...", "content": "High availability implies a system has been designed for durability, redundancy, and automatic failover such that the applications supported by the system can operate continuously and without downtime for a long period of time. MongoDB replica sets support high availability when deployed according to the documented best practices.The priority settings of replica set members affect both the timing and the outcome of elections for primary. Higher-priority members are more likely to call elections, and are more likely to win. The MongoDB documentation outlines a procedure to Force a Member to be Primary by Setting its Priority High, which can be easily demonstrated by changing the replica set’s priorities through a replSetReconfig command.In this article we will demonstrate how to utilize replica set member priority to ensure a given node assumes the primary role under ideal circumstances, as well as under load (when there is consistent replication lag).Initial Setup# setup a PSS replica setmlaunch init --replicaset --nodes 3 --binarypath $(m bin 4.2.17-ent)# using the mongo shell reconfigure the set# with the 3rd node having a priority of 10mongo \"mongodb://localhost:27017/?replicaset=replset\" --quiet \\ --eval \"c = rs.conf(); c.members[2].priority = 10; rs.reconfig(c)\"To setup the replica set the m version manager is used along with mtools. Once the replica set is reconfigured via the mongo shell, checking the logs for the rs3 node that should now be PRIMARY should show the results of the priority takeover:tail -n 1000 data/replset/rs3/mongod.log | grep -E \"(ELECTION|REPL)\"2021-10-06T06:50:38.730-0400 I REPL [replexec-2] New replica set config in use: { _id: \"replset\", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: \"localhost:27017\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: \"localhost:27018\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: \"localhost:27019\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 10.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('615d7f403cd84ab3e708fdea') } }2021-10-06T06:50:38.730-0400 I REPL [replexec-2] This node is localhost:27019 in the config2021-10-06T06:50:38.737-0400 I ELECTION [replexec-3] Scheduling priority takeover at 2021-10-06T06:50:48.909-04002021-10-06T06:50:48.911-0400 I REPL [replexec-4] Canceling priority takeover callback2021-10-06T06:50:48.911-0400 I ELECTION [replexec-4] Starting an election for a priority takeover2021-10-06T06:50:48.911-0400 I ELECTION [replexec-4] conducting a dry run election to see if we could be elected. current term: 12021-10-06T06:50:48.912-0400 I REPL [replexec-4] Scheduling remote command request for vote request: RemoteCommand 183 -- target:localhost:27017 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: true, term: 1, candidateIndex: 2, configVersion: 2, lastCommittedOp: { ts: Timestamp(1633517438, 1), t: 1 } }2021-10-06T06:50:48.912-0400 I REPL [replexec-4] Scheduling remote command request for vote request: RemoteCommand 184 -- target:localhost:27018 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: true, term: 1, candidateIndex: 2, configVersion: 2, lastCommittedOp: { ts: Timestamp(1633517438, 1), t: 1 } }2021-10-06T06:50:48.912-0400 I ELECTION [replexec-9] VoteRequester(term 1 dry run) received a yes vote from localhost:27017; response message: { term: 1, voteGranted: true, reason: \"\", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1633517438, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1633517438, 1) }2021-10-06T06:50:48.912-0400 I ELECTION [replexec-5] dry election run succeeded, running for election in term 22021-10-06T06:50:48.941-0400 I REPL [replexec-5] Scheduling remote command request for vote request: RemoteCommand 185 -- target:localhost:27017 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: false, term: 2, candidateIndex: 2, configVersion: 2, lastCommittedOp: { ts: Timestamp(1633517438, 1), t: 1 } }2021-10-06T06:50:48.942-0400 I REPL [replexec-5] Scheduling remote command request for vote request: RemoteCommand 186 -- target:localhost:27018 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: false, term: 2, candidateIndex: 2, configVersion: 2, lastCommittedOp: { ts: Timestamp(1633517438, 1), t: 1 } }2021-10-06T06:50:48.957-0400 I ELECTION [replexec-1] VoteRequester(term 2) received a yes vote from localhost:27018; response message: { term: 2, voteGranted: true, reason: \"\", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1633517438, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1633517438, 1) }2021-10-06T06:50:48.960-0400 I ELECTION [replexec-8] election succeeded, assuming primary role in term 22021-10-06T06:50:48.961-0400 I REPL [replexec-8] transition to PRIMARY from SECONDARYThe above example will work every time on a local cluster with no traffic and no lag. What happens when we start applying some traffic to this cluster using a process that supports Retryable Writes?Retryable WorkloadRetryable writes allow MongoDB drivers to automatically retry certain write operations a single time if they encounter network errors, or if they cannot find a healthy primary in the replica sets or sharded cluster. Based on this guarantee our assumption would be that the write workload should continue uninterrupted:curl -s https://gist.githubusercontent.com/alexbevi/955c6675337107e16d637233f865b1e3/raw/cca0390f6c30898140cc55490930b80c5cad527b/template.json | \\ mgeneratejs -n 5000000 | \\ mongoimport --uri \"mongodb://localhost:27017/test?replicaSet=replset&amp;w=majority&amp;retryWrites=true\" --collection data --numInsertionWorkers 1 --dropThe mongoimport command, along with mgeneratejs are used to generate data and write the results to the test.data namespace. Note the version of mongoimport used is &gt; 100.2.1 as this ensures retryable writes are supported (per TOOLS-2745).Assuming the retryable writes guarantee is accurate, performing another replSetReconfig to shift the highest priority to another node should allow the election to occur without interrupting the mongoimport workload.Using tmux to manage multiple terminals (as seen in the screenshot above), the mongoimport (left panel) continues to import data while the election is triggered and completes (per the logs of the new PRIMARY in the right panel).Retryable Workload with an Oplog DelayIn the above example the mongoimport workload was able to successfully continue with retryable writes enabled. Retryable writes are not a requirement in this scenario however (per the Retryable Writes Specification) having this feature enabled will ensure server errors such as NotWritablePrimary, NotPrimaryNoSecondaryOk, NotPrimaryOrSecondary, PrimarySteppedDown will be retried.Next we want to run the same test but with an artificial lag introduced to the node that is expected to step up as primary due to priority.Simulating an Oplog DelayReplication lag is a delay between an operation on the primary and the application of that operation from the oplog to the secondary (see “Check Replication Lag”). Our test above was able to easily complete a priority takeover and election as the nodes are all on the same host (localhost) and there should be virtually no delay between writes and replicated operations.The following script uses the fsync and fsyncUnlock commands to block a target node from performing writes. By doing this in a timed fashion we can simulate replication lag on a secondary as the node cannot apply writes from the oplog while it is locked.// file: secondaryDelay.js//// the host:port of the node to connect to and lock/unlockconst NODE = \"localhost:27019\";// how long to block operations (in milliseconds)const DELAY = 3500;function delaySecondary(c, t) { c.getDB(\"test\").fsyncLock(); sleep(t); c.getDB(\"test\").fsyncUnlock(); // print secondary replication info to show lag print(db.printSecondaryReplicationInfo());}var c = new Mongo(NODE);while(true) { delaySecondary(c, DELAY); sleep(100);}When the secondaryDelay.js script is run and the replica set reconfiguration is performed, the priority takeover should now fail.To simulate the failure, the following commands were running in tmux windows: mongoimport mongo --quiet secondaryDelay.js mongo \"mongodb://localhost:27017/?replicaset=replset\" --quiet --eval \"c = rs.conf(); c.members[0].priority = 1; c.members[2].priority = 10; rs.reconfig(c)\" tail -n 1000 data/replset/rs3/mongod.log | grep -E \"(ELECTION|REPL)\"2021-10-21T06:53:56.388-0400 I ELECTION [replexec-12] Scheduling priority takeover at 2021-10-21T06:54:07.210-04002021-10-21T06:54:07.210-0400 I REPL [replexec-10] Canceling priority takeover callback2021-10-21T06:54:07.210-0400 I ELECTION [replexec-10] Starting an election for a priority takeover2021-10-21T06:54:07.210-0400 I ELECTION [replexec-10] conducting a dry run election to see if we could be elected. current term: 72021-10-21T06:54:07.210-0400 I REPL [replexec-10] Scheduling remote command request for vote request: RemoteCommand 1129 -- target:localhost:27017 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: true, term: 7, candidateIndex: 2, configVersion: 9, lastCommittedOp: { ts: Timestamp(1634813644, 1000), t: 7 } }2021-10-21T06:54:07.211-0400 I REPL [replexec-10] Scheduling remote command request for vote request: RemoteCommand 1130 -- target:localhost:27018 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: true, term: 7, candidateIndex: 2, configVersion: 9, lastCommittedOp: { ts: Timestamp(1634813644, 1000), t: 7 } }2021-10-21T06:54:07.211-0400 I ELECTION [replexec-9] VoteRequester(term 7 dry run) received a no vote from localhost:27017 with reason \"candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1634813644, 1000), t: 7 }, my last applied OpTime: { ts: Timestamp(1634813645, 1000), t: 7 }\"; response message: { term: 7, voteGranted: false, reason: \"candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1634813644, 1000), t: 7 }, my last applied OpTime: { ts: Timest...\", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1634813645, 1000), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1634813645, 1000) }2021-10-21T06:54:07.211-0400 I ELECTION [replexec-2] VoteRequester(term 7 dry run) received a no vote from localhost:27018 with reason \"candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1634813644, 1000), t: 7 }, my last applied OpTime: { ts: Timestamp(1634813645, 1000), t: 7 }\"; response message: { term: 7, voteGranted: false, reason: \"candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1634813644, 1000), t: 7 }, my last applied OpTime: { ts: Timest...\", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1634813645, 1000), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1634813645, 1000) }2021-10-21T06:54:07.211-0400 I ELECTION [replexec-2] not running for primary, we received insufficient votes2021-10-21T06:54:07.211-0400 I ELECTION [replexec-2] Lost dry run electionThe simulation is successful if we can verify the priority takeover could not complete due to “candidate’s data is staler than mine” failures. This implies the node trying to run for election requested votes from other nodes, which voted “no” as the candidate node needs to catch up. Note that if the oplog of the candidate node is more than 2 seconds behind, the priority takeover will not be scheduled and a failure such as the following will be logged:2021-10-21T06:55:18.490-0400 I ELECTION [replexec-13] Not starting an election for a priority takeover, since we are not electable due to: Not standing for election because member is not caught up enough to the most up-to-date member to call for priority takeover - must be within 2 seconds (mask 0x80)Note that this threshold is defined by the priorityTakeoverFreshnessWindowSeconds server parameter which controls how caught up in replication a secondary with higher priority than the current primary must be before it will call for a priority takeover election.Ensuring Priority Takeover when LaggingAssuming a 1-2 second delay is consistent, the priority takeover will continually be rescheduled but the election will always fail.This particular scenario can be overcome using a combination of replSetFreeze and replSetStepDown as follows.// file: freezeAndStepdown.js//function stepDown() { var host = db.hello().primary; var client = new Mongo(host) printjson(client.adminCommand({ replSetStepDown: 30, secondaryCatchUpPeriodSecs: 20 }));}function freezeNode(host) { var client = new Mongo(host) print(\"Freezing \" + host) client.adminCommand({ replSetFreeze: 30 })}freezeNode(\"localhost:27017\");freezeNode(\"localhost:27018\");stepDown();All other replica set members aside from the one we’ve given the highest priority (localhost:27019 in our scenario) are first frozen to prevent them from seeking election for 30 seconds. The primary is then stepped down with a secondaryCatchUpPeriodSecs of 20 seconds set to allow eligible secondaries to catch up to the primary. As all other nodes are frozen and won’t seek election, the only remaining node which was consistently lagging will catch up and stand for election.Our previous reproduction using tmux is updated as follows to produce this result: tail -n 1000 data/replset/rs3/mongod.log | grep -E \"(ELECTION|REPL)\" mongoimport mongo --quiet secondaryDelay.js mongo \"mongodb://localhost:27017/?replicaset=replset\" --quiet --eval \"c = rs.conf(); c.members[0].priority = 1; c.members[2].priority = 10; rs.reconfig(c)\" mongo --quiet freezeAndStepdown.jsThe end result should be the priority takeover attempt resulting in a successful dry run election and subsequent election.2021-10-21T10:30:04.892-0400 I ELECTION [replexec-15] not running for primary, we received insufficient votes2021-10-21T10:30:04.907-0400 I ELECTION [replexec-15] Lost dry run election2021-10-21T10:30:05.772-0400 I ELECTION [replexec-20] Scheduling priority takeover at 2021-10-21T10:30:16.346-04002021-10-21T10:30:09.777-0400 I REPL [replexec-14] Member localhost:27017 is now in state SECONDARY2021-10-21T10:30:16.346-0400 I REPL [replexec-24] Canceling priority takeover callback2021-10-21T10:30:16.346-0400 I ELECTION [replexec-24] Starting an election for a priority takeover2021-10-21T10:30:16.346-0400 I ELECTION [replexec-24] conducting a dry run election to see if we could be elected. current term: 142021-10-21T10:30:16.346-0400 I REPL [replexec-24] Scheduling remote command request for vote request: RemoteCommand 6065 -- target:localhost:27017 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: true, term: 14, candidateIndex: 2, configVersion: 16, lastCommittedOp: { ts: Timestamp(1634826609, 1000), t: 14 } }2021-10-21T10:30:16.346-0400 I REPL [replexec-24] Scheduling remote command request for vote request: RemoteCommand 6066 -- target:localhost:27018 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: true, term: 14, candidateIndex: 2, configVersion: 16, lastCommittedOp: { ts: Timestamp(1634826609, 1000), t: 14 } }2021-10-21T10:30:16.347-0400 I ELECTION [replexec-23] VoteRequester(term 14 dry run) received a yes vote from localhost:27017; response message: { term: 14, voteGranted: true, reason: \"\", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1634826609, 1000), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1634826609, 1000) }2021-10-21T10:30:16.347-0400 I ELECTION [replexec-23] dry election run succeeded, running for election in term 152021-10-21T10:30:18.621-0400 I REPL [replexec-22] Scheduling remote command request for vote request: RemoteCommand 6069 -- target:localhost:27017 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: false, term: 15, candidateIndex: 2, configVersion: 16, lastCommittedOp: { ts: Timestamp(1634826609, 1000), t: 14 } }2021-10-21T10:30:18.621-0400 I REPL [replexec-22] Scheduling remote command request for vote request: RemoteCommand 6070 -- target:localhost:27018 db:admin cmd:{ replSetRequestVotes: 1, setName: \"replset\", dryRun: false, term: 15, candidateIndex: 2, configVersion: 16, lastCommittedOp: { ts: Timestamp(1634826609, 1000), t: 14 } }2021-10-21T10:30:18.635-0400 I ELECTION [replexec-15] VoteRequester(term 15) received a yes vote from localhost:27017; response message: { term: 15, voteGranted: true, reason: \"\", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1634826609, 1000), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1634826609, 1000) }2021-10-21T10:30:18.638-0400 I ELECTION [replexec-23] election succeeded, assuming primary role in term 152021-10-21T10:30:18.638-0400 I REPL [replexec-23] transition to PRIMARY from SECONDARYFrom the screenshots we can see that during all election attempts the mongoimport workload continued to operate without issue.Did this article help you? Let me know in the comments below ;)" }, { "title": "MongoDB Stable API", "url": "/blog/2021/10/07/mongodb-versioned-api/", "categories": "MongoDB", "tags": "mongodb", "date": "2021-10-07 06:17:50 -0400", "snippet": " MongoDB’s Versioned API was renamed to the Stable API, so this guide was updated to use the new terminologyThe Stable API For Drivers Specification states “When applications interact with MongoDB...", "content": " MongoDB’s Versioned API was renamed to the Stable API, so this guide was updated to use the new terminologyThe Stable API For Drivers Specification states “When applications interact with MongoDB, both the driver and the server participate in executing operations. Therefore, when determining application compatibility with MongoDB, both the driver and the server behavior must be taken into account.”As MongoDB moves toward more frequent releases (a.k.a. continuous delivery), they want to enable users to take advantage of rapidly released features, without exposing applications to incompatible server changes due to automatic server upgrades. A Stable API will help accomplish that goal (see “Upgrade Fearlessly with the MongoDB Stable API”).The Stable API encompasses the subset of MongoDB commands that applications commonly use to read and write data, create collections and indexes, and so on. We commit to keeping these commands backward-compatible in new MongoDB versions. We can add new features (such as new command parameters, new aggregation operators, new commands, etc.) to the Stable API, but only in backward-compatible ways.Identifying Stable API CommandsFrom a mongo or mongosh shell connected to a 5.0+ cluster the following helper function can be used to determine which commands are included in a specific API version.function printCommandsByAPIVersion(version) { version = version.toString(); var result = db.runCommand({ listCommands: 1 }) var keys = Object.keys(result.commands); var commands = []; for (var i = 0; i &lt; keys.length; i++) { var cmd = result.commands[keys[i]]; if (cmd.apiVersions.indexOf(version) &gt;= 0 ) { commands.push(keys[i]); } } return { version: db.serverBuildInfo().version, apiVersion: version, commands: commands.sort() };}This can be used to print out the commands supported by a specific API Version (only version \"1\" for now):printCommandsByAPIVersion(1);API v1 CommandsNote this is also documented now, however the output of the printCommandsByAPIVersion against a MongoDB 5.0 cluster would output the following:abortTransaction, aggregate, authenticate, collMod, commitTransaction, create, createIndexes, delete, drop, dropDatabase, dropIndexes, endSessions, explain, find, findAndModify, getMore, hello, insert, killCursors, listCollections, listDatabases, listIndexes, ping, refreshSessions, saslContinue, saslStart, updateStable API TestThe following test using the latest master branch of the Ruby Driver can be used to test the behavior of an application with and without strict checking. This example sends a replSetGetStatus command to the server, and if the strict option is set shows the APIStrictError being raised:# test.rbrequire 'bundler/inline'gemfile do source 'https://rubygems.org' gem 'mongo', github: 'mongodb/mongo-ruby-driver', branch: \"master\"endstrict_api = ARGV[0] == \"true\"# connect to a MongoDB 5.0+ replica setclient = Mongo::Client.new([ 'alexs-mbp:27017' ], database: 'mydb', replica_set: 'rs0', server_api: { version: 1, strict: strict_api })begin admin_client = client.use('admin') p admin_client.database.command(replSetGetStatus: 1).documents.firstrescue Mongo::Error::OperationFailure =&gt; ex puts exensure client.closeendThe above script can be tested by passing true/false as the first argument to validate the behavior of the driver in strict mode:$ ruby test.rb{\"set\"=&gt;\"rs0\", \"date\"=&gt;2021-05-14 14:49:33 UTC, \"myState\"=&gt;1, \"term\"=&gt;7, \"syncSourceHost\"=&gt;\"\", \"syncSourceId\"=&gt;-1, \"heartbeatIntervalMillis\"=&gt;2000, \"majorityVoteCount\"=&gt;1, \"writeMajorityCount\"=&gt;1, \"votingMembersCount\"=&gt;1, \"writableVotingMembersCount\"=&gt;1, \"optimes\"=&gt;{\"lastCommittedOpTime\"=&gt;{\"ts\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1dc78 @seconds=1621003767, @increment=1&gt;, \"t\"=&gt;7}, \"lastCommittedWallTime\"=&gt;2021-05-14 14:49:27 UTC, \"readConcernMajorityOpTime\"=&gt;{\"ts\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1d9d0 @seconds=1621003767, @increment=1&gt;, \"t\"=&gt;7}, \"appliedOpTime\"=&gt;{\"ts\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1d868 @seconds=1621003767, @increment=1&gt;, \"t\"=&gt;7}, \"durableOpTime\"=&gt;{\"ts\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1d700 @seconds=1621003767, @increment=1&gt;, \"t\"=&gt;7}, \"lastAppliedWallTime\"=&gt;2021-05-14 14:49:27 UTC, \"lastDurableWallTime\"=&gt;2021-05-14 14:49:27 UTC}, \"lastStableRecoveryTimestamp\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1d340 @seconds=1621003737, @increment=1&gt;, \"electionCandidateMetrics\"=&gt;{\"lastElectionReason\"=&gt;\"electionTimeout\", \"lastElectionDate\"=&gt;2021-05-14 14:27:57 UTC, \"electionTerm\"=&gt;7, \"lastCommittedOpTimeAtElection\"=&gt;{\"ts\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1cfa8 @seconds=0, @increment=0&gt;, \"t\"=&gt;-1}, \"lastSeenOpTimeAtElection\"=&gt;{\"ts\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1ce40 @seconds=1620999663, @increment=1&gt;, \"t\"=&gt;6}, \"numVotesNeeded\"=&gt;1, \"priorityAtElection\"=&gt;1.0, \"electionTimeoutMillis\"=&gt;10000, \"newTermStartDate\"=&gt;2021-05-14 14:27:57 UTC, \"wMajorityWriteAvailabilityDate\"=&gt;2021-05-14 14:27:57 UTC}, \"members\"=&gt;[{\"_id\"=&gt;0, \"name\"=&gt;\"Alexs-MBP:27017\", \"health\"=&gt;1.0, \"state\"=&gt;1, \"stateStr\"=&gt;\"PRIMARY\", \"uptime\"=&gt;1300, \"optime\"=&gt;{\"ts\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1c6c0 @seconds=1621003767, @increment=1&gt;, \"t\"=&gt;7}, \"optimeDate\"=&gt;2021-05-14 14:49:27 UTC, \"syncSourceHost\"=&gt;\"\", \"syncSourceId\"=&gt;-1, \"infoMessage\"=&gt;\"\", \"electionTime\"=&gt;#&lt;BSON::Timestamp:0x00007fb303a1c378 @seconds=1621002477, @increment=1&gt;, \"electionDate\"=&gt;2021-05-14 14:27:57 UTC, \"configVersion\"=&gt;1, \"configTerm\"=&gt;7, \"self\"=&gt;true, \"lastHeartbeatMessage\"=&gt;\"\"}], \"ok\"=&gt;1.0, \"$clusterTime\"=&gt;{\"clusterTime\"=&gt;#&lt;BSON::Timestamp:0x00007fb3049dff30 @seconds=1621003767, @increment=1&gt;, \"signature\"=&gt;{\"hash\"=&gt;&lt;BSON::Binary:0x70203426668280 type=generic data=0x0000000000000000...&gt;, \"keyId\"=&gt;0}}, \"operationTime\"=&gt;#&lt;BSON::Timestamp:0x00007fb3049dfd00 @seconds=1621003767, @increment=1&gt;}$ ruby test.rb true[323:APIStrictError]: Provided apiStrict:true, but the command replSetGetStatus is not in API Version 1 (on alexs-mbp:27017)There are presently no plans or schedules to deprecate version 1, which is reinforced by the following excerpt from the internal technical design document: The Versioned API frees us from this bind. We say that today’s semantics are part of the MongoDB API Version 1, and the new semantics are in Version 2. MongoDB servers will support both.Additional References: Reference Documentation Architecture Documentation DRIVERS-996: Versioned MongoDB API for Drivers" }, { "title": "Copy MongoDB Index Definitions", "url": "/blog/2021/09/24/copy-mongodb-index-definitions/", "categories": "MongoDB", "tags": "mongodb, indexes, queries", "date": "2021-09-24 06:24:14 -0400", "snippet": "To support your application’s workload properly, you’ll want to ensure you’re creating indexes to support your queries. When doing this in a development environment, unless the the Driver or ODM in...", "content": "To support your application’s workload properly, you’ll want to ensure you’re creating indexes to support your queries. When doing this in a development environment, unless the the Driver or ODM in use allows you to manage index definitions via annotations in code (and you use that feature) it’s possible your development cluster’s indexes can diverge from that in production.The following script will allow you to quickly generate all index creation commands for your cluster in a way that can be copy/pasted to another cluster.function generateCreateIndexesCommands(options) { var getIndexName = function(keys) { var name = \"\"; var keyz = Object.keys(keys); for (var k = 0; k &lt; keyz.length; k++) { var v = keys[keyz[k]]; if (typeof v == \"function\") continue; if (name.length &gt; 0) name += \"_\"; name += keyz[k] + \"_\"; name += v; } return name.substring(0, 126); }; if (options === undefined) { options = {} } var truncateIndexName = options[\"truncateIndexName\"] || true; var ensureBackground = options[\"ensureBackground\"] || false; db.getMongo().getDBNames().filter(x =&gt; ![\"admin\", \"config\", \"local\"].includes(x)).forEach(function (d) { db.getSiblingDB(d).getCollectionInfos({type : \"collection\"}).forEach(function (c) { var keys = db.getSiblingDB(d).getCollection(c.name).getIndexes(); var idPosition = -1; for (var i = 0; i &lt; keys.length; i++) { if (keys[i].name == \"_id_\") { idPosition = i; } else { keys[i].name = (truncateIndexName) ? getIndexName(keys[i].key) : keys[i].key if (ensureBackground) { // force all indexes to be created in the background keys[i].background = true; } } } // remove the { _id: 1 } default index as it will exist already anyway keys.splice(idPosition, 1); if (keys.length &gt; 0) { print(\"db.getSiblingDB('\" + d + \"').\" + c.name + \".createIndexes(\" + JSON.stringify(keys) + \")\"); } }); })}The generateCreateIndexesCommands function will output all createIndexes commands for the cluster including all options. The function can optionally be configured using the following parameters (in an object): truncateIndexName: Limits an index definitions name field to less than 128 characters. Prior to MongoDB 4.2 (see SERVER-32959) this Index Name Length limitation existed and under certain circumstances, compound index auto naming could result in this limit being exceeded. (default: true) ensureBackground: Prior to MongoDB 4.2 (which introduced the optimized build process) indexes could be built in either the foreground or the background. Foreground index builds were fast and produced more efficient index data structures, but required blocking all read-write access to the parent database of the collection being indexed for the duration of the build. Background index builds were slower and had less efficient results, but allowed read-write access to the database and its collections during the build process.This option ensure index definitions contain a { background: true } option in case you want to copy the commands to a MongoDB 4.0 cluster (or earlier) and ensure index builds occur in the background. (default: false)// run with no optionsgenerateCreateIndexesCommands()// run with optionsgenerateCreateIndexesCommands({ truncateIndexName: true, ensureBackground: true });Running this script will produce output such as the following:db.getSiblingDB('test').foo.createIndexes([{\"v\":2,\"key\":{\"a\":1,\"b\":1},\"name\":\"a_1_b_1\",\"background\":true},{\"v\":2,\"key\":{\"key\":1},\"name\":\"key_1\",\"collation\":{\"locale\":\"en\",\"caseLevel\":false,\"caseFirst\":\"off\",\"strength\":2,\"numericOrdering\":false,\"alternate\":\"non-ignorable\",\"maxVariable\":\"punct\",\"normalization\":false,\"backwards\":false,\"version\":\"57.1\"}}])db.getSiblingDB('test').restaurants.createIndexes([{\"v\":2,\"key\":{\"cuisine\":1,\"name\":1},\"name\":\"cuisine_1_name_1\",\"partialFilterExpression\":{\"rating\":{\"$gt\":5}}}])Did this script help you? Do you have any other options you’d like to see added? Let me know in the comments below … and happy coding ;)" }, { "title": "Labyrinth: The Computer Game (Lucasfilm Games) - 1986", "url": "/blog/2021/08/30/labyrinth/", "categories": "Let's Adventure!", "tags": "adventure, LucasArts, Activision", "date": "2021-08-30 08:12:50 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Labyrinth is a 1986 graphic adventure game based on the fantasy film Labyrinth, which tasks you ...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Labyrinth is a 1986 graphic adventure game based on the fantasy film Labyrinth, which tasks you with navigating a maze while solving puzzles and evading dangers. Your goal is to find and defeat the main antagonist, Jareth, within 13 real-time hours.Though not directly related to the game itself, I just wanted to call out the bygone era of splash screens and crack intros that used to come with pirated software in the 80s and 90s.The disk image I found for this review had a “Cracked” variation, which on booting up in VICE treated you to the following:Piracy is still an issue (as it was then), but there is a certain lost art form in these early crack screens that the “youngsters and their torrents” are missing out on.For some examples, see “The art of the crack” or Jason Scott’s “APPLE II CRACK SCREENS”.Labyrinth was the first adventure game created by Lucasfilm which, unlike other adventure games of the period, did not feature a command-line interface. Instead, the player uses two scrolling “word wheel” menus on the screen to construct basic sentences. This design clearly inspired the interface used in their next, which you’ve likely heard of (Maniac Mansion), and which we’ll cover in a later review.The game opens with you inputting your name, gender and favourite colour. The word wheels are presented on the bottom of the screen and a text interface is shown at the top, and this is your first interaction with the game world … which sucks. I’m not really sure why they decided to do this, but you immediately feel like you’re playing a much older interactive fiction game. Don’t get me wrong, IF games are great but not if you expected a graphical adventure.To be fair, this sequence does get you familiarized with the word wheel interface, and also the comedic writing style which will continue throughout the game. As you muddle through you will pick up some items, interact with a couple characters and learn about your inventory. It takes a bit, but eventually you will find your way into the theatre and the movie finally begins.I didn’t really know what to expect from this game as I had done very little research and also hadn’t see the movie it was based on in over 30 years, but holy cow was the quality of the interface and graphics surprising.Your character can see exits, objects and NPCs on a “minimap” above the word wheel. The background and characters are very well drawn and occasionally animated. For example, the Stone Faces will follow your character back and forth with their eyes depending on where you move.As you move around the labyrinth, you’ll find various items to pick up that need to be used to solve a puzzle or to evade an enemy. Occasionally these puzzles can be intuitive, but I found I had to refer back to the walkthrough for most of my play through. Unlike later LucasArts games, I found Labyrinth to actually be really difficult. This may be due to it being “the first” game they did like this, but also you can miss things and have to backtrack, or make a bigger mistake and actually die and have to load a previous save game.Later LucasArts games removed dead ends and death, but not here. Even with a walkthrough this game was challenging, as there were Goblins that could trap you and send you into a pit you’d have to try to escape from. Whenever this happened, more often than not I had NO idea how to escape, and even reading the solution didn’t always make sense. For example (from the walkthrough): CALL + NERD. When he/she starts babbling at you, move to the back of thecell and keep moving back and forth until the Nerd “drives you up the wall,”enabling you to climb out! (May not always work.)Tell me you would have thought of that yourself …There is intro music when the game starts and sound effects throughout the labyrinth. As you have to avoid the Goblins it’s a nice touch that you can see them approaching on the minimap as well as hear their footsteps as they approach.Not all screens are side scrolling as you can move in 4 directions. On some screens you are solving puzzles that involve moving through doors that warp you to another area of the same screen while you try and figure out the correct sequence of doors (ex: The Wise Man’s Garden or The Goblin Village).The “final battle” in The Upside-Down Room with Jareth is really interesting as it flips the walk surfaces repeatedly. The time limit in this room also drops down to 12 minutes, which creates a sense of urgency as you try and THROW a CRYSTAL BALL at Jareth to win the game.I’m not sure if having watched the movie more recently would have made this game any easier to complete, but I found it to be extremely challenging. Some of the puzzles would have taken many (MANY) more hours of trial and error to complete if I was trying to grind through this without any assistance, however the feedback from the game when you make a mistake keeps you engaged and enjoying the experience.For LucasArts’s first graphical adventure game they nailed the interface, and set the tone for subsequent games like Maniac Mansion, Monkey Island, Sam and Max and others that share the same light sense of humour and verb-selection interface.If you’re a fan of the movie I’d be curious to hear if the game progression is easier as certain puzzles/interactions become more obvious as a result. My guess is “no”, but I’d be curious to hear otherwise ;)Game Information Game Labyrinth: The Computer Game Developer Lucasfilm Games Publisher Activision Release Date 1986 Systems Apple IIe and IIc, Commodore 64/128, MSX2 Game Engine   Play Information Time to Completion 3 hour Version Played Commodore 64 via VICE for Windows Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 12 Story (25) 15 Experience (15) 11 Impact (10) 6   63% Gallery" }, { "title": "Transylvania (Penguin Software) - 1982", "url": "/blog/2021/08/24/transylvania/", "categories": "Let's Adventure!", "tags": "adventure, Penguin Software", "date": "2021-08-24 09:20:18 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Transylvania is an adventure game from 1982 that follows the player on a quest to rescue Princes...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Transylvania is an adventure game from 1982 that follows the player on a quest to rescue Princess Sabrina. Far away a clock strikes 12…The game comes with a time limit in the form of periodic updates indicating an hour has gone by. The game starts at midnight and if you aren’t finished by 5AM, the princess dies and the game is over.To interact with the world a basic VERB NOUN text parser is used, which is common for games of this era. Navigation can be done using shortcuts such as N for GO NORTH or U for GO UP, but overall the interface is pretty intuitive. Generally THE EXACT verb/noun combo is needed, and these games are pretty unforgiving if you get it wrong.As you play through the game you’ll encounter a Werewolf who if you don’t immediately leave the screen will kill you.You will meet the Werewolf A LOT.I’m sure you can kill the Werewolf, and according to the walkthrough and Youtube video I watched it’s possible, but only in some versions of the game. The version I played didn’t allow you to GO NORTH at the house to find the cellar, get the crowbar and pry open the coffin. As a result I couldn’t get a bullet for the pistol to SHOOT WOLF.Oh well, no biggie. Though this game has a fair amount of fetch-quest style puzzles, there’s actually very little you are required to do to complete it.For example, you can basically just go get the cross, kill the vampire, get the ring, get the cloak, meet the aliens, get the black box, get the key from the goblin, get the elixir, go get Sabrina … and that’s it.Your inventory space is limited, and once it fills you can’t pick anything else up without dropping an item. Dropping items leaves a visual representation on the screen, so it’s easy to find where you left things.The puzzles are also intuitive and the clues the game gives you are generally sufficient to progress. I may have missed a clue somewhere but after you get the princess I wouldn’t have thought to go back to the frog and SAIL SHIP to close out the story. Thanks again walkthrough!Overall this game was an enjoyable adventure, though the story was pretty sparse. I doubt I’d ever replay this, but I had fun while it lasted :PGame Information Game Transylvania Developer Penguin Software Publisher Penguin Software Release Date 1982 Systems Apple II, Atari 8-bit, Commodore 64, FM-7, Macintosh, PC-88, PC-98 Game Engine   Play Information Time to Completion 1.5 hours Version Played Apple II via AppleWin Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 6 Story (25) 8 Experience (15) 5 Impact (10) 4   33% Gallery" }, { "title": "Retryable Writes, findAndModify and the impact on the MongoDB Oplog", "url": "/blog/2021/08/23/the-impact-of-retryable-writes-on-the-mongodb-oplog/", "categories": "MongoDB", "tags": "mongodb, replication", "date": "2021-08-23 15:18:09 -0400", "snippet": "When you’re monitoring your cluster using Ops Manager or MongoDB Atlas and the Replication Oplog Window is (X) drastically drops, what can you do?Having a very short operations log (oplog) window i...", "content": "When you’re monitoring your cluster using Ops Manager or MongoDB Atlas and the Replication Oplog Window is (X) drastically drops, what can you do?Having a very short operations log (oplog) window increases the likelihood that a SECONDARY member can fall off the oplog and require a full resync, however if the window remains small the resync may fail as the oplog window must be larger than the time needed to resync.One scenario that can result in the oplog window shrinking is the use of findAndModify operations when retryable writes are enabled, but how do we identify that they are the culprit?OverviewMongoDB introduced Retryable Writes in 3.6 as a way to allow Drivers (that have implemented the Retryable Write specification) a mechanism to retry certain write operations a single time if they encounter network errors, or if they cannot find a healthy primary in the replica sets or sharded cluster.When the findAndModify command was made retryable (via SERVER-30407), the implementation involved writing an additional no-op to the oplog that would contain a pre (or post) image of an update (or delete) operation.Prior to MongoDB 4.4, the oplog size was configured in megabytes. As it is a capped collection, once the configured size was filled the oldest entries were removed as new entries were written.With MongoDB 4.4 an option was added to configure a minimum oplog retention period (in hours), however this is not presently the default behavior when configuring a replica set.In this article we’ll be exploring the impact of retryable findAndModify operations on a local MongoDB 5.0.2 replica set running a Ruby script (see Appendix).Retryable Writes enabledWith the series of MongoDB Drivers that support MongoDB 4.2 Retryable Writes are enabled by default, however setting the retryWrites URI option can still be used to explicitly toggle this feature.When the script is run for the first time it will update the document configured during setup (see Appendix) and increment a value 300 times.Before the script runs the first event time (in the oplog) is 2021-08-23 11:31:38. After the script runs, the first event time is 2021-08-23 11:32:39. As our oplog is configured at 990MB this would imply MORE than 990MB were written as the oldest events appear to start after the script began.As a result, the oplog has fully churned during the course of this script running.The sample document we’re incrementing a value on is 5MB, and if we’re running 300 updates approximately 1.5GB of uncompressed would have been written to the oplog.Retryable Writes disabledFor this test run the application was modified to disable retryWrites from the connection string:# ...client = Mongo::Client.new(\"mongodb://localhost:27017/?retryWrites=false\")# ...As retryable writes are disabled there is no pre/post-image data being written to the oplog. This can be seen in the first event times being the same before and after the script runs (2021-08-23 11:32:39).NOTE: For Atlas clusters where setParameter is an Unsupported Command, disabling retryable writes or refactoring the findAndModify to instead perform a find followed by an update would be the best paths forward.Server Parameters and Retryable WritesAs a result of the impact on the oplog when using findAndModify, SERVER-56372 was created to allow the pre/post-image storage to be moved to a non-oplog collection.This functionality is available in MongoDB 5.0+, and was backported to MongoDB 4.4.7 and MongoDB 4.2.16.To enable this functionality an additional Server Parameter must be enabled at startup using the setParameter command as follows:mongod &lt;.. other options ..&gt; \\ --setParameter storeFindAndModifyImagesInSideCollection=true An additional setParameter of featureFlagRetryableFindAndModify=true was required to test this feature prior to MongodB 4.4.7/4.2.16This parameter would need to be set on each mongod node in the cluster. By doing this the pre/post-images will instead be saved to the config.image_collection namespace. storeFindAndModifyImagesInSideCollection is enabled by default starting in MongoDB 5.1When the script is run now with retryable writes enabled, the impact on the oplog should be as negligible as it was when retryWrites=false was set previously.For those that are curious, the config.image_collection namespace contains documents such as the following:(Due to the size of the data field it’s been projected out)ConclusionIf your applications are using findAndModify heavily and your cluster’s oplog is churning, there are a number of options to consider: Disable retryable writes (retryWrites=false in the connection string) Split the findAndModify operations into find and update operations Configure featureFlagRetryableFindAndModify and storeFindAndModifyImagesInSideCollection (assuming your version of MongoDB is 4.2.15+, 4.4.7+ or 5.0+)Let me know if you found this article useful in the comments below.Happy Coding!AppendixThe test script and steps to configure a local cluster are below if you want to validate the findings in this article yourself.SetupWe’ll be using m and mtools to setup the cluster, along with a Javascript script to automate configuration.# download and install MongoDB 5.0.2 enterprisem 5.0.2-ent# initialize a 3 node replicaset using MongoDB 5.0.2mlaunch init --replicaset --nodes 3 --binarypath $(m bin 5.0.2-ent)# wait about 30 seconds before running the scriptmongo setup.jsThe contents of the setup.js script above are:function generate_random_data(size){ var chars = 'abcdefghijklmnopqrstuvwxyz'.split(''); var len = chars.length; var random_data = []; while (size--) { random_data.push(chars[Math.random()*len | 0]); } return random_data.join('');}function setup() { // setup oplog with the minimum size of 990MB db.adminCommand({ replSetResizeOplog: 1, size: 990 }) db.foo.drop(); // setup 1 document with 5MB of junk data db.foo.insertOne({ _id: 1, pos: 1, data: generate_random_data(5 * 1024 * 1024) })}setup();The test.foo namespace is being setup with a single document that contains 5MB of junk data and a pos field we’ll be incrementing via the script below.Test ScriptWe’ll be using a standalone Ruby script that uses the latest MongoDB Ruby Driver (version 2.15 at time of writing).At a high level, the script does the following: Connect to the cluster Print replication info Using findAndModify increment (via $inc) a value 300 times Print replication info againThis script (see below) can be executed as follows:ruby test.rb# !/usr/bin/env ruby## filename: test.rb#require \"bundler/inline\"gemfile do source \"https://rubygems.org\" gem \"mongo\" gem \"progress_bar\"enddef get_seconds(ts) (ts.seconds &amp;&amp; ts.increment) ? ts.seconds : ts / 4294967296 # low 32 bits are ordinal #s within a secondenddef get_replication_info(client) db = client.use(:local).database olStats = db.command({ collStats: \"oplog.rs\" }).documents.first logSizeMB = olStats[\"maxSize\"] / (1024 * 1024) usedMB = olStats[\"size\"] / (1024 * 1024) coll = db[\"oplog.rs\"] first = coll.find.sort(\"$natural\": 1).first last = coll.find.sort(\"$natural\": -1).first tfirst = get_seconds(first[\"ts\"]) tlast = get_seconds(last[\"ts\"]) timeDiff = tlast - tfirst timeDiffHours = ((timeDiff / 36) / 100).round puts \"configured oplog size: #{logSizeMB}MB\\n\" puts \"log length start to end: #{timeDiff}secs (#{timeDiffHours}hrs)\\n\" puts \"oplog first event time: #{Time.at(tfirst)}\\n\" puts \"oplog last event time: #{Time.at(tlast)}\\n\" puts \"now: #{Time.now}\\n\"enddef update_docs_with_junk(coll) coll.find_one_and_update({ _id: 1 }, { \"$inc\": { pos: 1 } })enddef print_field_value(coll) v = coll.find(_id: 1).first[\"pos\"] puts \"value of pos: #{v}\\n\"endclient = Mongo::Client.new(\"mongodb://localhost:27017/?retryWrites=true\")puts get_replication_info(client)coll = client.use(:test).database[:foo]print_field_value(coll)max = 300pb = ProgressBar.new(max)max.times do pb.increment! update_docs_with_junk(coll)endprint_field_value(coll)puts get_replication_info(client)" }, { "title": "Shadowgate (ICOM Simulations) - 1987", "url": "/blog/2021/08/16/shadowgate/", "categories": "Let's Adventure!", "tags": "adventure, ICOM Simulations, MacVenture, Mindscape, Kemco", "date": "2021-08-16 07:33:48 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Shadowgate is a black-and-white 1987 point-and-click adventure video game originally for the App...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Shadowgate is a black-and-white 1987 point-and-click adventure video game originally for the Apple Macintosh in the MacVenture series. The game is named for its setting, Castle Shadowgate, residence of the evil Warlock Lord. The player, as the “last of a great line of hero-kings” is charged with the task of saving the world by defeating the Warlock Lord, who is attempting to summon up the demon Behemoth out of Hell.Since part of the fun of this series is trying to get these games to run on the original systems, I setup Mini vMac and found a StuffIt archive of Shadowgate. Since StuffIt is a proprietary format you have to then find a copy of StuffIt Expander that would work on the MacOS version in use (System 7.5.5 in this case), figure out how to get it into the emulated environment, install it, then expand the archive.As much fun as it was getting the MacOS environment up and running, since Shadowgate was also released for the NES I decided to play through that version instead.The NES release of Shadowgate is the only one to have music. I still remember the Entryway music as being my favourite track, and was not disappointed hearing this looping in the background during this play through.The game is split into three “views”. There is a game view that shows the entirety of the current room, and inventory view and an action view. These views are interacted with using a cursor. The action view contains verbs that when selected allow you to either apply an action to the game view or inventory view. For example, you can OPEN a door you see in the game view, or MOVE to a marked exit.Every new room of the castle you investigate offers new puzzles, most having a “wrong choice” that will result in you dying and having to reload a previous save. As a result, “Save Early, Save Often” as it’s not always obvious what a wrong choice may look like. For example, opening the wrong crypt will spill corrosive ooze that won’t kill you unless you try to exit through the wrong door.As you move through the rooms the torches you have slowly fade. If you forget to light new torches the game ends and you die, but additional torches can be found along the way.Game play is very much item collection and combination based. The very first screen of the game shows a door with a little skull above it, which can be OPENed and hides a key. This key is needed to unlock the door in the second room of the game …. and so on. You also learn a handful of spells, most of which are mostly used to solve a single puzzle.The game is very linear but the story progresses intuitively. If you pay attention to the clues the puzzles aren’t overly difficult, but I did have to refer to a walkthrough a couple times mostly out of laziness. I find these types of menu-driven games to be enjoyable as they feel like upgraded versions of the interactive fiction text adventures (like Zork) I started off with as a kid.Assuming you found all the right items, were able to find the ferryman and cross over to the warlocks lair and solve the puzzle to open the door you’ll finally find the warlock you’ve been sent to beat. You have to combine multiple items to create the staff that can be used to shoot the behemoth, causing it to return to the abyss while dragging the warlock down with him.Overall I enjoyed the game. I’ve played through this title many times over the years as you can burn through it in about an hour while still enjoying navigating familiar puzzles. A remake was released by Zojoi in 2014, but I’ve never actually tried this version (though the release trailer look pretty sweet).Though Shadowgate isn’t the first in the MacVentures series, it’s definitely my favourite. If you’ve never tried a menu-driven adventure game and are wondering where to start, look no further ;)Game Information Game Shadowgate Developer ICOM Simulations Publisher Mindscape, Kemco Release Date July 30, 1987 Systems Apple IIGS, Atari ST, Amiga, CD-i, Game Boy Color, Apple Macintosh,Nintendo Entertainment System, Palm OS, DOS,Pocket PC (ARM, MIPS), Mobile Phone Game Engine MacVenture Play Information Time to Completion 1 hour Version Played NES via NES.emu for Android Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 18 Story (25) 17 Experience (15) 13 Impact (10) 8   80% Gallery" }, { "title": "Princess Tomato in the Salad Kingdom (Hudson Soft) - 1984", "url": "/blog/2021/08/15/princess-tomato-in-the-salad-kingdom/", "categories": "Let's Adventure!", "tags": "adventure, Hudson Soft", "date": "2021-08-15 19:53:32 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Princess Tomato in the Salad Kingdom (サラダの国のトマト姫, Sarada no Kuni no Tomato Hime) is an adventure...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Princess Tomato in the Salad Kingdom (サラダの国のトマト姫, Sarada no Kuni no Tomato Hime) is an adventure game by Hudson Soft originally released in 1984 on a number of Japanese Home Computer systems. It was eventually ported to the Famicom in 1988, then localized and released in North America in 1991 on the NES.Apparently the original was a text parser game. Though I don’t intend to play any non-English titles for this series, I had to sink some time into getting a copy of this running on a home computer emulator just to try it out. After 30 minutes of frustration, I managed to get this going using a Hitachi S1 emulator just to take a screenshot ;)I remember renting this game when I was a kid from a local video store (ask your parents kids!) around the same time I had discovered Shadowgate. The interface was similar but “dumbed down” a bit to make it completely menu driven without the need for a cursor.The world is interacted with using a fixed number of verbs that surround the screen. The final verb PERCY only becomes available when you rescue the thirsty persimmon on the second screen of the game. Percy then becomes your sidekick and can provide tips, insight into certain situation or comic relief.With all characters being vegetables, there are plenty of opportunities for puns to be slipped into the dialog and story. The story is well paced and interesting enough to keep you rolling through the various fetch-quests that make up the game play. Princess Tomato has been kidnapped by Minister Pumpkin and is forcing her to marry him so he can reign over the Salad kingdom. You play as Sir Cucumber who has been tasked with rescuing the princess.The bad guys in this game are humans known as “Farmies”, who have aligned with Minister Pumpkin and are acting as his enforcers. Throughout the later part of the game you’ll run into Farmies and other enemies and have to thrown down in a hard core Rock-Paper-Scissors themed battle.The fights themselves are pretty straightforward and not overly difficult, but there is a certain novelty to it that keeps it interesting.Though only available in the NES version, there is an excellent soundtrack to back the game up. My favourite track is Minister Pumpkin’s Palace, which I’m pretty sure I tried to record to a “best of” casette back in the day when I first got into emulation.I mentioned the fetch quests that make up the majority of the game, but the remaining gameplay consists of maze navigation.In my younger years mazes were more fun and busting out the graph paper to track your progress held a certain appeal, but now I find these to generally just be frustrating. Thankfully, walkthroughs are easy enough to find and can get you through these sections quickly enough if you so choose.Overall I still enjoy playing through this game. I like revisiting the story and I like menu-driven adventure games such as this one and the MacVenture games. I know this game may not appeal to everyone, but if you’re looking for something a little different in an adventure game, give Princess Tomato a try.Game Information Game Princess Tomato in the Salad Kingdom Developer Hudson Soft Publisher Hudson Soft Release Date 1984 Systems NEC PC-8801, NEC PC-6001, FM-7, MSX, NES/Famicom Game Engine   Play Information Time to Completion 2 hours 40 minutes Version Played NES via NES.emu for Android Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 16 Story (25) 20 Experience (15) 12 Impact (10) 8   80% Gallery" }, { "title": "Space Quest: The Sarien Encounter (Sierra On-Line) - 1986", "url": "/blog/2021/08/09/space-quest-the-sarien-encounter/", "categories": "Let's Adventure!", "tags": "adventure, AGI, Sierra On-Line", "date": "2021-08-09 08:29:08 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Though I’m pretty sure I played King’s Quest first, Space Quest is the game that really stands o...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Though I’m pretty sure I played King’s Quest first, Space Quest is the game that really stands out to me as being a “great game of that era”. What set this title apart was the use of humour throughout the story, interactions with characters and the environment and the (countless) pitfalls and death traps. Series designers Scott Murphy and Mark Crowe (known as the “Two Guys from Andromeda” in later games) wanted to do something not set in a medieval or fantasy atmosphere, and also with the goal of creating a “fun, silly game”.You play as a bumbling janitor aboard the starship Arcada. Later games would identify the lead character as “Roger Wilco”, but here you enter your name and take on the role of the character instead. While sleeping peacefully in your broom closet you awaken to find the ship has been taken over by aliens who have killed all of your crewmates and taken the Star Generator. You must find your way off the Arcada, work your way through the desert of Kerona and find the Deltaur to destroy the Star Generator.As with prior AGI titles, you interact with the game via a text parser and move about with the arrow keys. The game’s main theme song is extremely memorable (even the original on PC Speaker), and there are background noises and interaction sounds throughout the game.Essentially every single choice you make in this game can result in your untimely demise, so “save early, save often”. I found myself rotating through 5-6 save slots and saving on every screen. The messages you get when you die are really funny though, so I’d encourage you to purposely make mistakes just to see how they describe your death … then reload :PThe story is very straightforward and the puzzles aren’t overly difficult. When you’re in Ulence Flats and need to buy a ship and a droid to get off the planet, you find the slot machine in the bar and can gamble until you get enough money. This part is a bit tedious (and the wrong jackpot WILL kill you), but overall progressing through the game wasn’t too challenging.The journey to Ulence Flats involves a sand skimmer mini game, where you’re just avoiding rocks. If you hit 5-6 rocks you’ll be ejected and die and have to reload and try again, but I found if you drop the speed down to Slow it wasn’t too difficult. Lowering the speed also helps once you’re on board the Deltaur and get a gun and have to shoot first (like Han?) once you lose your helmet.I’d highly recommend playing through this game. If text parser games aren’t your cup of tea, Sierra remade this title using the SCI engine in 1991. This remake is a point-and-click interface and updates the graphics from EGA to VGA and features digital sound and music. Though I said I wouldn’t cover the VGA remakes for this series, I’m guessing I might end up making an exception here …Game Information Game Space Quest: The Sarien Encounter Developer Sierra On-Line Publisher Sierra On-Line Release Date October 1986 Systems DOS, Macintosh, Apple II, Apple IIGS, Amiga, Atari ST Game Engine Adventure Game Interpreter (AGI) Play Information Time to Completion 45 minutes Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 17 Story (25) 21 Experience (15) 13 Impact (10) 8   84% Gallery" }, { "title": "Adventure Games (1980-1999): Sorted by Score", "url": "/blog/2021/08/08/adventure-games-1980-1999-sorted-by-score/", "categories": "Let's Adventure!", "tags": "series, adventure", "date": "2021-08-08 18:47:28 -0400", "snippet": "This page is a list of the reviewed games from the “Let’s Adventure!” series, sorted by rating. See the “Let’s Adventure!” category page for a list of content by date of publication.{ progress: { ...", "content": "This page is a list of the reviewed games from the “Let’s Adventure!” series, sorted by rating. See the “Let’s Adventure!” category page for a list of content by date of publication.{ progress: { finished: 42, skipped: 11, total: 363 }, complete: \"14.6%\"} 89% Full Throttle (LucasArts) - 1995 87% Grim Fandango (LucasArts) - 1998 86% Beneath a Steel Sky (Revolution Software) - 1994 86% Sanitarium (DreamForge Intertainment) - 1998 84% Sam &amp; Max Hit the Road (LucasArts) - 1993 84% Space Quest: The Sarien Encounter (Sierra On-Line) - 1986 81% King’s Quest: Quest for the Crown (Sierra On-Line) - 1984 80% Princess Tomato in the Salad Kingdom (Hudson Soft) - 1984 80% Shadowgate (ICOM Simulations) - 1987 77% Blue Force (Tsunami Games) - 1993 76% Rise of the Dragon (Dynamix) - 1990 73% I Have No Mouth, and I Must Scream (The Dreamers Guild) - 1995 71% Police Quest II: The Vengeance (Sierra On-Line) - 1988 69% Manhunter: New York (Evryware) - 1988 67% Police Quest: In Pursuit of the Death Angel (Sierra On-Line) - 1987 66% Mystery House (On-Line Systems) - 1980 66% Nightshade (Beam Software) - 1992 66% Space Quest II: Vohaul’s Revenge (Sierra On-Line) - 1987 64% Ringworld: Revenge of the Patriarch (Tsunami Games) - 1992 64% Leisure Suit Larry Goes Looking for Love (in Several Wrong Places) (Sierra On-Line) - 1988 63% Labyrinth: The Computer Game (Lucasfilm Games) - 1986 61% Countdown (Access Software) - 1990 57% Leisure Suit Larry in the Land of the Lounge Lizards (Sierra On-Line) - 1987 57% Mean Streets (Access Software) - 1989 51% Beavis and Butt-Head in Virtual Stupidity (Viacom New Media) - 1995 50% Déjà Vu: A Nightmare Comes True (ICOM Simulations) - 1985 49% Duckman: The Graphic Adventures of a Private Dick (The Illusions Gaming Company) - 1997 49% Urban Runner (Coktel Vision) - 1996 47% Hopkins FBI (MP Entertainment) - 1998 46% Ulysses and the Golden Fleece (On-Line Systems) - 1981 43% Eternam (Infogrames) - 1992 41% Hugo’s House of Horrors (Gray Design Associates) - 1990 41% King’s Quest II: Romancing the Throne (Sierra On-Line) - 1985 40% The Crimson Crown - Further Adventures in Transylvania (Penguin Software) - 1985 39% Cruise for a Corpse (Delphine Software International) - 1991 34% Murder on the Mississippi (Activision) - 1986 33% Transylvania (Penguin Software) - 1982 29% Wizard and the Princess (On-Line Systems) - 1980 27% Kabul Spy (Sirius Software) - 1982 19% Mission Asteroid (On-Line Systems) - 1980 13% Emmanuelle (Coktel Vision) - 1989 11% Geisha (Coktel Vision) - 1990" }, { "title": "Ulysses and the Golden Fleece (On-Line Systems) - 1981", "url": "/blog/2021/08/08/ulysses-and-the-golden-fleece/", "categories": "Let's Adventure!", "tags": "adventure, ADL, Sierra On-Line", "date": "2021-08-08 18:46:54 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Ulysses and the Golden Fleece (also known as Hi-Res Adventure #4) is a graphic adventure game re...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Ulysses and the Golden Fleece (also known as Hi-Res Adventure #4) is a graphic adventure game released in 1981 for the Apple II. The premise of the game is pretty straightforward: You’re in ancient Greece, the king gives you (Ulysses) the task of finding the Golden Fleece.Though there is no title screen per-say and no music, the graphics for this game are quite a bit more elaborate than some of the earlier Hi-Res Adventure games. I actually started playing the DOS version of this game first, but having to press Scroll Lock every time there was text to scroll became so tedious I had to quit and move onto another system instead.The backgrounds are well drawn, and the scenes draw you in, however as you navigate the game it feels like you’re just running through a maze. Most of the time the descriptions you’re getting for the screen you’re on is just telling you that you’re lost in a forrest, lost in the jungle, or lost at sea. I’m assuming playing through this game without a walkthrough would heavily benefit from some graph paper and a lot of saving/reloading.As with all On-Line System’s (and Sierra) games, the wrong move will result in death and an inevitable reload from a point further back than you’d like. Meticulous saving is the answer here, but I found myself resorting to the walkthrough to just plow through to the end.There are plenty of puzzles, a lot of which are not really intuitive and can be very difficult to solve. Some puzzles are multi-phase in that you give a command, then the text parser will ask you a follow up question you need to respond to for the command to be fulfilled. For example, when trying to get past the sirens in the ocean, you have to TIE SELF to the mast of the ship (so you won’t go crazy from the sirens’ song). When you enter the command, the game responds with “to what”, and the correct response is TO MAST, but before actually doing this, the game asks you if you want to take this action, to which you have to reply YES.The puzzles in general aren’t intuitive, and would require A LOT of trial and error. Getting across the fjord on the Island of Storms by doing a TIE LEATER … TO LEATHER and then THROW LEATHER to make a lasso out of the leather is super confusing … especially when you have a rope!!! Full disclosure, I was using the walkthrough at this point and didn’t try the rope to see if it would work …I’m sure if I took the time to properly explore this game I’d have learned the magic words (SEVENSEAS to get rid of the harpies, or SUPPELTUEL to open a cave door) but my patience for this title had already run out. For anyone that played this game in the early 80’s and was able to complete it on their own, my hat’s off to you. This title is just too full of monotonous maze navigation and overly complicated puzzles to be an enjoyable adventure.That’s my 2 cents on this. The visuals are nice for 1981, but I wouldn’t recommend this title unless you’re trying to complete a bunch of similar titles for your own blog/video series ;)Game Information Game Ulysses and the Golden Fleece Developer On-Line Systems Publisher On-Line Systems Release Date 1981 Systems Apple II, Atari 8-bit, Commodore 64, DOS Game Engine ADL (Adventure Development Language) Play Information Time to Completion &lt; 1 hour Version Played Apple II via AppleWin Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 12 Story (25) 12 Experience (15) 6 Impact (10) 4   46% Gallery" }, { "title": "King’s Quest: Quest for the Crown (Sierra On-Line) - 1984", "url": "/blog/2021/08/06/kings-quest/", "categories": "Let's Adventure!", "tags": "adventure, AGI, Sierra On-Line", "date": "2021-08-06 05:54:31 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.For a lot of us 80’s kids, King’s Quest is where it all began. The colours were vibrant, there w...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.For a lot of us 80’s kids, King’s Quest is where it all began. The colours were vibrant, there was intro music and sound effects throughout the game, the story felt immersive, and oh shit, you died, time to reload …. assuming you remembered to save.Unlike Mystery House where you’re sorta just dropped into the world and have to figure out why you’re there and muddle your way through, King’s Quest provides the player with a summary as soon as you speak to the king. Once you do, he tells you he’s dying and has chosen you as his heir, but first you have to prove yourself by going out and finding three things that he wants you to bring back.Moving about the world is done using the arrow keys, and the game engine actually allows you to move in four directions as well as creating a sense of immersion by allowing you to walk behind things. This was a huge leap forward compared to the text parser mechanism previously used to navigate game worlds as instead of typing GO EAST and a new wall of text describing the interaction, you just … move yourself east to the edge of the screen and a new screen renders as a result.Interacting with the world is still done using a text parser, however this time it’s much more sophisticated and can parse multi word phrases and can understand certain word variations, making for a less frustrating experience.Let’s not kid ourselves though, this game can still be extremely frustrating. Basically everything can (and will) kill you if you get too close to it, there are certain screens with random encounters (ogres, wizards, the with) that will end your game and if you fall in the water and forget to type swim … you drown.The puzzles are generally intuitive, and progressing through through the game isn’t overly difficult. For example, when you meet a couple in their cabin and see there’s a fiddle in the corner, you assume you’re going to need this item and can either take it or have to do something for the couple for them to give it to you. If you try to take it, the game says something like “you can’t take their only possession!”, then if you talk to them they say they can’t offer you anything as they have no food.While walking about, you find a random bowl on the ground with the words “FILL” in it. Typing fill once you’ve picked it up will fill the bowl with food magically. I wonder what you’re supposed to do with this … Oh right :PThere are some frustrating puzzles as well, like the quest for the magic beans. Since King’s Quest loosely follows a fairy tale story line and integrates a number of tropes, you’re supposed to know things like “goats don’t like trolls” to be able to cross a bridge. Once you puzzle that out, you get to an old man who will give you the beans if you can guess his name. A clue in the game tells you “it helps to think backwards”, and if you know the Rumpelstiltskin story you know you just need to say his name backwards … but Sierra decided to make this even more complicated by instead making you reverse the letter characters of “Rumpelstiltskin” instead of just typing it in backwards. Oh yeah … I needed a walkthrough for that …You finally find the three treasures, kill a dragon and a giant, DJ a Leprechaun dance party and bring all that shit back to the king and … he dies … but not before crowning you king of Daventry and setting you up for a 7 game franchise (yeah I know it’s 8 … but we don’t speak of Mask of Eternity).Overall this game is still very accessible and playable, even as a text parser game. There was an SCI Remake in 1990 which updated the graphics, but the text parser interface remained (check out the demo trailer for the SCI remake!). For anyone interested in early adventure games, this one should be high on their list and as it can be played in ScummVM you can play it on any system you happen to be reading this on.Game Information Game King’s Quest: Quest for the Crown Developer Sierra On-Line Publisher Sierra On-Line Release Date May 10, 1984 Systems PCjr, Tandy 1000, Amiga, Apple II, Apple IIGS,Atari ST, Macintosh, DOS, Master System Game Engine Adventure Game Interpreter (AGI) Play Information Time to Completion 31 minutes Version Played DOS via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 17 Story (25) 20 Experience (15) 12 Impact (10) 8   81% Gallery" }, { "title": "Mystery House (On-Line Systems) - 1980", "url": "/blog/2021/08/03/mystery-house/", "categories": "Let's Adventure!", "tags": "adventure, ADL, Sierra On-Line, Online Systems", "date": "2021-08-03 07:25:36 -0400", "snippet": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Mystery House is an adventure game released by On-Line Systems in 1980. It was designed, written...", "content": " This review is part of the “Let’s Adventure!” series. See all reviewed games sorted by rating here.Mystery House is an adventure game released by On-Line Systems in 1980. It was designed, written and illustrated by Roberta Williams, and programmed by Ken Williams for the Apple II. Mystery House is the first graphical adventure game ever and the first game produced by On-Line Systems, the company which would evolve into Sierra On-Line, and is one of the earliest horror video games.This game holds some fond memories as I do remember playing this with a friend when I was a kid. I didn’t own an Apple II, and didn’t get a 386 until I was about 8 years old so I had to rely on others to play these games. I started out by finding some Apple II disk images and trying to get this running on an emulator, but it turns out the full game can be legally downloaded from the ScummVM Game Downloads page.It took a bit of getting used to the text parser interface as it is very unforgiving when you don’t enter the right commands. This is to be expected considering this is one of the oldest adventure games available, but having been spoiled by 30+ years of playing adventure games made getting back into this mindset challenging. Once you reacquaint yourself with the 2 word parser, go UP and OPEN DOOR you’re greeted by all the people who’ve been invited to this mystery house.There are random notes scattered through the house that indicate there is a murderer amongst the group, as well as a treasure hidden somewhere. Your goal it seems is to survive and escape with said treasure, so into the house you go, collecting items and trying to use them in the correct locations to solve puzzles along the way.As On-Line Systems would eventually become Sierra On-Line, it’s fitting that one of the recurring tropes of their games would begin here. If you don’t act appropriately, you die :P Unlike the LucasArts games that would come later (and prove to be much more forgiving), Sierra games really pushed you to “Save Early, Save Often” as there were myriad ways to die as you progressed through their stories.The story here doesn’t really get set up and you’re plopped in front of the house without much to go on. It becomes apparent as you progress what you’re supposed to do, and there aren’t many scenes to explore or items to collect. Though the images are monochromatic and there is no sound, this WAS a graphical adventure game, unlike the interactive fiction games that would come later (such as Zork).Since this was the first graphical adventure game you can’t really compare it to anything else. Mystery House set the stage for 2 decades of adventure games and the rise of Sierra Online, LucasArts and others who would release some of my personal favourite games of all time. You can breeze through this title pretty quickly and it really has no replayability factor, however if you’re a fan of the genre it’s worth spending some time with the title that started it all.Game Information Game Mystery House Developer Online Systems Publisher Online Systems Release Date 1980 Systems Apple II Game Engine ADL (Adventure Development Language) Play Information Time to Completion 40 minutes Version Played Apple II via ScummVM Notes Walkthrough ScoreSee here for a refresher on how we’re scoring these games. Atmosphere (20) 15 Story (25) 15 Experience (15) 8 Impact (10) 8   66% Gallery" }, { "title": "Let's Adventure! A Journey into Adventure Games (1980-1999)", "url": "/blog/2021/07/28/adventure-games-1980-1999/", "categories": "Let's Adventure!", "tags": "series, adventure", "date": "2021-07-28 13:01:39 -0400", "snippet": "When I was growing up in the 80’s my favourite games to play were adventure games. I’ve always found myself going back to this genre, and as I grew up and moved into software development the ScummV...", "content": "When I was growing up in the 80’s my favourite games to play were adventure games. I’ve always found myself going back to this genre, and as I grew up and moved into software development the ScummVM project held a certain appeal as it combined two passions for me.Though I was never very active in the project, I tried to contribute in various ways over the years, finally seeing some success recently with the Asylum engine being merged.When I stumbled across the CRPG Addict’s blog many years ago I wanted to do something similar, but for adventure games. I’ve decided to take a run at this as (a very long time ago) I used to write for an online magazine (Game Over) under the pseudonym “Lothian”. A quick Google search will unearth some examples if you’re curious …It’s worth calling out the reviewers at The Adventure Gamer, who have been blogging about these games in a similar fashion as the CRPG Addict for nearly as long. Both sites go far more in depth than I plan to and focus on documenting their playthroughs. My goal here is to summarize the experience, though the format may change over time depending on how well received this is (and if I can stay focused or not :P).GuidelinesThe Full Game List is 363 titles, however I only plan on playing those that are available in English. Any title that can be played using ScummVM likely will be as this facilitates things like keeping track of play time, taking screenshots, managing save states and more.I don’t plan on playing the games in order either, as this might become a bit tedious. I will be starting with Mystery House however as we might as well kick this journey off at the very beginning and see where it takes us from there.For a number of the older Sierra Online games there are SCI remakes that replaced the text parser with a point-and-click interface. I may review some of these games twice to cover both experiences, however this likely won’t be the norm.ScoringMy criteria won’t be as involved as the CRPG Addict’s GIMLET, however I want to be consistent with how each game is being measured.When writing for Game Over we also had a fixed set of criteria (Graphics, Sound, Gameplay, Fun, Multiplayer, Overall). I want to bring something like this over to this series, however in a way that won’t penalize older entries in the list due to “poorer” graphics/sound.The scoring we’ll be using for this series will focus on the following:Atmosphere: This will focus on how the game makes you feel while you’re playing it and includes inputs such as Music, Sound Effects, Graphics, Cut Scenes and overall quality.Max Points: 20Story: Adventure games are very story-driven, and as such the story needs to be evaluated as a top-level criteria. Focus will be on Quality, Immersion, Complexity, Progression and Pacing.Max Points: 25Experience: How much Fun was the game? Was the Difficulty too high/low? Were the puzzles a Challenge or were they repetitive and felt derivative and lazy? Was there any introduction for the player or are you dropped in blind?Max Points: 15Impact: Did you enjoy the overall experience? Would you play this again? Would you want to know more about this world and its characters in subsequent adventures?Max Points: 10Each game will be ranked using the above, and a percentage generated from the scores to show the overall score of the game.Games ListThe games list for this journey is available via this Google Sheet, and has been taken from Wikipedia’s List of Graphic Adventure Games. This only includes entries from 1980 through 1999.As I work through the list I’ll link the reviews below. You can also find the reviews (in order of publication) under the “Let’s Adventure!” category page or sorted by score.{ progress: { finished: 42, skipped: 11, total: 363 }, complete: \"14.6%\"} The list below was generated on July 28, 2021 and may have diverged from what is currently listed on Wikipedia Mystery House (On-Line Systems) - 1980 Wizard and the Princess (On-Line Systems) - 1980 Mission Asteroid (On-Line Systems) - 1980 Cranston Manor (On-Line Systems) - 1981 Ulysses and the Golden Fleece (On-Line Systems) - 1981 Adventureland (Adventure International) - 1982 Kabul Spy (Sirius Software) - 1982 Time Zone (On-Line Systems) - 1982 Transylvania (Penguin Software) - 1982 The Dark Crystal (On-Line Systems) - 1983 Star Arthur Legend: Planet Mephius (T&amp;E Soft) - 1983 (Japanese only) Portopia Renzoku Satsujin Jiken (Yuji Horii, Chunsoft) - 1983 (Japanese only) Rendezvous with Rama (Telarium) - 1984 Below the Root (Dale Disharoon) - 1984 Dallas Quest (Datasoft) - 1984 Okhotsk ni Kiyu: Hokkaido Rensa Satsujin Jiken (Yuji Horii, Login Software) - 1984 (Japanese only) Princess Tomato in the Salad Kingdom (Hudson Soft) - 1984 Wingman (TamTam) - 1984 (Japanese only) King’s Quest: Quest for the Crown (Sierra On-Line) - 1984 The Crimson Crown - Further Adventures in Transylvania (Penguin Software) - 1985 Déjà Vu: A Nightmare Comes True (ICOM Simulations) - 1985 Perry Mason: The Case of the Mandarin Murder (Telarium) - 1985 King’s Quest II: Romancing the Throne (Sierra On-Line) - 1985 The Black Cauldron (Sierra On-Line) - 1986 J.B. Harold Murder Club (Riverhillsoft) - 1986 Labyrinth: The Computer Game (Lucasfilm Games) - 1986 Murder on the Mississippi (Activision) - 1986 The Scoop (Telarium) - 1986 Tass Times in Tonetown (Interplay Productions, Brainwave Creations) - 1986 Uninvited (ICOM Simulations) - 1986 Space Quest: The Sarien Encounter (Sierra On-Line) - 1986 King’s Quest III: To Heir Is Human (Sierra On-Line) - 1986 Suishō no Dragon (Square) - 1986 (Japanese only, though a fan translation exists) La Abadía del Crimen (The Abbey of Crime) (Opera Soft) - 1987 (Spanish only) Mortville Manor (Lankhor) - 1987 Police Quest: In Pursuit of the Death Angel (Sierra On-Line) - 1987 Leisure Suit Larry in the Land of the Lounge Lizards (Sierra On-Line) - 1987 Shadowgate (ICOM Simulations) - 1987 The Faery Tale Adventure (MicroIllusions) - 1987 Méwilo (Coktel Vision) - 1987 (German, French only) Maniac Mansion (Lucasfilm Games) - 1987 Space Quest II: Vohaul’s Revenge (Sierra On-Line) - 1987 Déjà Vu II: Lost in Las Vegas (ICOM Simulations) - 1988 Gold Rush! (Sierra On-Line) - 1988 Manhunter: New York (Evryware) - 1988 Neuromancer (Interplay Productions) - 1988 King’s Quest IV: The Perils of Rosella (Sierra On-Line) - 1988 Leisure Suit Larry Goes Looking for Love (in Several Wrong Places) (Sierra On-Line) - 1988 Zak McKracken and the Alien Mindbenders (Lucasfilm Games) - 1988 Police Quest II: The Vengeance (Sierra On-Line) - 1988 Snatcher (Konami) - 1988 B.A.T. (Computer’s Dream) - 1989 Emmanuelle (Coktel Vision) - 1989 Legend of Djel (Coktel Vision) - 1989 Future Wars (Delphine Software International) - 1989 KULT: The Temple of Flying Saucers (Exxos, ERE informatique) - 1989 Manhunter 2: San Francisco (Evryware) - 1989 Mean Streets (Access Software) - 1989 Transylvania III: Vanquish the Night (Polarware) - 1989 Space Quest III: The Pirates of Pestulon (Sierra On-Line) - 1989 Indiana Jones and the Last Crusade (Lucasfilm Games) - 1989 Quest for Glory: So You Want to Be a Hero (Sierra On-Line) - 1989 The Colonel’s Bequest (Sierra On-Line) - 1989 Codename: ICEMAN (Sierra On-Line) - 1989 Leisure Suit Larry III: Passionate Patti in Pursuit of the Pulsating Pectorals (Sierra On-Line) - 1989 Altered Destiny (Accolade) - 1990 Countdown (Access Software) - 1990 Earthrise: A Guild Investigation (Matt Gruson) - 1990 Geisha (Coktel Vision) - 1990 Hugo’s House of Horrors (Gray Design Associates) - 1990 Les Manley in: Search for the King (Accolade) - 1990 Maupiti Island (Lankhor) - 1990 Operation Stealth (Delphine Software International) - 1990 Quest for Glory II: Trial by Fire (Sierra On-Line) - 1990 Rise of the Dragon (Dynamix) - 1990 Conquests of Camelot: The Search for the Grail (Sierra On-Line) - 1990 Loom (Lucasfilm Games) - 1990 Spellcasting 101: Sorcerers Get All The Girls (Legend Entertainment) - 1990 The Secret of Monkey Island (Lucasfilm Games) - 1990 King’s Quest V: Absence Makes the Heart Go Yonder! (Sierra On-Line) - 1990 The Adventures of Willy Beamish (Dynamix) - 1991 Conquests of the Longbow: The Legend of Robin Hood (Sierra On-Line) - 1991 Cruise for a Corpse (Delphine Software International) - 1991 Fascination (Coktel Vision) - 1991 EcoQuest: The Search for Cetus (Sierra On-Line) - 1991 Elvira II: The Jaws of Cerberus (Horrorsoft) - 1991 Gobliiins (Coktel Vision) - 1991 Heart of China (Dynamix) - 1991 Hugo II, Whodunit? (Gray Design Associates) - 1991 Les Manley in: Lost in L.A. (Accolade) - 1991 Martian Memorandum (Access Software) - 1991 Police Quest III: The Kindred (Sierra On-Line) - 1991 Sherlock Holmes: Consulting Detective (ICOM Simulations) - 1991 Spellcasting 201: The Sorcerer’s Appliance (Legend Entertainment) - 1991 Timequest (Legend Entertainment) - 1991 Space Quest IV: Roger Wilco and the Time Rippers (Sierra On-Line) - 1991 Metal Slader Glory (HAL Laboratory) - 1991 (Japanese only, though a fan translation exists) Leisure Suit Larry 5: Passionate Patti Does a Little Undercover Work (Sierra On-Line) - 1991 Monkey Island 2: LeChuck’s Revenge (LucasArts) - 1991 Amazon: Guardians of Eden (Access Software) - 1992 B.A.T. II – The Koshan Conspiracy (Computer’s Dream) - 1992 Bargon Attack (Coktel Vision) - 1992 The Dagger of Amon Ra (Sierra On-Line) - 1992 Dark Seed (Cyberdreams) - 1992 Daughter of Serpents (The Scroll) (Eldritch Games) - 1992 Dune (Cryo Interactive) - 1992 Eternam (Infogrames) - 1992 Gobliins 2: The Prince Buffoon (Coktel Vision) - 1992 Hook (Ocean) - 1992 Hugo III, Jungle of Doom! (Gray Design Associates) - 1992 Inca (Coktel Vision) - 1992 Inspector Gadget: Mission 1 - Global Terror! (Azeroth, Inc.) - 1992 KGB (Cryo Interactive) - 1992 Leather Goddesses of Phobos 2: Gas Pump Girls Meet the Pulsating Inconvenience from Planet X! (Infocom) - 1992 The Legacy: Realm of Terror (Magnetic Scrolls) - 1992 Linus Spacehead’s Cosmic Crusade/Cosmic Spacehead (Codemasters (NES version), Supersonic Software) - 1992 The Lost Files of Sherlock Holmes: The Case of the Serrated Scalpel (Mythos Software, Inc.) - 1992 The Palace of Deceit: The Dragon’s Plight (Game Syndicate Productions (Cliff Bleszinski)) - 1992 Plan 9 from Outer Space (Gremlin Ireland) - 1992 Putt-Putt Joins the Parade (Humongous Entertainment) - 1992 Sherlock Holmes: Consulting Detective Vol. II (ICOM Simulations) - 1992 Ringworld: Revenge of the Patriarch (Tsunami Games) - 1992 Spellcasting 301: Spring Break (Legend Entertainment) - 1992 Star Trek: 25th Anniversary (Interplay) - 1992 Nightshade (Beam Software) - 1992 Frederik Pohl’s Gateway (Legend Entertainment) - 1992 Indiana Jones and the Fate of Atlantis (LucasArts) - 1992 Lure of the Temptress (Revolution Software) - 1992 Quest for Glory III: Wages of War (Sierra On-Line) - 1992 The Legend of Kyrandia: Fables and Fiends (Westwood Studios) - 1992 King’s Quest VI: Heir Today, Gone Tomorrow (Sierra On-Line) - 1992 Curse of Enchantia (Core Design) - 1992 Rex Nebular and the Cosmic Gender Bender (MicroProse) - 1992 The Adventures of Melvin Freebush (Sherwood Forest Games) - 1993 Blue Force (Tsunami Games) - 1993 Dare to Dream (Epic MegaGames) - 1993 Dracula Unleashed (ICOM Simulations) - 1993 EcoQuest 2: Lost Secret of the Rainforest (Sierra On-Line) - 1993 Eric the Unready (Legend Entertainment) - 1993 Fatty Bear’s Birthday Surprise (Humongous Entertainment) - 1993 Freddy Pharkas: Frontier Pharmacist (Sierra On-Line) - 1993 Gobliins Quest 3 (Coktel Vision) - 1993 Inca II: Nations of Immortality (Coktel Vision) - 1993 Innocent Until Caught (Divide By Zero) - 1993 The Legend of Kyrandia: Hand of Fate (Westwood Studios) - 1993 Lost in Time (Coktel Vision) - 1993 Pepper’s Adventures in Time (Sierra On-Line) - 1993 Putt-Putt Goes to the Moon (Humongous Entertainment) - 1993 Return of the Phantom (MicroProse) - 1993 Sherlock Holmes: Consulting Detective Vol. III (ICOM Simulations) - 1993 The Journeyman Project (Presto Studios) - 1993 Space Quest V: Roger Wilco – The Next Mutation (Dynamix) - 1993 Tajemnica Statuetki (Metropolis Software House) - 1993 (Polish only) Veil of Darkness (Event Horizon Software) - 1993 Shadow of the Comet (Infogrames) - 1993 The 7th Guest (Trilobyte) - 1993 Leisure Suit Larry 6: Shape Up or Slip Out! (Sierra On-Line) - 1993 Maniac Mansion: Day of the Tentacle (LucasArts) - 1993 Return to Zork (Infocom) - 1993 Myst (Cyan) - 1993 Simon the Sorcerer (Adventure Soft) - 1993 Companions of Xanth (Legend Entertainment) - 1993 Police Quest: Open Season (Sierra On-Line) - 1993 Sam &amp; Max Hit the Road (LucasArts) - 1993 Star Trek: Judgment Rites (Interplay) - 1993 Gateway II: Homeworld (Legend Entertainment) - 1993 Jack in the Dark (Infogrames) - 1993 Quest for Glory: Shadows of Darkness (Sierra On-Line) - 1993 Gabriel Knight: Sins of the Fathers (Sierra On-Line) - 1993 Jurassic Park (Sega of America) - 1993 Alice: An Interactive Museum (Toshiba-EMI) - 1994 Are You Afraid of the Dark: The Tale of Orpheo’s Curse (Viacom New Media) - 1994 The Bizarre Adventures of Woodruff and the Schnibble (Coktel Vision) - 1994 BloodNet (MicroProse) - 1994 Commander Blood (Cryo Interactive) - 1994 Return to Ringworld (Tsunami Games) - 1994 Dreamweb (Creative Reality) - 1994 Gadget: Invention, Travel, &amp; Adventure (Synergy, Inc.) - 1994 Hell: A Cyberpunk Thriller (Take-Two Interactive Software) - 1994 Igor: Objective Uikokahonia (Pendulo Studios) - 1994 Inherit the Earth: Quest for the Orb (The Dreamers Guild) - 1994 The Journeyman Project Turbo (Presto Studios) - 1994 The Legend of Kyrandia: Malcolm’s Revenge (Westwood Studios) - 1994 Noctropolis (Flashpoint Productions) - 1994 Plague of the Moon (Rebelsoft) - 1994 Superhero League of Hoboken (Legend Entertainment) - 1994 Under a Killing Moon (Access Software) - 1994 Universe (Core Design) - 1994 Dragonsphere (MicroProse) - 1994 Beneath a Steel Sky (Revolution Software) - 1994 Policenauts (Konami) - 1994 Death Gate (Legend Entertainment) - 1994 King’s Quest VII: The Princeless Bride (Sierra On-Line) - 1994 Dragon Lore: The Legend Begins (Cryo Interactive) - 1994 The Beast Within: A Gabriel Knight Mystery (Sierra On-Line) - 1995 The Big Red Adventure (Dynabyte) - 1995 Chewy: Esc from F5 (Blue Byte) - 1995 Chop Suey (game) (Theresa Duncan) - 1995 Cosmology of Kyoto (Softedge) - 1995 The Daedalus Encounter (Mechadeus) - 1995 The Dark Eye (Inscape) - 1995 Discworld (Teeny Weeny Games/Perfect 10 Productions) - 1995 Dust: A Tale of the Wired West (Cyberflix) - 1995 EVE Burst Error (C’s Ware) - 1995 Flight of the Amazon Queen (Interactive Binary Illusions) - 1995 Frankenstein: Through the Eyes of the Monster (Amazing Media) - 1995 Guilty (Divide By Zero) - 1995 Heaven’s Dawn (Art 9 Entertainment Inc.) - 1995 In the 1st Degree (Adair &amp; Armstrong) - 1995 Jewels of the Oracle (Dreamcatcher Interactive) - 1995 The Journeyman Project 2: Buried in Time (Presto Studios) - 1995 Karma Curse of the 12 Caves (Dreamcatcher Interactive) - 1995 Kingdom: The Far Reaches (Virtual Image Productions) - 1995 The Orion Conspiracy (Divide By Zero) - 1995 Panic in the Park (Imagination Pilots) - 1995 Prisoner of Ice (Infogrames) - 1995 Qin: Tomb of the Middle Kingdom (Learning Technologies Interactive) - 1995 SilverLoad (Millennium Interactive) - 1995 Skyborg: Into the Vortex (SkyBox International) - 1995 Teen Agent (Metropolis Software House) - 1995 Touché: The Adventures of the Fifth Musketeer (Clipper Software) - 1995 Wolfsbane (Midnight Games) - 1995 Lost Eden (Cryo Interactive) - 1995 Nocturnal Illusion (Apricot) - 1995 Full Throttle (LucasArts) - 1995 Aliens: A Comic Book Adventure (Cryo Interactive) - 1995 Star Trek: The Next Generation – A Final Unity (Spectrum HoloByte) - 1995 Space Quest 6: The Spinal Frontier (Sierra On-Line) - 1995 Roberta Williams’ Phantasmagoria (Sierra On-Line) - 1995 Beavis and Butt-Head in Virtual Stupidity (Viacom New Media) - 1995 Scooby-Doo Mystery (The Illusions Gaming Company) - 1995 Clock Tower (Human Entertainment) - 1995 (Japanese only, though a fan translation exists) Ripley’s Believe It or Not!: The Riddle of Master Lu (Sanctuary Woods) - 1995 I Have No Mouth, and I Must Scream (The Dreamers Guild) - 1995 Torin’s Passage (Sierra On-Line) - 1995 Mission Critical (Legend Entertainment) - 1995 Shivers (Sierra On-Line) - 1995 Simon the Sorcerer II: The Lion, the Wizard and the Wardrobe (Adventure Soft) - 1995 Total Distortion (Pop Rocket) - 1995 Dark Seed II (Destiny Media Technologies) - 1995 The Dig (LucasArts) - 1995 The 11th Hour (Trilobyte) - 1995 Shannara (Legend Entertainment) - 1995 Chronomaster (DreamForge Intertainment) - 1995 Robot City (Brooklyn Multimedia) - 1995 Alien Incident (Housemarque) - 1996 Amber: Journeys Beyond (Hue Forest Entertainment) - 1996 Blue Ice (Art of Mind Productions) - 1996 Creature Crunch (TechToons Ltd.) - 1996 Urban Runner (Coktel Vision) - 1996 The Dame Was Loaded (Beam Software) - 1996 Discworld II: Missing Presumed…!? (Perfect Entertainment) - 1996 Dragon Lore II: The Heart of the Dragon Man (Cryo Interactive) - 1996 Fable (Simbiosis Interactive) - 1996 The Gene Machine (Divide By Zero) - 1996 Kingdom II: Shadoan (Virtual Image Productions) - 1996 Monty Python &amp; the Quest for the Holy Grail (7th Level) - 1996 Mutation of J.B. (Invention) - 1996 Orion Burger (Sanctuary Woods) - 1996 UFOs (Artech) - 1996 Angel Devoid: Face of the Enemy (Electric Dreams) - 1996 Chronicles of the Sword (Synthetic Dimensions) - 1996 Bad Mojo (Pulse Entertainment) - 1996 Ripper (Take-Two Interactive) - 1996 Spycraft: The Great Game (Activision) - 1996 Zork Nemesis (Zombie LLC) - 1996 Normality (Gremlin Interactive) - 1996 Kingdom O’ Magic (SCi) - 1996 The Pandora Directive (Access Software) - 1996 Azrael’s Tear (Intelligent Games) - 1996 Harvester (DigiFX Interactive) - 1996 The Lost Files of Sherlock Holmes: The Case of the Rose Tattoo (Mythos Software, Inc.) - 1996 Broken Sword: The Shadow of the Templars (Revolution Software) - 1996 Timelapse (GTE Interactive Media) - 1996 Lighthouse: The Dark Being (Sierra On-Line) - 1996 Ace Ventura (7th Level) - 1996 Blazing Dragons (The Illusions Gaming Company) - 1996 Drowned God: Conspiracy of the Ages (Epic Multimedia Group) - 1996 Martian Chronicles, The (Byron Preiss Multimedia Company) - 1995 The Neverhood (The Neverhood, Inc.) - 1996 The Pink Panther: Passport to Peril (Wanderlust Interactive) - 1996 Toonstruck (Burst Studios) - 1996 Rama (Dynamix) - 1996 Titanic: Adventure Out of Time (Cyberflix) - 1996 Leisure Suit Larry: Love for Sail! (Sierra On-Line) - 1996 Phantasmagoria: A Puzzle of Flesh (Sierra On-Line) - 1996 YU-NO: A girl who chants love at the bound of this world (ELF) - 1996 Ark of Time (Trecision) - 1997 The City of Lost Children (Psygnosis) - 1997 Crusader: Adventure Out of Time (Index+) - 1997 Egypt 1156 B.C. (Cryo Interactive Entertainment) - 1997 Jack Orlando (TopWare Interactive) - 1997 Riana Rouge (Black Dragon Productions) - 1997 Sentient (Psygnosis) - 1997 The Simpsons: Virtual Springfield (Digital Evolution) - 1997 Versailles 1685 (Cryo Interactive) - 1997 Voodoo Kid (Infogrames, Eric Sterling Collins, Hubert Chardot) - 1997 WuKung: A Legendary Adventure (Abudoe Software) - 1997 Realms of the Haunting (Gremlin Interactive) - 1997 Obsidian (Rocket Science Games) - 1997 Atlantis: The Lost Tales (Cryo Interactive) - 1997 The Last Express (Smoking Car Productions) - 1997 Shivers II: Harvest of Souls (Sierra On-Line) - 1997 Callahan’s Crosstime Saloon (Legend Entertainment) - 1997 Safecracker (Daydream Software) - 1997 Duckman: The Graphic Adventures of a Private Dick (The Illusions Gaming Company) - 1997 The Feeble Files (Adventure Soft) - 1997 The Journeyman Project: Pegasus Prime (Presto Studios) - 1997 The Space Bar (Boffo Games) - 1997 Spy Fox in “Dry Cereal” (Humongous Entertainment) - 1997 Temüjin: A Supernatural Adventure (SouthPeak Games) - 1997 Broken Sword II: The Smoking Mirror (Revolution Software) - 1997 The Pink Panther: Hokus Pokus Pink (Wanderlust Interactive) - 1997 Dark Earth (Kalisto Entertainment) - 1997 Armed &amp; Delirious (Makh-Shevet) - 1997 Riven (Cyan) - 1997 The Curse of Monkey Island (LucasArts) - 1997 Zork: Grand Inquisitor (Activision) - 1997 Blade Runner (Westwood Studios) - 1997 Hollywood Monsters (Pendulo Studios) - 1997 (Spanish only, though a fan translation exists) Ankh: The Tales of Mystery (Artex Software) - 1998 Morpheus (Soap Bubble Productions) - 1998 Nightlong: Union City Conspiracy (Team17, Trecision) - 1998 ZeroZone (R&amp;P Electronic Media) - 1998 The Journeyman Project 3: Legacy of Time (Presto Studios) - 1998 Black Dahlia (Take-Two Interactive) - 1998 Tex Murphy: Overseer (Access Software) - 1998 Starship Titanic (The Digital Village) - 1998 Sanitarium (DreamForge Intertainment) - 1998 The X-Files Game (HyperBole Studios) - 1998 Hopkins FBI (MP Entertainment) - 1998 Ring: The Legend of the Nibelungen (Arxel Tribe) - 1998 Grim Fandango (LucasArts) - 1998 Nancy Drew: Secrets Can Kill (Her Interactive) - 1998 John Saul’s Blackstone Chronicles: An Adventure in Terror (Legend Entertainment) - 1998 Juggernaut (Tonkin House) - 1998 King’s Quest: Mask of Eternity (Sierra Studios) - 1998 Dark Side of the Moon: A Sci-Fi Adventure (SouthPeak Games) - 1998 Quest for Glory V: Dragon Fire (Sierra On-Line) - 1998 Discworld Noir (Perfect Entertainment, Teeny Weeny Games (PS)) - 1999 The Forgotten: It Begins (Ransom Interactive) - 1999 Paris 1313 (Dramæra) - 1999 Physicus (HEUREKA-Klett Softwareverlag) - 1999 Scooby-Doo: Mystery of the Fun Park Phantom (Engineering Animation, Inc.) - 1999 Tony Tough and the Night of Roasted Moths (Nayma Software, Prograph Research S.r.l.) - 1999 Zero Critical (Istvan Pely Productions) - 1999 Biosys (JumpStart Solutions Ltd.) - 1999 Beavis and Butt-Head Do U. (The Illusions Gaming Company) - 1999 Amerzone (Microïds) - 1999 Aztec: The Curse in the Heart of the City of Gold (Cryo Interactive) - 1999 Traitors Gate (Daydream Software) - 1999 Dracula: Resurrection (Index+, Canal+ Multimedia and France Telecom Multimedia) - 1999 Faust (Arxel Tribe) - 1999 The Silver Case (Grasshopper Manufacture) - 1999 Spy Fox 2: “Some Assembly Required” (Humongous Entertainment) - 1999 Omikron: The Nomad Soul (Quantic Dream) - 1999 Atlantis II (Cryo Interactive) - 1999 Freddi Fish 4: The Case of the Hogfish Rustlers of Briny Gulch (Humongous Entertainment) - 1999 Nancy Drew: Stay Tuned for Danger (Her Interactive) - 1999 The Longest Journey (Funcom) - 1999 Gabriel Knight 3: Blood of the Sacred, Blood of the Damned (Sierra On-Line) - 1999 Star Trek: Hidden Evil (Presto Studios) - 1999 The Crystal Key (Earthlight Productions) - 1999" }, { "title": "Generating a ScummVM Games List", "url": "/blog/2021/05/31/generating-a-scummvm-games-list/", "categories": "Programming", "tags": "scummvm, ruby", "date": "2021-05-31 06:55:45 -0400", "snippet": " 2021-06-01: Note that you can actually generate the games list directly from the scummvm binary by running scummvm --list-all-games. Oops …A long time ago I reached out to the ScummVM leadership ...", "content": " 2021-06-01: Note that you can actually generate the games list directly from the scummvm binary by running scummvm --list-all-games. Oops …A long time ago I reached out to the ScummVM leadership about a script I’d been working on that could be used to scrape detection entries and generate a definitive games list.As with most of my ScummVM-related development, this fell by the wayside for a number of years, but recently I’ve rekindled my love of scripting (and Ruby) so I wanted to try and get this script at least into a (somewhat) working state.def replace_target(path, target) p = path.split(&#39;/&#39;) p[-1] = target return p.join(&#39;/&#39;).to_senddef cleanup_title(c) title = c.gsub(&#39;{&#39;,&#39;&#39;).gsub(&#39;}&#39;,&#39;&#39;).gsub(&quot;\\t&quot;, &#39; &#39;).split(&#39;, &#39;)[1..-1].join.strip title = title.delete_prefix(&#39;&quot;&#39;).delete_suffix(&#39;&quot;&#39;).delete_suffix(&#39; ,&#39;).delete_suffix(&quot;\\&quot;,&quot;).delete_suffix(&quot;\\&quot;&quot;).delete_suffix(&quot;\\&quot; ;&quot;) titleendtarget = &quot;detection.cpp&quot;fallback = &quot;detection_tables.h&quot;search_term = &quot;static const PlainGameDescriptor &quot;fallback_term = &quot;const PlainGameDescriptor &quot;files = %x[find `pwd` -name &#39;#{target}&#39;].split(&quot;\\n&quot;)result = []files.each do |f| offset = %x[grep -n &quot;#{search_term}&quot; #{f}].split(&#39;:&#39;)[0].to_i if offset == 0 f = replace_target(f, fallback) offset = %x[grep -n &quot;#{search_term}&quot; #{f}].split(&#39;:&#39;)[0].to_i if File.exists?(f) # kyra offset = %x[grep -n &quot;#{fallback_term}&quot; #{f}].split(&#39;:&#39;)[0].to_i if offset == 0 &amp;&amp; File.exists?(f) end if offset &gt; 0 terminator1 = %x[sed -n &#39;#{offset + 1}, $ p&#39; &#39;#{f}&#39; | grep -n &#39;0, 0&#39;].split(&#39;:&#39;)[0].to_i terminator2 = %x[sed -n &#39;#{offset + 1}, $ p&#39; &#39;#{f}&#39; | grep -n &#39;;&#39;].split(&#39;:&#39;)[0].to_i length = (terminator1 &lt; terminator2) ? terminator1 : terminator2 content = %x[sed -n &#39;#{offset + 1}, #{offset + length - 1} p&#39; #{f}] engine = f.split(&#39;/&#39;)[-2] content.strip.split(&quot;,\\n&quot;).each do |c| title = cleanup_title(c) if title.index(&quot;//&quot;).nil? result &lt;&lt; { engine: engine, title: title } unless title == &quot;nullptr&quot; else titles = c.split(&quot;\\n\\t&quot;) titles.each do |t| result &lt;&lt; { engine: engine, title: cleanup_title(t) } end end end endendresult.reject! { |obj| obj[:title].empty? }puts &quot;| Engine | Title (#{result.length}) |&quot;puts &quot;|--------|--------------------------|&quot;result.sort_by { |obj| obj[:title] }.each do |o| puts &quot;|#{o[:engine]}|#{o[:title]}|&quot;endThis script just needs to be downloaded to the ScummVM source directory and run. At the time of writing this script doesn’t deduplicate results and is not guaranteed to be 100% accurate, but it should be close enough to give you an idea how many great games are currently available (though possibly still under development) within ScummVM at the commit point you currently have checked out.The results below are recent as of commit d053c8b494: Engine Title (770) wintermute 1 1/2 Ritter: Auf der Suche nach der hinreissenden Herzelinde toltecs 3 Skulls of the Toltecs agt A Bloody Life ags A Christmas Tale ags A Day In The Future wage A Mess O’ Trouble director A Silly Noisy House agi AGI Demo agi AGI Tetris ags AGS Game Scanner director ALeX-WORLD director AMBER: Journeys Beyond ags ASAP Adventure ags Aaron’s Epic Journey sludge Above The Waves ags Ace Duswell - Where’s The Ace director Activision’s Atari 2600 Action Pack wintermute Actual Destination adrift Adrift IF Game advsys AdvSys Game ags Adventure Game ags Adventure Game Studio Game lilliput Adventures of Robin Hood alan2 Alan2 Game alan3 Alan3 Game director Alice: An Interactive Museum wintermute Alimardan Meets Merlin wintermute Alimardan’s Mischief wintermute Alpha Polaris director AmandaStories access Amazon: Guardians of Eden director Ankh 2: Mystery of Tutankhamen director Ankh 3 director Ankh: Mystery of the Pyramids wage Another Fine Mess ags Anton Ulvfot’s Mid-Town Shootout wintermute Apeiron director Arc of Doom archetype Archetype IF Game wintermute Art of Murder 1: FBI Confidential director ArtRageous! sci Astro Chicken ags BLUECUP - on the run scumm Backyard Baseball scumm Backyard Baseball 2001 scumm Backyard Baseball 2003 scumm Backyard Basketball scumm Backyard Football scumm Backyard Football 2002 scumm Backyard Soccer scumm Backyard Soccer 2004 scumm Backyard Soccer MLS Edition director Bad Day on the Midway wintermute Barrow Hill - The Dark Path wintermute Basis Octavus scumm Bear Stormin’ illusions Beavis and Butt-head Do U bbvs Beavis and Butt-head in Virtual Stupidity sky Beneath a Steel Sky ags Bert the Newsreader wintermute Beyond the Threshold director Beyond the Wall of Stars wintermute Bickadoodle scumm Big Thinkers First Grade scumm Big Thinkers Kindergarten bladerunner Blade Runner bladerunner Blade Runner with restored content dragons Blazing Dragons tsage Blue Force scumm Blue’s 123 Time Activities scumm Blue’s ABC Time Activities scumm Blue’s Art Time Activities scumm Blue’s Birthday Adventure scumm Blue’s Reading Time Activities scumm Blue’s Treasure Hunt ags Bob’s Quest 2: The quest for the AGS Blue cup award wintermute Book of Gron Part One ags Book of Spells 1 ags Book of Spells 2 ags Book of Spells 3 wintermute Boredom of Agustin Cordes sword25 Broken Sword 2.5 sword1 Broken Sword: The Shadow of the Templars”; tucker Bud Tucker in Double Trouble director Busy People of Hamsterland director Byron Preiss Multimedia Catalog agi Caitlyn’s Destiny ags Calsoon wage Camp Cantitoe wintermute Carol Reed 10 - Bosch’s Damnation wintermute Carol Reed 11 - Shades Of Black wintermute Carol Reed 12 - Profound Red wintermute Carol Reed 13 - The Birdwatcher wintermute Carol Reed 14 - The Fall Of April wintermute Carol Reed 15 - Geospots wintermute Carol Reed 16 - Quarantine Diary wintermute Carol Reed 4 - East Side Story wintermute Carol Reed 5 - The Colour of Murder wintermute Carol Reed 6 - Black Circle wintermute Carol Reed 7 - Blue Madonna wintermute Carol Reed 8 - Amber’s Blood wintermute Carol Reed 9 - Cold Case Summer ags Carver Island 2: Mrs. Rodriguez’s Revenge sci Castle of Dr. Brain ags Chamber chewy Chewy: Esc from F5 wintermute Chivalry is Not Dead director Chop Suey sci Christmas Card 1988 sci Christmas Card 1990: The Seasoned Professional sci Christmas Card 1992 director Chu-Teng groovie Clandestiny sci Codename: Iceman wintermute Colors on Canvas ags Compensation sci Conquests of Camelot: King ArthurQuest for the Grail sci Conquests of the Longbow: The Adventures of Robin Hood wintermute Conspiracao Dumont wintermute Corrosion: Cold Winter Waiting magnetic Corruption director Cosmology of Kyoto sci Crazy Nick’s Software Picks: King Graham’s Board Game Challenge sci Crazy Nick’s Software Picks: Leisure Suit Larry’s Casino sci Crazy Nick’s Software Picks: Parlor Games with Laura Bow sci Crazy Nick’s Software Picks: Robin Hood’s Game of Skill and Chance sci Crazy Nick’s Software Picks: Roger Wilco’s Spaced Out Game Pack cruise Cruise for a Corpse ultima Crusader: No Regret ultima Crusader: No Remorse sludge Cubert BadboneP.I. director DEVO Presents: Adventures of the Smart Patrol wintermute DFAF Adventure composer Darby the Dragon wintermute Dark Fall: Lost Souls ags Darts scumm Day of the Tentacle wintermute Dead City ags Deepbright macventure Deja Vu macventure Deja Vu II ags Demon Slayer 1 ags Demon Slayer 2 ags Demon Slayer 3 ags Demon Slayer 4 agos Demon in my Pocket director Derrat Sorcerum wintermute Des Reves Elastiques Avec Mille Insectes Nommes Georges wintermute Devil In The Capital saga Dinotopia ags Dirk Chafberg wintermute Dirty Split tinsel Discworld tinsel Discworld 2: Missing Presumed …!? tinsel Discworld Noir director Don’t Quit Your Day Job agi Donald Duck’s Playground wintermute Dr. Bohus wintermute Dr. Doyle - Mystery Of The Cloche Hat draci Draci Historie mads Dragonsphere wage Drakmyth Castle drascula Drascula: The Vampire Strikes Back dreamweb DreamWeb wintermute Dreamcat wintermute Dreamscape illusions Duckman dm Dungeon Master director Earthtia Saga: Larthur’s Legend director Earthworm Jim ags Earwig Is Angry! director Eastern Mind: The Lost Souls of Tong Nou ags Eclair 1 ags Eclair 2 sci EcoQuest II: Lost Secret of the Rainforest sci EcoQuest: The Search for Cetus”// floppy is SCI1CD SCI1.1 wintermute Eight Squares in The Garden ags El Burro agos Elvira - Mistress of the Dark agos Elvira II - The Jaws of Cerberus wage Enchanted Scepters director Ernie ags Ernie’s Big Adventure ags Ernie’s Big Adventure 2 director Escape from Planet Arizona wintermute Escape from the Mansion wintermute Everyday Grey ags Exile kyra Eye of the Beholder kyra Eye of the Beholder II: The Legend of Darkmoon ags Eyes of the Jade Sphinx wintermute Face Noir saga Faery Tale Adventure II: Halls of the Dead wintermute Fairy Tales About Toshechka and Boshechka agi Fanmade AGI game sci Fanmade SCI Game scumm Fatty Bear’s Birthday Surprise scumm Fatty Bear’s Fun Pack wintermute Finding Hope ags Firewall magnetic Fish! wintermute Five Lethal Demons wintermute Five Magical Amulets queen Flight of the Amazon Queen ags Floyd wintermute Forgotten Sound 1 - Revelation wintermute Forgotten Sound 2 - Destiny wintermute Four wintermute FoxTail wintermute Framed director Frankenstein: Through the Eyes of the Monster sludge Frasse and the Peas of Kejick director Freak Show scumm Freddi Fish 1: The Case of the Missing Kelp Seeds scumm Freddi Fish 2: The Case of the Haunted Schoolhouse scumm Freddi Fish 3: The Case of the Stolen Conch Shell scumm Freddi Fish 4: The Case of the Hogfish Rustlers of Briny Gulch scumm Freddi Fish 5: The Case of the Creature of Coral Cove scumm Freddi Fish and Luther’s Maze Madness scumm Freddi Fish and Luther’s Water Worries scumm Freddi Fish’s One-Stop Fun Shop sci Freddy Pharkas: Frontier Pharmacist ngi Full Pipe scumm Full Throttle sci Fun Seeker’s Guide cine Future Wars sci Gabriel Knight: Sins of the Fathers sci Gabriel Knight: Sins of the Fathers director Gadget: InventionTravel&amp; Adventure ags Gaea Fallen director Gahan Wilson’s The Ultimate Haunted House wintermute Ghost in the Sheet glulx Glulx Game gnap Gnap agi Gold Rush! ags Gorther of the Cave People ags Granny Zombiekiller in: Mittens Murder Mystery ags Greg’s Mountainous Adventure composer Gregory and the Hot Air Balloon grim Grim Fandango director Gundam 0079: The War for Earth hadesch Hades Challenge wintermute Hamlet or the last game without MMORPG featuresshaders and product placement director Hamsterland: The Time Machine wintermute Helga Deep In Trouble ags Hermit adl Hi-Res Adventure #0: Mission Asteroid adl Hi-Res Adventure #1: Mystery House adl Hi-Res Adventure #2: Wizard and the Princess adl Hi-Res Adventure #3: Cranston Manor adl Hi-Res Adventure #4: Ulysses and the Golden Fleece adl Hi-Res Adventure #5: Time Zone adl Hi-Res Adventure #6: The Dark Crystal hopkins Hopkins FBI wintermute Hor sci Hoyle Bridge sci Hoyle Children’s Collection sci Hoyle Classic Card Games sci Hoyle Classic Games sci Hoyle Official Book of Games: Volume 1 sci Hoyle Official Book of Games: Volume 2 sci Hoyle Official Book of Games: Volume 3 sci Hoyle Solitaire hugo Hugo 1: Hugo’s House of Horrors hugo Hugo 2: Whodunit? hugo Hugo 3: Jungle of Doom hugo Hugo IF Game scumm Humongous Interactive Catalog director HyperBlade hdb Hyperspace Delivery Boy! saga I Have No Mouth and I Must Scream wintermute I Must Kill…: Fresh Meat sci ImagiNation Network (INN) Demo icb In Cold Blood scumm Indiana Jones and the Fate of Atlantis scumm Indiana Jones and the Last Crusade scumm Indiana Jones and the Last Crusade &amp; Loom scumm Indiana Jones and the Last Crusade &amp; Zak McKracken wintermute Informer Alavi - Murder of Miss Rojan saga Inherit the Earth: Quest for the Orb sci Inside the Chest”// aka Behind the Developer’s Shield ags Interface Show-off director Iron Helix director Isis wintermute J.U.L.I.A. wintermute J.U.L.I.A.: Among the Stars wintermute J.U.L.I.A.: Untold director JUMP: The David Bowie Interactive CD-ROM ags James Bondage wintermute James Peris: No License Nor Control director Jewels of the Oracle ags Jingle Bells magnetic Jinxter sci Jones in the Fast Lane agos Jumble director Just Me &amp; My Dad wintermute K’NOSSOS director Karma: Curse of the 12 Caves ags Kidnapped agi King’s Quest I: Quest for the Crown sci King’s Quest I: Quest for the Crown”// Note: There was also an AGI version of this agi King’s Quest II: Romancing the Throne agi King’s Quest III: To Heir Is Human agi King’s Quest IV: The Perils of Rosella sci King’s Quest IV: The Perils of Rosella”// Note: There was also an AGI version of this sci King’s Quest V: Absence Makes the Heart Go Yonder sci King’s Quest VI: Heir TodayGone Tomorrow sci King’s Quest VII: The Princeless Bride sci King’s Questions kingdom Kingdom: The Far Reaches wintermute Kulivocko director L-ZONE lab Labyrinth of Time director Labyrinthe kyra Lands of Lore: The Throne of Chaos ags Larry Vales II: Dead Girls are Easy ags Larry Vales III: Time Heals All ‘Burns ags Larry Vales: Traffic Division ags Lassi Quest I ags Lassi and Roger ags Lassi and Roger Meet God sci Laura Bow 2: The Dagger of Amon Ra sci Laura Bow: The Colonel’s Bequest made Leather Goddesses of Phobos 2 sci Leisure Suit Larry 2: Goes Looking for Love (in Several Wrong Places) sci Leisure Suit Larry 3: Passionate Patti in Pursuit of the Pulsating Pectorals sci Leisure Suit Larry 5: Passionate Patti Does a Little Undercover Work sci Leisure Suit Larry 6: Shape Up or Slip Out! sci Leisure Suit Larry 6: Shape Up or Slip Out! sci Leisure Suit Larry 7: Love for Sail! agi Leisure Suit Larry in the Land of the Lounge Lizards sci Leisure Suit Larry in the Land of the Lounge Lizards”// Note: There was also an AGI version of this sludge Lepton’s Quest scumm Let’s Explore the Airport with Buzzy scumm Let’s Explore the Farm with Buzzy scumm Let’s Explore the Jungle with Buzzy sludge Life Flashes By wintermute Life In 3 Minutes sci Lighthouse: The Dark Being wintermute Limbo of the Lost director Lion twine Little Big Adventure ags Little Jonny Evil ags Little Willie wintermute Looky scumm Loom avalanche Lord Avalot d’Argent cryo Lost Eden director Louis Cat Orze: The Mystery of the Queen’s Necklace wintermute Lov Mamuta ags Lupo Inutile lure Lure of the Temptress wintermute Machu Mayu director Macromedia Director All Movies Test Target director Macromedia Director Game director Macromedia Director Test Target director Mad Mac Cartoons ags Mafio ngi Magic Dream composer Magic Tales composer Magic Tales: Baba Yaga and the Magic Geese composer Magic Tales: Imo and the King composer Magic Tales: Liam Finds a Story composer Magic Tales: Sleeping Cub’s Test of Courage composer Magic Tales: The Little Samurai composer Magic Tales: The Princess and the Crab magnetic Magnetic Scrolls Game director Majestic Part I: Alien Encounter sludge Mandy Christmas Adventure agi Manhunter 1: New York agi Manhunter 2: San Francisco scumm Maniac Mansion access Martian Memorandum director Masters of the Elements director MechWarrior 2 director Meet Mediaband ags Men In Brown wintermute Mental Repairs Inc agi Mickey's Space Adventure director Microsoft Bookshelf ‘94 director Microsoft Encarta ‘94 director Microsoft Encarta ‘95 xeen Might and Magic IV: Clouds of Xeen xeen Might and Magic V: Darkside of Xeen xeen Might and Magic: Swords of Xeen xeen Might and Magic: World of Xeen director Mirage wintermute Mirage supernova Mission Supernova 1 agi Mixed-Up Mother Goose sci Mixed-Up Mother Goose sci Mixed-Up Mother Goose sci Mixed-up Fairy Tales ags Mom’s Quest wintermute Monday Starts on Saturday scumm Monkey Island 2: LeChuck’s Revenge ags Monkey Plank scumm Moonbase Commander ags Moose Wars: Desire For More Cows mortevielle Mortville Manor ags Mr. Grey’s Greyt Adventure sci Ms. Astro Chicken director Mummy: Tomb of the Pharaoh director Muppet Treasure Island ags Murder wintermute Murder In Tehran’s Alleys 1933 wintermute Murder In Tehran’s Alleys 2016 mutationofjb Mutation of J.B. director Mylk mohawk Myst myst3 Myst III Exile director Mysterious Egypt magnetic Myth wintermute Myth: A Guff’s Tale nancy Nancy Drew: Message in a Haunted Mansion nancy Nancy Drew: Secret of the Scarlet Hand nancy Nancy Drew: Secrets Can Kill nancy Nancy Drew: Stay Tuned for Danger nancy Nancy Drew: The Final Scene nancy Nancy Drew: Treasure in the Royal Tower sludge Nathan’s Second Chance director Necrobius ags Nicholas Wolfe part I: Framed wintermute Night Train ags Night of the Plumber trecision Nightlong: Union City Conspiracy ngi Nikita Game Interface game director Nile: Passage to Egypt director Nine Worlds hosted by Patrick Stewart parallaction Nippon Safes Inc. agos NoPatience director Noir: A Shadowy Thriller ags Novo Mestro wintermute Oknytt wintermute On the Tracks of Dinosaurs wintermute One wintermute One Helluva Day wintermute Open Quest director Opera Fatal cine Operation Stealth director Operation Teddy Bear director Operation: Eco-Nightmare director Operation: Weather Disaster sludge Out Of Order director P.A.W.S.: Personal Automated Wagging System wintermute Paintaria scumm Pajama Sam 1: No Need to Hide When It’s Dark Outside scumm Pajama Sam 2: Thunder and Lightning Aren’t so Frightening scumm Pajama Sam 3: You Are What You Eat From Your Head to Your Feet scumm Pajama Sam’s Lost &amp; Found scumm Pajama Sam’s One-Stop Fun Shop scumm Pajama Sam’s Sock Works scumm Pajama Sam: Games to Play on Any Day wintermute Palladion wintermute Papa’s Daughters wintermute Papa’s Daughters Go to the Sea director Paradise Rescue scumm Passport to Adventure sci Pepper’s Adventure in Time ags Permanent Daylight ags Perpetrator agos Personal Nightmare sci Phantasmagoria sci Phantasmagoria 2: A Puzzle of Flesh director Phantasmagoria Amusement Planet wintermute Pigeons in the Park director Pitfall: The Mayan Adventure wintermute Pizza Morgana: Episode 1 - Monsters and Manipulations in the Magical Forest ags Pizza Quest plumbers Plumbers Don’t Wear Ties! ags Point Blank wintermute Pole Chudes agi Police Quest I: In Pursuit of the Death Angel sci Police Quest II: The Vengeance sci Police Quest III: The Kindred sci Police Quest IV: Open Season sci Police Quest IV: Open Season”// floppy is SCI2CD SCI2.1 sci Police Quest: In Pursuit of the Death Angel”// Note: There was also an AGI version of this sci Police Quest: SWAT ags Porn Quest private Private Eye wintermute Project Joe wintermute Project Lonely Robot wintermute Project: Doom scumm Putt-Putt &amp; Fatty Bear’s Activity Pack scumm Putt-Putt Enters the Race scumm Putt-Putt Goes to the Moon scumm Putt-Putt Joins the Circus scumm Putt-Putt Joins the Parade scumm Putt-Putt Saves the Zoo scumm Putt-Putt Travels Through Time scumm Putt-Putt and Pep’s Balloon-O-Rama scumm Putt-Putt and Pep’s Dog on a Stick scumm Putt-Putt’s Fun Pack scumm Putt-Putt’s One-Stop Fun Shop wintermute Qajary Cat ags Quest For Colours quest Quest Game ags Quest for Glory 4 1/2 sci Quest for Glory I: So You Want to Be a Hero”// Note: There was also a SCI0 version of this (further up) sci Quest for Glory I: So You Want to Be a Hero”// Note: There was also a SCI11 VGA remake of this (further down) sci Quest for Glory II: Trial by Fire sci Quest for Glory III: Wages of War sci Quest for Glory IV: Shadows of Darkness sci Quest for Glory IV: Shadows of Darkness”// floppy is SCI2CD SCI2.1 sci RAMA ags RIPP ags Racing Manager director Ray Bradbury’s The Martian Chronicles Adventure Game wage Ray’s Maze ags Raymond’s Keys wintermute Rebecca Carlson Mystery 01 - Silent Footsteps ags Red wintermute Red Comrades 0.2: Operation F. petka Red Comrades 1: Save the Galaxy petka Red Comrades 2: For the Great Justice petka Red Comrades Demo director Refixion director Refixion II: Museum or Hospital director Refixion III: The Reindeer Story mads Return of the Phantom tsage Return to Ringworld made Return to Zork wintermute Reversion: The Escape wintermute Reversion: The Meeting wintermute Reversion: The Return mads Rex Nebular and the Cosmic Gender Bender wintermute Rhiannon: Curse of the four Branches ags Richard Longhurst and the Box That At ags Ricky Longhurst and the Box that Ate Time tsage Ringworld: Revenge of the Patriarch ags Rob Blanc I: Better Days of a Defender of the Universe ags Rob Blanc II: Planet of the Pasteurised Pestilence ags Rob Blanc III: The Temporal Terrorists sludge Robin’s Rescue director Robotoid Assembly Toolkit ags Rode Kill: A Day In the Life ags Rode Quest director Rodney’s Funscreen made Rodney’s Funscreen ags Rollinfoy lilliput Rome: Pathway to Power wintermute Rosemary scumm SPY Fox 1: Dry Cereal scumm SPY Fox 2: Some Assembly Required scumm SPY Fox 3: Operation Ozone scumm SPY Fox in Cheese Chase scumm SPY Fox in Hold the Mustard director Sakin II scumm Sam &amp; Max Hit the Road ags Sam The Pirate Monkey asylum Sanitarium director Santa Fe Mysteries: The Elk Moon Murder wintermute Satan and Sons director Science Smart director Scientific American Library: Illusion director Scientific American Library: The Universe director Screaming Metal wintermute Securanote agi Serguei’s Destiny 1 agi Serguei’s Destiny 2 cge2 Sfinx wintermute Shaban macventure Shadowgate wintermute Shadows on the Vatican - Act I: Greed wintermute Shadows on the Vatican - Act II: Wrath director Shanghai: Great Moments sci Shivers sci Shivers II: Harvest of Souls”// Not SCI agi Sierra AGI game sci Sierra SCI Game agos Simon the Sorcerer 1 agos Simon the Sorcerer 2 director SkyBorg: Into the Vortex ags Slacker Quest sci Slater &amp; Charlie Go Camping sludge Sludge Game ags Snail Quest ags Snail Quest 2 ags Snail Quest 3 wintermute Sofia’s Debt ags Sol cge Soltys ags Space wintermute Space Invaders wintermute Space Madness agi Space Quest 0: Replicated sci Space Quest 6: The Spinal Frontier agi Space Quest I: The Sarien Encounter sci Space Quest I: The Sarien Encounter”// Note: There was also an AGI version of this agi Space Quest II: Vohaul’s Revenge sci Space Quest III: The Pirates of Pestulon sci Space Quest IV: Roger Wilco and the Time Rippers”// floppy is SCI1CD SCI1.1 sci Space Quest V: The Next Mutation agi Space Quest X: The Lost Chapter director Spaceship Warlock director Spy Club director Spycraft: The Great Game director Star Trek Encyclopedia 1998 director Star Trek Omnipedia startrek Star Trek: 25th Anniversary director Star Trek: Borg director Star Trek: Deep Space Nine Episode Guide startrek Star Trek: Judgment Rites director Star Trek: Klingon director Star Trek: The Next Generation Episode Guide director Star Trek: The Next Generation Interactive Technical Manual titanic Starship Titanic director Stay Tooned! ags Stickmen wintermute Strange Change wintermute Stroke of Fate: Operation Bunker wintermute Stroke of Fate: Operation Valkyrie wintermute Sunrise: The game director SuperSpy 1 ags Superdisk agos Swampy Adventures tads TADS 2 Game ags TV Quest wintermute Tanya Grotter and the Disappearing Floor wintermute Tanya Grotter and the Magical Double Bass teenagent Teen Agent groovie Tender Loving Care testbed Testbed: The Backend Testing Framework groovie The 11th Hour: The Sequel to The 7th Guest ags The 6 Day Assassin groovie The 7th Guest wintermute The Ancient Mark - Episode 1 director The ApartmentInteractive demo sci The Beast Within: A Gabriel Knight Mystery parallaction The Big Red Adventure agi The Black Cauldron wintermute The Box director The C.H.A.O.S. Continuum sherlock The Case of the Rose Tattoo sherlock The Case of the Serrated Scalpel ags The Crown of Gold jacl The Curse of Eldor” // Competition 96 scumm The Curse of Monkey Island director The Daedalus Encounter director The Dark Eye sci The Dating Pool wintermute The Death of Erin Myers scumm The Dig wintermute The Driller Incident agos The Feeble Files sludge The Game Jam Game About GamesSecrets and Stuff sludge The Game That Takes Place on a Cruise Ship wintermute The Golden Calf griffon The Griffon Legend magnetic The Guild of Thieves director The History of the United States for Young People wintermute The Idiot’s Tale ags The Inexperienced Assassin sludge The Interview ags The Island sci The Island of Dr. Brain director The Journeyman Project director The Journeyman Project 2: Buried in Time buried The Journeyman Project 2: Buried in Time pegasus The Journeyman Project: Pegasus Prime wintermute The Kite wintermute The Last Crown - Midnight Horror lastexpress The Last Express kyra The Legend of Kyrandia kyra The Legend of Kyrandia: Malcolm’s Revenge kyra The Legend of Kyrandia: The Hand of Fate stark The Longest Journey wintermute The Lost Crown - A Ghost-Hunting Adventure tsage The Lost Files of Sherlock Holmes (Logo) director The Magic Death made The Manhole neverhood The Neverhood Chronicles pink The Pink Panther: Hokus Pokus Pink pink The Pink Panther: Passport to Peril prince The Prince and the Coward director The Riddle of the Maze ags The Secret of Carver Island scumm The Secret of Monkey Island sludge The Secret of Tremendous Corporation director The Seven Colors: Legend of PSY-S City wintermute The Shine of a Star director The Simpsons: Cartoon Studio director The Simpsons: Cartoon Studio Player ags The Tower wintermute The Trader of Stories ags The Trials of Odysseus Kent director The Ultimate Einstein director The Ultimate Frank Lloyd Wright: America’s Architect nancy The Vampire Diaries ags The Warp wintermute The Way Of Love: Sub Zero wintermute The White Chamber director The X-Files Unrestricted Access ags Thendor ags Tom Mato’s Grand Wing-Ding tony Tony Tough and the Night of Roasted Moths toon Toonstruck sci Torin’s Passage director Total Distortion touche Touche: The Adventures of the Fifth Musketeer comprehend Transylvania director Tri-3D-Trial agi Troll's Tale ags Tullie’s World 1: The Roving of Candale wage Twisted! director Twisty Night #1 director Twisty Night #2 director Twisty Night #3 ultima Ultima I - The First Age of Darkness ultima Ultima IV - Quest of the Avatar ultima Ultima IV - Quest of the Avatar - Enhanced ultima Ultima VI - The False Prophet ultima Ultima VI - The False Prophet - Enhanced ultima Ultima VIII - Pagan groovie Uncle Henry’s Playhouse sludge Verb Coin cryomni3d Versailles 1685 director Victor Vector &amp; Yondo: The Cyberplasm Formula director Victor Vector &amp; Yondo: The Hypnotic Harp director Victor Vector &amp; Yondo: The Last Dinosaur Egg director Victor Vector &amp; Yondo: The Vampire’s Coffin ags VonLudwig voyeur Voyeur wintermute Vsevolod wage WAGE wintermute War agos Waxworks sludge Welcome Example director Who Killed Brett Penance? director Who Killed Sam Rupert? director Who Killed Taylor French? The Case of the Undressed Reporter wintermute Wilma Tetris agi Winnie the Pooh in the Hundred Acre Wood wintermute Wintermute 3D Characters Technology Demo wintermute Wintermute Engine Technology Demo wintermute Wintermute engine game director Wishbone and the Amazing Odyssey magnetic Wonderland ultima Worlds of Ultima: Martian Dreams ultima Worlds of Ultima: Martian Dreams - Enhanced ultima Worlds of Ultima: The Savage Empire ultima Worlds of Ultima: The Savage Empire - Enhanced director Wrath of the Gods director Xanthus agi Xmas Card director Yellow Brick Road director Yellow Brick Road II director Yellow Brick Road III scumm Zak McKracken &amp; Loom scumm Zak McKracken and the Alien Mindbenders wintermute Zbang! The Game director Zeddas: Horror Tour 2 director Zeddas: Servant of Sheol wintermute Zilm: A Game of Reflex zvision Zork Nemesis: The Forbidden Lands director Zork Nemesis: The Forbidden Lands zvision Zork: Grand Inquisitor director iD4 Mission Disk 1 - Alien Supreme Commander director iD4 Mission Disk 10 - Alien Bomber director iD4 Mission Disk 11 - Area 51 director iD4 Mission Disk 2 - Alien Science Officer director iD4 Mission Disk 3 - Warrior Alien director iD4 Mission Disk 4 - Alien Navigator director iD4 Mission Disk 5 - Captain Steve Hiller director iD4 Mission Disk 6 - Dave’s Computer director iD4 Mission Disk 7 - President Whitmore director iD4 Mission Disk 8 - Alien Attack Fighter director iD4 Mission Disk 9 - FA-18 Fighter Jet magnetic the Pawn This method should work in OSX and Linux.Happy Coding!" }, { "title": "Formatting MongoDB 4.4+ Logs", "url": "/blog/2021/05/26/formatting-mongodb-4-dot-4-plus-logs/", "categories": "MongoDB", "tags": "mongodb", "date": "2021-05-26 08:21:00 -0400", "snippet": "MongoDB has always output log entries as plaintext.Starting in MongoDB 4.4, mongod / mongos instances now output all log messages in structured JSON format. This includes log output sent to the fil...", "content": "MongoDB has always output log entries as plaintext.Starting in MongoDB 4.4, mongod / mongos instances now output all log messages in structured JSON format. This includes log output sent to the file, syslog, and stdout (standard out) log destinations, as well as the output of the getLog command.The documentation includes Log Parsing Examples using the jq command line utility, but what if we want to tail the logs and produce a similar result as to what was present prior to the introduction of structured logging?For the following example I’ve used the m MongoDB Version Manager to install MongoDB 4.4.6:m 4.4.6-entmkdir datamongod --dbpath data --logpath data/mongodb.log --forkTailing the log now (tail -n 30 data/mongodb.log) will show the structured log output that is the default in MongoDB 4.4+, however using jq we can reformat (and colourize!!!) the output using one of the following:# Windowstail -f data\\mongodb.log | jq-win64 --compact-output -r -C \".msg |= sub(\\\"\\\\n\\\";\\\"\\\") | .t.\\\"$date\\\"+\\\" \\\"+.c+\\\" [\\\"+.ctx+\\\"] \\\"+.msg, .attr | select(.!=null)# Linux/OSXtail -f data/mongodb.log | jq --compact-output -r -C '.msg |= sub(\"\\n\";\"\") | .t.\"$date\"+\" \"+.c+\" [\"+.ctx+\"] \"+.msg, .attr | select(.!=null)'This makes visually consuming the logs a lot easier! Some log messages, more commonly seen with increased Logging Verbosity may contain escape sequences (ex: \\n and \\t):To render these escape sequences on screen while also tailing the logs in follow (-f) mode try the following:# Linux/OSXstdbuf -o0 tail -f mongod.log | stdbuf -o0 jq -r -C '.msg |= sub(\"\\n\";\"\") | .t.\"$date\"+\" \"+.c+\" [\"+.ctx+\"] \"+.msg, .attr | select(.!=null)' | sed 's/\\\\n/\\n/g; s/\\\\t/\\t/g'Note the stdbuf and sed commands can be installed on OSX via brew install coreutils gnu-sed.I personally find this a lot easier when monitoring a node’s logs while troubleshooting.Let me know if you find this useful :)" }, { "title": "Sanitarium: The ScummVM Asylum Engine Journey Concludes", "url": "/blog/2021/05/19/scummvm-asylum-engine/", "categories": "ScummVM", "tags": "sanitarium, asylum, scummvm", "date": "2021-05-19 06:32:45 -0400", "snippet": "With PR#2982 - ENGINES: Sanitarium engine being recently merged into ScummVM I wanted to take some time to chronicle the original development efforts which began in 2009.At the time I was looking t...", "content": "With PR#2982 - ENGINES: Sanitarium engine being recently merged into ScummVM I wanted to take some time to chronicle the original development efforts which began in 2009.At the time I was looking to work on a new engine in an attempt to improve my knowledge of C++ as well as to “give something back” to the open source community. Some of the engines I was toying with at the time included one’s for Uplink: Hacker Elite, the Dynamix Game Development System as well as Sanitarium.When I first played Dreamforge’s Sanitarium (the Razor 1911 rip at the time …) I was immediately drawn to the darker story and ambiance. After finishing it I ended up buying the full version and replaying it several times. Though the gameplay was linear I found myself continually drawn into the world and the story.I began working on an engine for Sanitarium, but soon found a post on the ScummVM forums by someone trying to do the same thing. I reached out to Alex Fontoura as he had also been reverse engineering the game’s executable (sntrm.exe) and we decided to merge our efforts.We began collaborating on an IDA Pro project to see how far we could get. Filippos Karapetis, a veteran ScummVM developer, also joined us early on and helped correct quite a few bad C++ habits both Alex and I had at the time :PWith Filippos’ help and an old Gamasutra Postmortem article providing some insight as to the inner workings of the game, we were making rapid progress. To expedite our efforts I started digging for more “insider” information, so using the Moby Games Credits page for Sanitarium I started “cold-calling” (emailing) some team members.One team member I was able to get a hold of was Mike Breitkreutz (via MySpace), who shared what he remembered.Benjamin Haisch (aka. John Doe), another ScummVM team member, had by this point provided a far superior IDA database to work from which we used until Julian Templier joined us in 2010 and essentially finished the IDA RE efforts.Julian implemented the event processing subsystem and the save/load functionality along with cleaning up a lot of the codebase. His blog shares some of the progress from July 2011 which is shortly before he left to focus on other work (such as the Ring engine).Our progress stagnate from 2012 through 2015, at which point I published a progress update in hopes of reigniting interest …. it didn’t work.Fast forward to 2021 and we find out that Alexander Panov has picked up the development and completed the engine! The outstanding work left was the inventory management, encounters and a number of bugfixes, which he was able to get sorted out over the course of a few months.Alexander is now a member of the ScummVM team, and thanks to his efforts this 12 year journey can finally conclude with a working engine merged into ScummVM that will allow countless others to enjoy Sanitarium." }, { "title": "Ruby Call Path Analysis using TracePoint", "url": "/blog/2021/05/07/call-path-analysis-using-tracepoint/", "categories": "Programming", "tags": "ruby, mongodb", "date": "2021-05-07 09:56:11 -0400", "snippet": "During a recent diagnostic analysis exercise I needed to identify if there was additional “work” being done based on a single option being changed. As Ruby offers numerous productivity tools for de...", "content": "During a recent diagnostic analysis exercise I needed to identify if there was additional “work” being done based on a single option being changed. As Ruby offers numerous productivity tools for developers it should come as no surprise that a mechanism to easily produce a full call stack for one to many operations exists.The code below is using the MongoDB Ruby Driver and Mongoid ODM to $sample a single document with a Read Preference passed in from the command line. The collection likely won’t exist however the goal of this analysis was simply to see what differences changing the read preference would expose.For the purposes of my analysis I wanted to produce a diff of two call stacks to try and see “where” there may be a difference in the amount of “work” being performed. To do this the first step was to introduce instrumentation via a TracePoint.# test.rb# ruby test.rb [primary|secondary]require 'bundler/inline'gemfile do source 'https://rubygems.org' gem 'mongoid', '7.0.4' gem 'mongo', '2.11.1'endMongoid.configure do |config| config.clients.default = { uri: \"mongodb+srv://...\" }endclass Sample include Mongoid::Documentenddef with_tracepoint trace = [] tp = TracePoint.new(:call) do |x| trace &lt;&lt; \"#{x.defined_class}##{x.method_id.to_s} @ #{x.path}\" end tp.enable yield return traceensure tp.disable unless tp.nil?end# first argument to symbolread_pref = ARGV[0].nil? ? :primary : ARGV[0].to_sym# run first command to establish a connectionSample.collection.aggregate([{ :'$sample' =&gt; { size: 1 } }], read: { mode: read_pref }).firsttrace = with_tracepoint do Sample.collection.aggregate([{ :'$sample' =&gt; { size: 1 } }], read: { mode: read_pref }).firstendputs traceThe above will trace all Ruby method calls executed within the block and push them into an array. By running the above script twice with different options and feeding the results into a diff tool (such as icdiff) a visual representation of how the call stacks differ can be generated.icdiff -N &lt;(ruby test.rb primary) &lt;(ruby test.rb secondary) | less(Open screenshot in a new tab to get a better look)The with_tracepoint helper method in the script above is only filtering on :calls, however can easily be modified to filter based on your particular needs (see TracePoint Events for the full list).Let me know if this approach helped you troubleshoot a particular issue or identify an interesting defect.Happy Coding!" }, { "title": "Same Blog, New Look!", "url": "/blog/2021/03/28/same-blog-new-look/", "categories": "Writing", "tags": "blog, jekyll, octopress", "date": "2021-03-28 12:28:46 -0400", "snippet": "Ever since I started this blog in 2012, I’ve been using Octopress to generate the pages and push to generated content to GitHub Pages for hosting. Octopress is a set of scripts and helpers built on...", "content": "Ever since I started this blog in 2012, I’ve been using Octopress to generate the pages and push to generated content to GitHub Pages for hosting. Octopress is a set of scripts and helpers built on top of Jekyll and provided a lot of convenience in the form of deployment helpers, themes and scripts.Unfortunately, Octopress fell out of maintenance many years ago (2015?) and has been stagnating ever since. As a number of Octopress users have before me, I decided it was time to migrate the site back to vanilla Jekyll. I also wanted to refresh the look and feel as version I had was beginning to feel … dated:I also wanted to continue managing the deployment client-side (as opposed to using automation via GitHub Actions).In the event this helps anyone, here’s the process I followed.MigrationOLD_BLOG=/path/to/octopress/sourcejekyll new blogcd blogbundle installcp -R $OLD_BLOG/_posts/* _postscp -R $OLD_BLOG/images .jekyll serveOnce copied over there will be some build failures when you try to run jekyll serve, such as: Missing plugins in _plugins directory Missing plugins in GemfileInstead of hunting down an alternate plugin, for most of my build failures the issue was resolved by converting Liquid Tags versions of img, blockquote, and others to the pure Markdown versions instead.Look and FeelI chose to use the Chirpy theme for Jekyll as I like the dark look and navigation layout.Following the installation and configuration steps was straightforward, however the one gotcha I hit was with Jekyll Pagination. Because I had both an index.html and index.md file in the blog root, the paginator wouldn’t work until I deleted index.md.DeploymentSince I use GitHub Pages to host my site I wanted to be able to switch back and forth between old and new layouts while testing. I’m quite happy with the Octopress approach of deploying using rake tasks, so I just copied over parts of the existing Rakefile as follows:require \"rubygems\"require \"bundler/setup\"require \"stringex\"posts_dir = \"_posts\" # directory for blog filesnew_post_ext = \"markdown\" # default new post file extension when using the new_post taskdeploy_dir = \"_site\" # deploy directory (for Github pages deployment)deploy_branch = \"master2\"# usage rake new_post[my-new-post] or rake new_post['my new post'] or rake new_post (defaults to \"new-post\")desc \"Begin a new post in /#{posts_dir}\"task :new_post, :title do |t, args| if args.title title = args.title else title = get_stdin(\"Enter a title for your post: \") end filename = \"#{posts_dir}/#{Time.now.strftime('%Y-%m-%d')}-#{title.to_url}.#{new_post_ext}\" if File.exist?(filename) abort(\"rake aborted!\") if ask(\"#{filename} already exists. Do you want to overwrite?\", ['y', 'n']) == 'n' end puts \"Creating new post: #{filename}\" open(filename, 'w') do |post| post.puts \"---\" post.puts \"layout: post\" post.puts \"title: \\\"#{title.gsub(/&amp;/,'&amp;amp;')}\\\"\" post.puts \"date: #{Time.now.strftime('%Y-%m-%d %H:%M:%S %z')}\" post.puts \"comments: true\" post.puts \"categories: \" post.puts \"tags: \" post.puts \"---\" endenddesc \"deploy public directory to github pages\"multitask :push do puts \"## Deploying branch to Github Pages \" puts \"## Pulling any updates from Github Pages \" cd \"#{deploy_dir}\" do Bundler.with_clean_env { system \"git pull\" } end cd \"#{deploy_dir}\" do system \"git add -A\" message = \"Site updated at #{Time.now.utc}\" puts \"\\n## Committing: #{message}\" system \"git commit -m \\\"#{message}\\\"\" puts \"\\n## Pushing generated #{deploy_dir} website\" Bundler.with_clean_env { system \"git push origin #{deploy_branch}\" } puts \"\\n## Github Pages deploy complete\" endenddesc \"Generate jekyll site\"task :generate do puts \"## Generating Site with Jekyll\" # system \"compass compile --css-dir assets/css\" system \"jekyll build\"enddef get_stdin(message) print message STDIN.gets.chompenddef ask(message, valid_options) if valid_options answer = get_stdin(\"#{message} #{valid_options.to_s.gsub(/\"/, '').gsub(/, /,'/')} \") while !valid_options.include?(answer) else answer = get_stdin(message) end answerendSince the _site directory where Jekyll generates content is in the .gitignore, we can initialize a new Git repo here:cd _sitegit init .git remote add git@github.com:alexbevi/alexbevi.github.com.gitOnce this is set up we can then build the site and publish it to a dedicated branch in our GitHub repository (I used master2 in the above example):rake generaterake pushAfter changing the GitHub Pages source branch in our repository in GitHub, the blog was now using the new layout." }, { "title": "Analysis and Optimization of an N+1 Scenario in Mongoid", "url": "/blog/2021/03/26/analysis-and-optimization-of-an-n-plus-1-scenario-in-mongoid/", "categories": "Ruby", "tags": "mongodb, mongoid, ruby", "date": "2021-03-26 09:16:09 -0400", "snippet": "The N + 1 queries problem is a common issue Rails applications face whereby iterating an array of models and accessing an association results in a sub-optimal pattern of recurring queries.To addres...", "content": "The N + 1 queries problem is a common issue Rails applications face whereby iterating an array of models and accessing an association results in a sub-optimal pattern of recurring queries.To address this Rails offers Eager Loading Associations and there are gems (such as bullet) that can be used to detect the N + 1 pattern in an application.This N + 1 problem can manifest in any ODM or ORM, including when working with MongoDB and Ruby.For the purposes of this example we have a Ruby application that is using the Mongoid ODM with a minimal Document model that contains a single Field definition. The scripts to setup the cluster and seed the data are shared at the end of this article (see Reproduction).Our model only defines a single field, but an external process updates the underlying document in the MongoDB cluster with telemetry entries. As no schema is enforced, this is perfectly valid however the documents may grow too large over time for which we’ve written the following Ruby application to periodically clean up:## mongoid_n_plus_1.rb#require 'benchmark'require 'bundler/inline'gemfile do source 'https://rubygems.org' gem 'mongoid' gem 'mongo'endMongoid.configure do |config| config.clients.default = { uri: \"mongodb://localhost:27017,localhost:27018,localhost:27019/test\" }endclass Test include Mongoid::Document field :name, type: String # return all entries from the document that aren't defined as Mongoid fields def entries attr = self.attributes.dup attr.delete_if { |k, v| self.fields.key?(k) } attr.sort end def compact! # unset each field that doesn't belong to the Mongoid document as an # explicitly defined field self.entries.each do |entry| self.unset entry.first end endendUsing a Ruby REPL we can require the above code and verify it’s connecting to our cluster and interacting with our document:&gt; require_relative 'mongoid_n_plus_1' =&gt; true&gt; t = Test.first=&gt; #&lt;Test _id: 1.0, name: \"Alex\"&gt;&gt; t.entries.length=&gt; 10000&gt; t.entries[0..15]=&gt; [[\"2021-03-25T10:30:21\", 1.0], [\"2021-03-25T10:30:22\", 2.0], [\"2021-03-25T10:30:23\", 3.0], [\"2021-03-25T10:30:24\", 4.0], [\"2021-03-25T10:30:25\", 5.0], [\"2021-03-25T10:30:26\", 6.0], [\"2021-03-25T10:30:27\", 7.0], [\"2021-03-25T10:30:28\", 8.0], [\"2021-03-25T10:30:29\", 9.0], [\"2021-03-25T10:30:30\", 10.0], [\"2021-03-25T10:30:31\", 11.0], [\"2021-03-25T10:30:32\", 12.0], [\"2021-03-25T10:30:33\", 13.0], [\"2021-03-25T10:30:34\", 14.0], [\"2021-03-25T10:30:35\", 15.0], [\"2021-03-25T10:30:36\", 16.0]]Next let’s measure our compact! operations:&gt; puts Benchmark.measure { t.compact! } 17.319826 1.025931 18.345757 ( 55.004495)We’re only modifying a single document, after reviewing the FTDC1 it appears there are about 10K update operations issued, which is taking about a minute to completed.After reviewing the documentation for MongoDB’s $unset update operator it appears we can pass more than one field at a time. If this is the case, we can send a single command with a list of 10K fields as opposed to 10K commands with a single field each.To test this we modify our compact! method as follow: def compact! # each field in the `entries` array is a [k,v] array # map the `k` (field) values into a single array and send them all # as a single unset command self.unset self.entries.map(&amp;:first) endAfter testing this the result confirms our theory and the performance is significantly better:&gt; puts Benchmark.measure { t.compact! } 0.150776 0.002509 0.153285 ( 0.273297)Reviewing the FTDC again shows only a single command was issued:Anytime operations are being sent to the server from within a loop there may be an opportunity to group/batch actions in a more efficient manner. The first step to improving performance is being able to identify an opportunity for improvement, which I hope this article helps you do.Happy Coding!ReproductionFirst, we spin up a local replica set using some open source MongoDB helper utilities (see m and mtools):m 4.2.13mlaunch init --replicaset --nodes 3 --binarypath $(m bin 4.2.13) --host localhost --bind_ip_allmongo \"mongodb://localhost:27017,localhost:27018,localhost:27019/test\" test.jsThe JavaScript file (test.js) above is used to seed the collection with a single document and 10,000 telemetry entries:// test.jsdb.tests.drop();db.tests.insert({ _id: 1, name: \"Alex\" });var d = new Date();var update = {};for (var i = 0; i &lt; 10000; i++) { d.setSeconds(d.getSeconds() + 1); update[d.toISOString().split('.')[0]] = i+1;}db.tests.update({ _id: 1 }, { $set: update })From the mongo or mongosh the result of this script can be verified: mongo \"mongodb://localhost:27017,localhost:27018,localhost:27019/test\" --quiet --eval 'db.tests.find().pretty()'{ \"_id\": 1, \"name\": \"Alex\", \"2021-03-25T10:55:23\": 1, \"2021-03-25T10:55:24\": 2, \"2021-03-25T10:55:25\": 3, \"2021-03-25T10:55:26\": 4, ... \"2021-03-25T13:41:59\": 9997, \"2021-03-25T13:42:00\": 9998, \"2021-03-25T13:42:01\": 9999, \"2021-03-25T13:42:02\": 10000}1: As a MongoDB Technical Services engineer I have access to tools we can use to parse the cluster’s FTDC (see “What is MongoDB FTDC (aka. diagnostic.data)” for more info) and visualize time series performance telemetry, which were used to generate the following charts." }, { "title": "Visualizing a Replica Set's Sync Source Chain", "url": "/blog/2021/03/23/visualizing-a-replica-sets-sync-source-chain/", "categories": "MongoDB", "tags": "mongodb, replication, scripting", "date": "2021-03-23 10:47:41 -0400", "snippet": "A MongoDB replica set is a group of mongod processes that maintain the same data set. The PRIMARY node receives all write operations and The SECONDARY nodes replicate the PRIMARY’s oplog and apply ...", "content": "A MongoDB replica set is a group of mongod processes that maintain the same data set. The PRIMARY node receives all write operations and The SECONDARY nodes replicate the PRIMARY’s oplog and apply the operations to their data sets such that the secondaries’ data sets reflect the primary’s data set.Secondaries capture data from the primary member to maintain an up to date copy of the sets’ data unless chained replication is enabled, which changes the replication source selection to allow a secondary member to replicate from another secondary member instead of from the primary.To determine which node each SECONDARY is syncing from you have to manually review the entries generated by the rs.status() shell helper (or replSetGetStatus command) and parse each node’s syncSourceHost.When evaluating larger clusters this approach can be cumbersome.For example, using a 9 node cluster created using mtools and m will produce the following:# launch a 9 node replica set using MongoDB 4.4.4m 4.4.4-entmlaunch init --replicaset --nodes 9 --binarypath $(m bin 4.4.4-ent)&gt; rs.status(){ \"set\" : \"replset\", ... \"members\" : [ { \"_id\" : 0.0, \"name\" : \"localhost:27017\", ... \"syncSourceHost\" : \"\", .... }, { \"_id\" : 1.0, \"name\" : \"localhost:27018\", ... \"syncSourceHost\" : \"localhost:27017\", ... }, { \"_id\" : 2.0, \"name\" : \"localhost:27019\", ... \"syncSourceHost\" : \"localhost:27017\", ... }, { \"_id\" : 3.0, \"name\" : \"localhost:27020\", ... \"syncSourceHost\" : \"localhost:27017\", ... }, { \"_id\" : 4.0, \"name\" : \"localhost:27021\", ... \"syncSourceHost\" : \"localhost:27017\", ... }, { \"_id\" : 5.0, \"name\" : \"localhost:27022\", ... \"syncSourceHost\" : \"localhost:27017\", ... }, { \"_id\" : 6.0, \"name\" : \"localhost:27023\", ... \"syncSourceHost\" : \"localhost:27017\", ... }, { \"_id\" : 7.0, \"name\" : \"localhost:27024\", ... \"syncSourceHost\" : \"localhost:27017\", ... }, { \"_id\" : 8.0, \"name\" : \"localhost:27025\", ... \"syncSourceHost\" : \"localhost:27017\", ... } ], ...}A much more legible version of the above is:&gt; printSyncSourceTree(rs.status());Replication Sync Source Tree============================-- [0] localhost:27017 (PRIMARY)---- [1] localhost:27018 (SECONDARY)---- [2] localhost:27019 (SECONDARY)---- [3] localhost:27020 (SECONDARY)---- [4] localhost:27021 (SECONDARY)---- [5] localhost:27022 (SECONDARY)---- [6] localhost:27023 (SECONDARY)---- [7] localhost:27024 (SECONDARY)---- [8] localhost:27025 (SECONDARY)The tree above was generated using the printSyncSourceTree() helper function (source code at end of post) from the mongo shell.When all nodes are syncing from the PRIMARY it’s not difficult to visualize the sync source topology, however let’s mix this up by manually configuring a SECONDARY’s sync target.function assignSyncSource(sourceId, syncTargetId) { var members = rs.status().members; var source = members.filter(obj =&gt; { return obj._id === sourceId })[0]; var target = members.filter(obj =&gt; { return obj._id === syncTargetId })[0]; var conn = new Mongo(source.name); var result = conn.adminCommand({ replSetSyncFrom: target.name }); printjson(result)}assignSyncSource(3, 1)assignSyncSource(2, 1)assignSyncSource(5, 3)assignSyncSource(4, 3)printReplicationTree(rs.status())The following output is our new replication sync source tree after fiddling with the sync source assignments:Replication Sync Source Tree============================-- [0] localhost:27017 (PRIMARY)---- [1] localhost:27018 (SECONDARY)------ [2] localhost:27019 (SECONDARY)------ [3] localhost:27020 (SECONDARY)-------- [4] localhost:27021 (SECONDARY)-------- [5] localhost:27022 (SECONDARY)---- [6] localhost:27023 (SECONDARY)---- [7] localhost:27024 (SECONDARY)---- [8] localhost:27025 (SECONDARY)Give this a shot and let me know what you think.function printReplicationTree(rs) { print(&quot;\\nReplication Sync Source Tree\\n============================&quot;); function _toConsumableArray(arr) { if (Array.isArray(arr)) { for (var i = 0, arr2 = Array(arr.length); i &lt; arr.length; i++) arr2[i] = arr[i]; return arr2; } else { return Array.from(arr); } } var idMapping = rs.members.reduce((acc, el, i) =&gt; { acc[el.name] = i; return acc; }, {}); var root = undefined; rs.members.forEach(function (el) { // Handle the root element if (el.syncSourceHost === &quot;&quot;) { root = el; return; } // Use our mapping to locate the parent element in our data array var parentEl = rs.members[idMapping[el.syncSourceHost]]; // Add our current el to its parent&#39;s `children` array parentEl.children = [].concat(_toConsumableArray(parentEl.children || []), [el]); }); function pp(node) { return &quot;[&quot; + node._id + &quot;] &quot; + node.name + &quot; (&quot; + node.stateStr + &quot;)&quot; } function walk(tree) { var indent = 1; function innerWalk(tree) { tree.forEach(function (node) { if (indent == 1 &amp;&amp; node.stateStr !== &quot;PRIMARY&quot;) return; print(&#39;--&#39; + Array(indent).join(&#39;--&#39;), pp(node)); if (node.children) { indent++; innerWalk(node.children); } if (tree.indexOf(node) === tree.length - 1) { indent--; } }) } innerWalk(tree); } walk(rs.members);}" }, { "title": "MongoDB 5.0 Initial Sync Progress Monitoring Improvements", "url": "/blog/2020/11/20/mongodb-5-dot-0-startup2-progress-monitoring-improvements/", "categories": "MongoDB", "tags": "mongodb, replication, scripting", "date": "2020-11-20 10:31:26 -0500", "snippet": "&lt;SHAMELESS_PLUG&gt;My previous article about initial sync progress monitoring got some attention, and as I’m a Technical Services Engineer at MongoDB, I got to provide direct feedback during the...", "content": "&lt;SHAMELESS_PLUG&gt;My previous article about initial sync progress monitoring got some attention, and as I’m a Technical Services Engineer at MongoDB, I got to provide direct feedback during the design phase of SERVER-47863: Initial Sync Progress Metrics!.You can be a part of this team and this awesome organization too! Head on over to MongoDB’s careers page to see what’s available, or feel free to ping me on LinkedIn if you have any questions.&lt;/SHAMELESS_PLUG&gt;The goal of this post is to showcase a change that is coming in MongoDB 5.0 that will significantly improve the feedback loop regarding initial sync progress monitoring. With SERVER-47863 being completed, the results of the db.adminCommand({ replSetGetStatus: 1, initialSync: 1 }) now include additional metrics that can be used to help determine how long (approximately) an initial sync will be running for.Though this feature is planned to be backported to versions 4.4.3 and 4.2.12, at the time of writing backports had not been completed.If you want to test this yourself, my setup was as follows:1) Build the server using the install-mongod SCons target. My version of the server was mongod-4.9.0-alpha-596-g4437864 as a result.2) Start up a single node replicaset as follows:mkdir data &amp;&amp; build/install/bin/mongod --dbpath data --bind_ip_all --replSet rs0 --logpath data/mongod.log3) Using the mongo shell, initialize the replica set:mongo --eval 'rs.initiate()'4) Seed the test.data namespace using the following mgeneratejs and mongoimport:curl -s https://gist.githubusercontent.com/alexbevi/955c6675337107e16d637233f865b1e3/raw/0c48178e9c570b7594f207559744f07ecf87ac28/template.json | mgeneratejs -n 1000000 | mongoimport --collection data --numInsertionWorkers 45) Start another mongod and add it to the replica setmkdir data2 &amp;&amp; build/install/bin/mongod --port 27018 --dbpath data2 --bind_ip_all --replSet rs0 --logpath data2/mongod.logmongo --eval 'rs.add(\"localhost:27018\")'The above steps will build the mongod and start up 2 nodes in a replica set with one in a STARTUP2 (initial sync) state.By connecting to the secondary node directly and issuing a replSetGetStatus command we can now review the progress of the copy. Note that this will need to be done while the initial sync is in progress; once the node exits the STARTUP2 state and enters a SECONDARY state, the initialSyncStatus details will be unavailable.For example:db.adminCommand({ replSetGetStatus: 1, initialSync: 1 });{\t\"set\" : \"rs0\",\t...\t\"initialSyncStatus\" : {\t\t\"failedInitialSyncAttempts\" : 0,\t\t\"maxFailedInitialSyncAttempts\" : 10,\t\t\"initialSyncStart\" : ISODate(\"2020-11-20T15:47:09.136Z\"),\t\t\"totalInitialSyncElapsedMillis\" : 14925, &lt;-----------\t\t\"initialSyncAttempts\" : [ ],\t\t\"approxTotalDataSize\" : 311911892, &lt;-----------\t\t\"approxTotalBytesCopied\" : 16811036, &lt;-----------\t\t\"remainingInitialSyncEstimatedMillis\" : 0, &lt;-----------\t\t\"appliedOps\" : 0,\t\t\"initialSyncOplogStart\" : Timestamp(1605887228, 1),\t\t\"totalTimeUnreachableMillis\" : NumberLong(0),\t\t\"databases\" : {\t\t\t\"databasesToClone\" : 1, &lt;-----------\t\t\t\"databasesCloned\" : 2,\t\t\t...\t\t\t},\t\t\t\"test\" : {\t\t\t\t\"collections\" : 1,\t\t\t\t\"clonedCollections\" : 0,\t\t\t\t\"start\" : ISODate(\"2020-11-20T15:47:13.968Z\"),\t\t\t\t\"test.data\" : {\t\t\t\t\t\"documentsToCopy\" : 250000,\t\t\t\t\t\"documentsCopied\" : 13481,\t\t\t\t\t\"indexes\" : 1,\t\t\t\t\t\"fetchedBatches\" : 2,\t\t\t\t\t\"bytesToCopy\" : 311911663, &lt;-----------\t\t\t\t\t\"approxBytesCopied\" : 16810807, &lt;-----------\t\t\t\t\t\"start\" : ISODate(\"2020-11-20T15:47:13.968Z\"),\t\t\t\t\t\"receivedBatches\" : 2\t\t\t\t}\t\t\t}\t\t}\t},\t...}The command output has been truncated to focus in on the new fields added to the initialSyncStatus document. The new metrics details are as follows:totalInitialSyncElapsedMillis Current Time - Start TimeremainingInitialSyncEstimatedMillis (totalInitialSyncElapsedMillis / approxTotalBytesCopied) * (approxTotalDataSize - approxTotalBytesCopied) If (approxBytesCopied == 0), this field will not be shownI’ve opened SERVER-53017 to review this particular entry as the value did not appear to be properly updated throughout the initial sync process. As MongoDB 5.0 is not expected to be GA until mid 2021, this will likely be addressed before then.Update 2021-01-26: SERVER-53017 has been fixed.&lt;collection&gt;.bytesToCopy = collStats.size&lt;collection&gt;.approxBytesCopied = collStats.avgObjSize * documentsCopieddatabasesToClone = length of listDatabases responseapproxTotalDataSize = Sum (dbStats.dataSize for all databases)approxTotalBytesCopied = Sum (&lt;collection&gt;.approxBytesCopied for all collections)What can we do with all this useful information you ask? Well I’ve written the following script that can be used to poll a secondary replica set member in STARTUP2 to provide details regarding the progress of the initial sync. This script will also provide estimated throughput and a calculated ETA based on data transfer rates.The output will appear similar to:# start monitoring the initial sync (default refresh interval is 5 seconds)mongo --host $SECONDARY_HOST --port $SECONDARY_PORT --quiet --eval \"load('measureInitialSyncProgress.js'); measureInitialSyncProgress();\"# to use a custom refresh interval, the value is in milliseconds, so for a 1 second refresh# pass 1000 to measureInitialSyncProgress as follows:# mongo --host $SECONDARY_HOST --port $SECONDARY_PORT --quiet --eval \"load('measureInitialSyncProgress.js'); measureInitialSyncProgress(1000);\"Initial Sync running for 00:18:05.2 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:54.2 to go!Initial Sync running for 00:18:10.8 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:18:17.9 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:18:22.9 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:18:27.9 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:18:33.1 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:18:38.4 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:49.1 to go!Initial Sync running for 00:18:43.5 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:18:48.5 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:18:53.9 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:18:59.5 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:44.2 to go!Initial Sync running for 00:19:05.3 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:39.2 to go!Initial Sync running for 00:19:11.0 (remainingInitialSyncEstimatedMillis 0). Cloned 1.1 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:34.2 to go!Initial Sync running for 00:19:16.6 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:29.2 to go!Initial Sync running for 00:19:22.3 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:19:28.4 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:24.2 to go!Initial Sync running for 00:19:34.5 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:19.2 to go!Initial Sync running for 00:19:39.6 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:14.2 to go!Initial Sync running for 00:19:45.3 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:19:54.6 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:09.2 to go!Initial Sync running for 00:20:01.9 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:20:10.1 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (15.9 MB in past 5 second(s)) at a rate of 3.2 MB/second - 00:00:04.2 to go!Initial Sync running for 00:20:15.7 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (No progress in past 5 second(s))Initial Sync running for 00:20:20.7 (remainingInitialSyncEstimatedMillis 0). Cloned 1.2 GB of 1.2 GB (11.6 MB in past 5 second(s)) at a rate of 2.3 MB/second - 00:00:00.7 to go!Node not currently performing an initial syncOnce SERVER-53017 has been addressed I will update this script so that the remainingInitialSyncEstimatedMillis values can be used consistently.Update 2021-01-26: SERVER-53017 was addressed and the script has been modified to use the remainingInitialSyncEstimatedMillis value directly!/** measureInitialSyncProgress* @author Alex Bevilacqua &lt;alex@alexbevi.com&gt;* @updated 2021-02-23** Can be run against a MongoDB 4.2.12+ mongod that is in STARTUP2 (intitial sync) state to gain some* insight into how the sync is progressing based on the improvements introduced with SERVER-47863.* For versions of MongoDB &lt; 4.2.12, see https://www.alexbevi.com/blog/2020/02/13/mongodb-initial-sync-progress-monitoring** usage:* mongo --quiet --eval &quot;load(&#39;measureInitialSyncProgress.js&#39;); measureInitialSyncProgress();&quot;* // pass a value in milliseconds to measureInitialSyncProgress to override the refresh interval* // default: 5000 (5 seconds)* mongo --quiet --eval &quot;load(&#39;measureInitialSyncProgress.js&#39;); measureInitialSyncProgress(1000);&quot;*/function msToTime(duration) { var milliseconds = parseInt((duration % 1000) / 100), seconds = Math.floor((duration / 1000) % 60), minutes = Math.floor((duration / (1000 * 60)) % 60), hours = Math.floor((duration / (1000 * 60 * 60)) % 24); hours = (hours &lt; 10) ? &quot;0&quot; + hours : hours; minutes = (minutes &lt; 10) ? &quot;0&quot; + minutes : minutes; seconds = (seconds &lt; 10) ? &quot;0&quot; + seconds : seconds; return hours + &quot;:&quot; + minutes + &quot;:&quot; + seconds + &quot;.&quot; + milliseconds;}var getReadableFileSizeString = function (fileSizeInBytes) { var i = -1; var byteUnits = [&#39; kB&#39;, &#39; MB&#39;, &#39; GB&#39;, &#39; TB&#39;, &#39;PB&#39;, &#39;EB&#39;, &#39;ZB&#39;, &#39;YB&#39;]; do { fileSizeInBytes = fileSizeInBytes / 1024; i++; } while (fileSizeInBytes &gt; 1024); return Math.max(fileSizeInBytes, 0.1).toFixed(1) + byteUnits[i];};function measureSyncProgress(previousResults) { var cmd = db.adminCommand({ replSetGetStatus: 1, initialSync: 1 }); if (!cmd.hasOwnProperty(&quot;initialSyncStatus&quot;)) { print(&quot;Node not currently performing an initial sync&quot;); return { exit: true }; } var status = cmd.initialSyncStatus; db.adminCommand({ replSetGetStatus: 1, initialSync: 1 }).initialSyncStatus.approxTotalBytesCopied var now = new Date(); var started = status.initialSyncStart; var approxTotal = status.approxTotalDataSize; var approxCopied = status.approxTotalBytesCopied; var message = &quot;Cloned &quot; + getReadableFileSizeString(approxCopied) + &quot; of &quot; + getReadableFileSizeString(approxTotal) + &quot; in &quot; + msToTime(now - started); if (previousResults !== undefined &amp;&amp; previousResults.hasOwnProperty(&quot;refreshIntervalMS&quot;) !== null &amp;&amp; previousResults.hasOwnProperty(&quot;copied&quot;)) { var refreshInterval = previousResults.refreshIntervalMS / 1000; var intervalProgress = approxCopied - previousResults.copied; if (intervalProgress == 0) { message += &quot; (No progress in past &quot; + refreshInterval + &quot; second(s))&quot;; } else { var rate = intervalProgress / refreshInterval; // We no longer have to calculate remaining ourselves as SERVER-53017 has fixed remainingInitialSyncEstimatedMillis // var remaining = msToTime(((approxTotal - approxCopied) / rate) * 1000); message += &quot; (&quot; + getReadableFileSizeString(intervalProgress) + &quot; in past &quot; + refreshInterval; message += &quot; second(s)) at a rate of &quot; + getReadableFileSizeString(rate) + &quot;/second - &quot;; message += msToTime(status.remainingInitialSyncEstimatedMillis) + &quot; to go!&quot;; } } print(message); return { copied: approxCopied };}function measureInitialSyncProgress(refreshIntervalMS) { var dbver = db.version().split(&#39;-&#39;)[0]; if (dbver &lt; &quot;4.2.12&quot;) { print(&quot;MongoDB 4.2.12 or greater required but found MongoDB &quot; + dbver); print(&quot;For previous versions see https://www.alexbevi.com/blog/2020/02/13/mongodb-initial-sync-progress-monitoring/&quot;); return; } if (refreshIntervalMS === undefined || refreshIntervalMS === null) { refreshIntervalMS = 5000; } var result = measureSyncProgress(result); result[&quot;refreshIntervalMS&quot;] = refreshIntervalMS; while (!result.exit) { sleep(refreshIntervalMS); result = measureSyncProgress(result); result[&quot;refreshIntervalMS&quot;] = refreshIntervalMS; }}" }, { "title": "MongoDB 4.4 Change Streams and Experimental Pre-Image 'Support'", "url": "/blog/2020/07/15/mongodb-4-dot-4-change-streams-and-pre-image-support/", "categories": "MongoDB", "tags": "mongodb, changestreams", "date": "2020-07-15 08:37:08 -0400", "snippet": " As of MongoDB 6.0, Change Streams Support Pre- and Post-Image Retrieval, DDL operations, and more! Warning - Not for Production Use (in MongoDB 4.4) MongoDB’s source code is available (through ...", "content": " As of MongoDB 6.0, Change Streams Support Pre- and Post-Image Retrieval, DDL operations, and more! Warning - Not for Production Use (in MongoDB 4.4) MongoDB’s source code is available (through an SSPL license) and the Core Server project in MongoDB’s JIRA is publicly accessible, which is where I found this information. Until officially announced as stable/official the methods described herein should not be considered as “production ready”. This post is for informational purposes and though at the time of writing it I am a MongoDB Inc. employee this should not be considered an official communication.IntroductionA feature that has long been requested since Change Streams were introduced in MongoDB 3.6 is the ability to support returning the n-1 state (or pre-image) of a document when it is changed.In SERVER-36941: Option to provide “before image” with change streams this request is captured, but at the time of writing this ticket is still in an Open state which would imply no progress has been made.Interestingly enough, there actually has been some progress due to efforts surrounding the support of Realm Sync, namely in the following tickets: SERVER-45806: Record pre-images on updates and deletes when recordPreImage is enabled SERVER-45807: Add change stream stage to fetch pre-image for update/replace/delete eventsPre-Image Support“Support” for this feature is only available in MongoDB 4.4+, so first we must ensure we are running a compatible version.Next, as change streams are only supported in replica sets or sharded clusters our cluster cannot be a standalone instance.Before a pre-image can be returned in a change stream support for the feature must be enabled at the collection level.Create a New Collection with Pre-Image SupportTo enable pre-image support on a new (non-existent) collection, the recordPreImages flag needs to be set when calling the create command or db.createCollection() shell method:db.coll1.drop();// create commanddb.runCommand({ create: \"coll1\", recordPreImages: true });// or// using the createCollection() helperdb.createCollection(\"coll1\", { recordPreImages: true });Updating an Existing Collection with Pre-Image SupportIf the collection already exists, the recordPreImages flag can be set using the collMod command:db.coll1.drop();db.coll1.insert({ _id: 1, created_at: new Date() });// update the collection's metadata using the collMod commanddb.runCommand({ collMod: \"coll1\", recordPreImages: true });Pre-Image Support in Change StreamsFirst it’s important to understand that a change stream is actually an Aggregation Pipeline Stage, even if it doesn’t appear as such in the documentation.This can be easily verified by checking the source code for the Mongo.prototype.watch implementation which coincides with db.collection.watch() shell method.We’ll be using the $changeStream pipeline stage directly to review the impact of pre-image support.First we’ll being by opening a change stream cursor against our modified collection:db.coll1.drop();db.createCollection(\"coll1\", { recordPreImages: true });db.coll1.insert({ _id: 1, created_at: new Date() });var cursor = db.coll1.aggregate([{ $changeStream: { fullDocumentBeforeChange: \"whenAvailable\" } }]);Note the options include a fullDocumentBeforeChange field, which can accept one of three (3) fullDocumentBeforeChange mode values: off: Disables support for the fullDocumentBeforeChange field whenAvailable: Only includes a fullDocumentBeforeChange document if it’s available, but won’t fail if it’s not present required: Require the fullDocumentBeforeChange document, and errors out if it’s not availableSince we have a change stream cursor open, we can update our test document and iterate the cursor to see the change this option produces:db.coll1.update({ _id: 1 }, { $set: { updated_at: new Date() } })cursor.next()/*{\t\"operationType\" : \"update\",\t\"fullDocumentBeforeChange\" : {\t\t\"_id\" : 1,\t\t\"created_at\" : ISODate(\"2020-07-15T17:41:32.043Z\")\t},\t\"ns\" : {\t\t\"db\" : \"test\",\t\t\"coll\" : \"coll1\"\t},\t\"documentKey\" : {\t\t\"_id\" : 1\t},\t\"updateDescription\" : {\t\t\"updatedFields\" : {\t\t\t\"updated_at\" : ISODate(\"2020-07-15T17:41:32.056Z\")\t\t},\t\t\"removedFields\" : [ ]\t}}*/The output contains a fullDocumentBeforeChange field which includes the full document prior to the changes the update operation would apply. As this was the first update to this document and the field was being added for the first time the value here may not be apparent, but running the operation again produces result that contains our previous created_at value along with our updated created_at value in the updateDescription:db.coll1.update({ _id: 1 }, { $set: { updated_at: new Date() } })cursor.next()/*{\t\"operationType\" : \"update\",\t\"fullDocumentBeforeChange\" : {\t\t\"_id\" : 1,\t\t\"created_at\" : ISODate(\"2020-07-15T17:41:32.043Z\"),\t\t\"updated_at\" : ISODate(\"2020-07-15T17:41:32.056Z\")\t},\t\"ns\" : {\t\t\"db\" : \"test\",\t\t\"coll\" : \"coll1\"\t},\t\"documentKey\" : {\t\t\"_id\" : 1\t},\t\"updateDescription\" : {\t\t\"updatedFields\" : {\t\t\t\"updated_at\" : ISODate(\"2020-07-15T17:44:29.494Z\")\t\t},\t\t\"removedFields\" : [ ]\t}}*/If the fullDocumentBeforeChange mode was set to required and the collection wasn’t created with the recordPreImages flag set, the change stream cursor will error out when iterated.db.coll2.drop();db.createCollection(\"coll2\");db.coll2.insert({ _id: 2, created_at: new Date() });var cursor = db.coll2.aggregate([{ $changeStream: { fullDocumentBeforeChange: \"required\" } }]);db.coll2.update({ _id: 2 }, { $set: { updated_at: new Date() } })cursor.next()/*2020-07-16T07:03:37.657-0400 E QUERY [js] Error: command failed: {\t\"ok\" : 0,\t\"errmsg\" : \"Change stream was configured to require a pre-image for all update, delete and replace events, but no pre-image optime was recorded for event: {_id: {_data: \\\"825F103409000000042B022C0100296E5A10044912D5BB665545B48DDAD38FCD774270461E5F6964002B040004\\\", _typeBits: BinData(0, \\\"40\\\")}, operationType: \\\"update\\\", clusterTime: Timestamp(1594897417, 4), ns: {db: \\\"test\\\", coll: \\\"coll2\\\"}, documentKey: {_id: 2}, updateDescription: {updatedFields: {updated_at: 2020-07-16T11:03:37.611Z}, removedFields: []}}\",\t\"code\" : 51770,}*/Pre-Image and { fullDocument: \"updateLookup\" }The change stream cursors we’ve been opening have not been specifying a fullDocument options, which results in the default value of { fullDocument: \"default\" } being used (or prior to SPEC-909 a value of none).When we set { fullDocument: \"updateLookup\" }, the cursor will look up the most current majority-committed version of the updated document and include a fullDocument field with the document lookup in addition to the updateDescription delta.db.coll3.drop();db.createCollection(\"coll3\", { recordPreImages: true });db.coll3.insert({ _id: 3, name: \"Alex\", role: \"TSE\", created_at: new Date() });var cursor = db.coll3.aggregate([{ $changeStream: { fullDocument: \"updateLookup\", fullDocumentBeforeChange: \"whenAvailable\" } }]);db.coll3.update({ _id: 3 }, { $set: { updated_at: new Date() } });cursor.next()/*{ \"_id\" : { \"_data\" : \"825F103644000000042B022C0100296E5A100452A662230D6B4C08B0AF844900F3A335461E5F6964002B060004\", \"_typeBits\" : BinData(0, \"QA==\") }, \"operationType\" : \"update\", \"clusterTime\" : Timestamp(1594897988, 4), \"fullDocument\" : { \"_id\" : 3.0, \"name\" : \"Alex\", \"role\" : \"TSE\", \"created_at\" : ISODate(\"2020-07-16T11:13:08.180+0000\"), \"updated_at\" : ISODate(\"2020-07-16T11:13:08.268+0000\") }, \"fullDocumentBeforeChange\" : { \"_id\" : 3.0, \"name\" : \"Alex\", \"role\" : \"TSE\", \"created_at\" : ISODate(\"2020-07-16T11:13:08.180+0000\") }, \"ns\" : { \"db\" : \"test\", \"coll\" : \"coll3\" }, \"documentKey\" : { \"_id\" : 3.0 }, \"updateDescription\" : { \"updatedFields\" : { \"updated_at\" : ISODate(\"2020-07-16T11:13:08.268+0000\") }, \"removedFields\" : [ ] }}*/By including both fullDocument and fullDocumentBeforeChange options, the cursor now returns three fields that represent the state of the document before the change, after the change as well as a description of the changes in the form of the updateDescription field.How are Pre-Images StoredSince change streams are messages in the oplog, which are themselves BSON documents they must adhere to the BSON Document Size limit of 16 megabytes.Pre-image details aren’t stored directly within the oplog entry that describes the update or delete (op type of u or d).When recordPreImages has been enabled for a collection, prior to an oplog entry for an update or delete, a noop (op: \"n\") is inserted into the oplog with current document state. The subsequent operation (the actual update or delete) will contain a preImageOpTime which contains the optime of the oplog entry containing this noop.db.getSiblingDB(\"local\").getCollection(\"oplog.rs\").find({ ns: \"test.coll3\" });/*{ \"op\": \"n\", \"ns\": \"test.coll3\", \"ui\": UUID(\"d2c8288d-2748-4abf-b5f0-37de24e3128e\"), \"o\": { \"_id\": 3, \"name\": \"Alex\", \"role\": \"TSE\", \"created_at\": ISODate(\"2020-07-17T13:09:52.612Z\") }, \"ts\": Timestamp(1594991392, 4), \"t\": NumberLong(4), \"wall\": ISODate(\"2020-07-17T13:09:52.649Z\"), \"v\": NumberLong(2)}{ \"op\": \"u\", \"ns\": \"test.coll3\", \"ui\": UUID(\"d2c8288d-2748-4abf-b5f0-37de24e3128e\"), \"o\": { \"$v\": 1, \"$set\": { \"updated_at\": ISODate(\"2020-07-17T13:09:52.648Z\") } }, \"o2\": { \"_id\": 3 }, \"preImageOpTime\": { \"ts\": Timestamp(1594991392, 4), \"t\": NumberLong(4) }, \"ts\": Timestamp(1594991392, 5), \"t\": NumberLong(4), \"wall\": ISODate(\"2020-07-17T13:09:52.649Z\"), \"v\": NumberLong(2)}*/What Could Go Wrong?Though enabling pre-images won’t bloat the oplog entry to the point where it may exceed the BSON max size, when the change stream cursor resolves resulting document the size cannot exceed the BSON max size.For example:function randomString(length) { var result = ''; var characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'; var charactersLength = characters.length; for ( var i = 0; i &lt; length; i++ ) { result += characters.charAt(Math.floor(Math.random() * charactersLength)); } return result;}// BSON max size less 512 bytesvar size = db.serverBuildInfo().maxBsonObjectSize - 512;db.coll4.drop();db.createCollection(\"coll4\", { recordPreImages: true });db.coll4.insert({ _id: 4, created_at: new Date(), junk: randomString(size) });var cursor = db.coll4.aggregate([{ $changeStream: { fullDocument: \"updateLookup\", fullDocumentBeforeChange: \"off\" } }]);db.coll4.update({ _id: 4 }, { $set: { updated_at: new Date() } });cursor.next()The above example has the fullDocumentBeforeChange disabled, which is the current default behavior of change streams. Though the result would be large (almost 16MB), iterating the change stream cursor would produce a result.Modifying the cursor to now request a fullDocumentBeforeChange (either required or whenAvailable) would now raise an error when the cursor is iterated.// ...db.coll4.drop();db.createCollection(\"coll4\", { recordPreImages: true });db.coll4.insert({ _id: 4, created_at: new Date(), junk: randomString(size) });var cursor = db.coll4.aggregate([{ $changeStream: { fullDocument: \"updateLookup\", fullDocumentBeforeChange: \"required\" } }]);db.coll4.update({ _id: 4 }, { $set: { updated_at: new Date() } });cursor.next();/*2020-07-16T07:40:38.307-0400 E QUERY [js] Error: command failed: {\t\"errmsg\" : \"BSONObj size: 33553900 (0x1FFFDEC) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: { _data: \\\"825F103CB5000000032B022C0100296E5A100484010E04E239427B8CBCA810283D05A5461E5F6964002B080004\\\", _typeBits: BinData(0, 40) }\",\t\"code\" : 10334,\t\"codeName\" : \"BSONObjectTooLarge\",}*/I personally look forward to pre-images being officially supported in MongoDB, however in the meantime be advised that this may not be ideal for production just yet.Happy Coding!" }, { "title": "Optimizing MongoDB Compound Indexes - The \"Equality - Sort - Range\" (ESR) Rule", "url": "/blog/2020/05/16/optimizing-mongodb-compound-indexes-the-equality-sort-range-esr-rule/", "categories": "MongoDB, Queries & Indexing", "tags": "mongodb", "date": "2020-05-16 07:35:11 -0400", "snippet": " UPDATE DOCS-11790 has finally been implemented and as a result the MongoDB public documentation now contains a tutorial for The ESR (Equality, Sort, Range) Rule!Working in Technical Services at ...", "content": " UPDATE DOCS-11790 has finally been implemented and as a result the MongoDB public documentation now contains a tutorial for The ESR (Equality, Sort, Range) Rule!Working in Technical Services at MongoDB I find that time and again customers need assistance understanding why the operations they’ve created indexes for may not be performing optimally. When providing supplementary documentation, the go-to article is “Optimizing MongoDB Compound Indexes” by MongoDB’s A. Jesse Jiryu Davis.I’ve presented this topic now at MongoDB.local Toronto 2019 (in “Tips and Tricks for Effective Indexing”) and at MongoDB World 2019 (in “The Sights (and Smells) of a Bad Query”). My colleague Chris Harris has also covered this topic at MongoDB World 2019 (in “Tips and Tricks++ for Querying and Indexing MongoDB”) and again at the MongoDB.local Houston 2019, for which a video is available.Though we have Jesse’s excellent (and still applicable and valid) article from 2012, I wanted to take this opportunity to collect some thoughts on this topic based on his work and previous presentations.The ESR “Rule”The ordering of index keys in a compound index is critically important, and the ESR “Rule” can be used as a rule of thumb to identify the optimal order in most cases.The reason we are putting “Rule” in quotations is because, though the guidance is applicable in most cases, there are exceptions to be aware of. These exceptions are covered in greater detail in my in “Tips and Tricks for Effective Indexing” presentation.The “Rules”(1) Equality predicates should be placed firstAn equality predicate is any filter condition that is attempting to match a value exactly. For example:find({ x: 123 })find({ x: { $eq: 123 } })aggregate([ { $match:{ \"x.y\": 123 } } ])These filters will be tightly bound when seen in the indexBounds of an Explain Plan:\"indexBounds\" : { \"x\" : [ \"[123.0, 123.0]\" ]}Note that multiple equality predicates do not have to be ordered from most selective to least selective. This guidance has been provided in the past however it is erroneous due to the nature of B-Tree indexes and how in leaf pages, a B-Tree will store combinations of all field’s values. As such, there is exactly the same number of combinations regardless of key order.(2) Sort predicates follow Equality predicatesSort predicates represent the entire requested sort for the operation and determine the ordering of results. For example:find().sort({ a: 1 })find().sort({ b: -1, a: 1 })aggregate([ { $sort: { b: 1 } } ])A sort predicate will be unbounded as it requires the entire key range to be scanned to satisfy the sort requirements:\"indexBounds\" : { \"b\" : [ \"[MaxKey, MinKey]\" ], \"a\" : [ \"[MinKey, MaxKey]\" ]}(3) Range predicates follow Equality and Sort predicatesRange predicates are filters that may scan multiple keys as they are not testing for an exact match. For example:find({ z: { $gte: 5} })find({ z: { $lt: 10 } })find({ z: { $ne: null } })The range predicates will be loosely bounded as a subset of the key range will need to be scanned to satisfy the filter requirements:\"indexBounds\" : { \"z\" : [ \"[5.0, inf.0]\" ]}\"indexBounds\" : { \"z\" : [ \"[-inf.0, 10.0)\" ]}\"indexBounds\" : { \"z\" : [ \"[MinKey, undefined)\", \"(null, MaxKey]\" ]}These three tenets of the “rule” have to do with how a query will traverse an index to identify the keys that match the query’s filter and sort criteria.SetupFor the duration of this section we’ll be working with the following data to help illustrate the various guiding principles:{ name: \"Shakir\", location: \"Ottawa\", region: \"AMER\", joined: 2015 }{ name: \"Chris\", location: \"Austin\", region: \"AMER\", joined: 2016 }{ name: \"III\", location: \"Sydney\", region: \"APAC\", joined: 2016 }{ name: \"Miguel\", location: \"Barcelona\", region: \"EMEA\", joined: 2017 }{ name: \"Alex\", location: \"Toronto\", region: \"AMER\", joined: 2018 }We will also be examining simplified (filtered) Explain Plan’s executionStats from each operation using a variation of the following command:find({ ... }).sort({ ... }).explain(\"executionStats\").executionStats(E) Equality FirstWhen creating queries that ensure selectivity, we learn that “selectivity” is the ability of a query to narrow results using the index. Effective indexes are more selective and allow MongoDB to use the index for a larger portion of the work associated with fulfilling the query.Equality fields should always form the prefix for the index to ensure selectivity.(E → S) Equality before SortPlacing Sort predicates after sequential Equality keys allow for the index to: Provide a non-blocking sort. Minimize the amount of scanning required.To better understand why this is we will begin with the following example:// operationcreateIndex({ name: 1, region: 1 })find({ region: \"AMER\" }).sort({ name: 1 })With the Sort predicate first, the full key range would have to be scanned prior to the more selective equality filter being applied:// execution stats\"nReturned\" : 3.0,\"totalKeysExamined\" : 5.0,\"totalDocsExamined\" : 5.0,\"executionStages\" : { ... \"inputStage\" : { \"stage\" : \"IXSCAN\", ... \"indexBounds\" : { \"name\" : [ \"[MinKey, MaxKey]\" ], \"region\" : [ \"[MinKey, MaxKey]\" ] },With this index, all 5 keys have to be scanned (totalKeysExamined) to identify the 3 matching documents (nReturned).// operationcreateIndex({ region: 1, name: 1 })find({ region: \"AMER\" }).sort({ name: 1 })With the Equality predict first, the tight bounds allow less keys to be scanned to satisfy the filter criteria:// execution stats\"nReturned\" : 3.0,\"totalKeysExamined\" : 3.0,\"totalDocsExamined\" : 3.0,\"executionStages\" : { ... \"inputStage\" : { \"stage\" : \"IXSCAN\", ... \"indexBounds\" : { \"region\" : [ \"[\\\"AMER\\\", \\\"AMER\\\"]\" ], \"name\" : [ \"[MinKey, MaxKey]\" ] },(E → R) Equality before RangeThough Range predicates scan a subset of keys (unlike Sort predicates), they should still be placed after Equality predicates to ensure the key ordering is optimized for selectivity.// operationcreateIndex({ joined: 1, region: 1 })find({ region: \"AMER\", joined: { $gt: 2015 } })Having the Range before the Equality predicate causes more keys to be scanned to identify the matching documents:// execution stats\"nReturned\" : 2.0,\"totalKeysExamined\" : 4.0,\"totalDocsExamined\" : 2.0,\"executionStages\" : { ... \"inputStage\" : { \"stage\" : \"IXSCAN\", ... \"indexBounds\" : { \"joined\" : [ \"(2015.0, inf.0]\" ], \"region\" : [ \"[\\\"AMER\\\", \\\"AMER\\\"]\" ] },In this example, 4 keys had to be scanned to identify the 2 matches. Changing the order of the keys to place the Equality predicate first will reduce the amount of scanning required:// operationcreateIndex({ region: 1, joined: 1 })find({ region: \"AMER\", joined: { $gt: 2015 } })// execution stats\"nReturned\" : 2.0,\"totalKeysExamined\" : 2.0,\"totalDocsExamined\" : 2.0,\"executionStages\" : { ... \"inputStage\" : { \"stage\" : \"IXSCAN\", ... \"indexBounds\" : { \"region\" : [ \"[\\\"AMER\\\", \\\"AMER\\\"]\" ], \"joined\" : [ \"(2015.0, inf.0]\" ] },After placing the Equality predicate before the Range predicate, only the number of keys necessary to satisfy the filter criteria are scanned.(S → R) Sort before RangeHaving a Range predicate before the Sort can result in a Blocking (In Memory) Sort being performed as the index cannot be used to satisfy the sort criteria.// operationcreateIndex({ joined: 1, region: 1 })find({ joined: { $gt: 2015 } }).sort({ region: 1 })// execution stats\"nReturned\" : 4.0,\"totalKeysExamined\" : 4.0,\"totalDocsExamined\" : 4.0,\"executionStages\" : { ... \"inputStage\" : { \"stage\" : \"SORT\", ... \"sortPattern\" : { \"region\" : 1.0 }, \"memUsage\" : 136.0, \"memLimit\" : 33554432.0, \"inputStage\" : { \"stage\" : \"SORT_KEY_GENERATOR\", ... \"inputStage\" : { \"stage\" : \"IXSCAN\", ... \"indexBounds\" : { \"joined\" : [ \"(2015.0, inf.0]\" ], \"region\" : [ \"[MinKey, MaxKey]\" ] },In this example, the filter was able to use the index selectively to identify the 4 keys needed to satisfy the query, however the results are not known to be in order. This results in the identified keys being sorted in memory prior to be returned to the calling stage in the execution plan.By moving the Sort predicate before the Range predicate however, even though more keys may need to be scanned the keys will be returned correctly ordered.// operationcreateIndex({ region: 1, joined: 1 })find({ joined: { $gt: 2015 } }).sort({ region: 1 })// execution stats\"nReturned\" : 4.0,\"totalKeysExamined\" : 5.0,\"totalDocsExamined\" : 5.0,\"executionStages\" : { ... \"inputStage\" : { \"stage\" : \"IXSCAN\", ... \"indexBounds\" : { \"region\" : [ \"[MinKey, MaxKey]\" ], \"joined\" : [ \"[MinKey, MaxKey]\" ] },Though this method requires scanning additional keys the lack of a blocking sort will generally be far more efficient/performant.I hope that the ESR “Rule” helps you optimize your MongoDB indexes and improve your query performance. If you have questions, feel free to hit me up in the comments, or check out the MongoDB Developer Community forums.If you need more timely assistance, consider MongoDB’s Atlas Developer Support or Enterprise Support.Cheers, and happy optimizing!" }, { "title": "Working around MongoDB Stitch's \"max async work queue\" limit", "url": "/blog/2020/03/30/working-around-mongodb-stitchs-max-async-work-queue-limit/", "categories": "MongoDB", "tags": "mongodb, realm", "date": "2020-03-30 05:19:32 -0400", "snippet": "MongoDB Stitch is a great way to build apps quickly with your data that’s already managed by MongoDB Atlas. Though these services empower you to focus on development without having to worry about i...", "content": "MongoDB Stitch is a great way to build apps quickly with your data that’s already managed by MongoDB Atlas. Though these services empower you to focus on development without having to worry about infrastructure, being a managed service there are occasionally limitations imposed by the vendor.This article summarizes why this limit exists, as well as how to adapt your MongoDB Stitch Functions to work around it.The following is an HTTP Service I’ve written that has an incoming webhook. When this webhook is called a MongoDB Stitch Function is run which inserts a number of documents. The number to insert is defined by the maxItems query parameter of the request payload provided to the incoming webhook.NOTE When doing a number of insertOne operations in a loop an insertMany would likely address the issue directly without requiring any additional workarounds. The following code is really best suited to a number of update or delete operations that have unique filters and cannot be logically grouped.// MongoDB Stitch Function code for the Incoming Webhookexports = function (payload, response) { let maxItems = parseInt(payload.query.maxItems); const CLUSTER = 'mongodb-atlas'; const DB = 'test'; const COLLECTION = 'web_worker_queue_failures'; const collection = context.services.get(CLUSTER).db(DB).collection(COLLECTION); let items = []; for(let i = 0; i &lt; maxItems; i++) { items.push({ a: i }); } let results = []; items.forEach((item) =&gt; { collection.insertOne(item).then(res =&gt; { results.push(res); }, error =&gt; { results.push({ error: error }); console.log(error); }); }); return { \"Processed\": items.length };};When the webhook is executed, the number of items processed is returned. In the following example we’ll specify that we want 900 items to be inserted:curl -w \"\\nTotal Time: %{time_total}s\\n\" \\ -H \"Content-Type: application/json\" -d '{}' \\ https://webhooks.mongodb-stitch.com/api/client/v2.0/app/cluster0-app0-abcde/service/WebWorkerFailureTest/incoming_webhook/webhook0?maxItems=900{\"Processed\":{\"$numberInt\":\"900\"}}Total Time: 1.729469sBased on the output returned from the webhook, 900 items were inserted. Next we’ll try with 9000 items:curl -w \"\\nTotal Time: %{time_total}s\\n\" \\ -H \"Content-Type: application/json\" -d '{}' \\ https://webhooks.mongodb-stitch.com/api/client/v2.0/app/cluster0-app0-abcde/service/WebWorkerFailureTest/incoming_webhook/webhook0?maxItems=9000{\"error\":\"exceeded max async work queue size of 1000\",\"error_code\":\"FunctionExecutionError\",\"link\":\"https://stitch.mongodb.com/groups/13c415400000000000000000/apps/13c415400000000000000000/logs?co_id=13c415400000000000000000\"}Total Time: 0.371383sFollowing the \"link\" would redirect you to the Application Log for the application that the webhook belongs to. This can be useful for debugging.The reason this error is thrown has to do with how the MongoDB Stitch platform handles async request execution within functions using an internal work queue. Operations such as insertOne return a Promise. To ensure these promises don’t queue infinitely waiting to be resolved, MongoDB Stitch will limit the number that can be enqueued, and if this limit is exceeded queuing stops and the exception is raised.To work around this limit we will adapt our earlier code to instead throttle our work loop to ensure batches of 1000 or less are processed before more work is attempted.const processWork = async function(items) { const CLUSTER = 'mongodb-atlas'; const DB = 'test'; const COLLECTION = 'web_worker_queue_failures'; const collection = context.services.get(CLUSTER).db(DB).collection(COLLECTION); const BATCH_SIZE = 1000; const totalItems = items.length; for (let i = 0; i &lt; totalItems; i += BATCH_SIZE) { const requests = items.slice(i, i + BATCH_SIZE).map(function(item) { return collection.insertOne(item).catch(e =&gt; console.log(e)); }); await Promise.all(requests).catch(e =&gt; console.log(`Errors in batch ${i}: ${e}`)); }}// MongoDB Stitch Function code for the Incoming Webhookexports = function (payload, response) { let maxItems = parseInt(payload.query.maxItems); let items = []; for(let i = 0; i &lt; maxItems; i++) { items.push({ a: i }); } processWork(items); return { \"Processed\": items.length };};The number of items to process (based on maxItems again) will now be broken up into batches (of BATCH_SIZE size). Following this, Promise.all will execute all the operations in a batch and ensure they are all fulfilled before another batch is processed.This method allows the workload to be artificially throttled to allow maxItems operations to be executed. Let’s try running our webhook again for 9000 items:curl -w \"\\nTotal Time: %{time_total}s\\n\" \\ -H \"Content-Type: application/json\" -d '{}' \\ https://webhooks.mongodb-stitch.com/api/client/v2.0/app/cluster0-app0-abcde/service/WebWorkerFailureTest/incoming_webhook/webhook0?maxItems=9000{\"Processed\":{\"$numberInt\":\"9000\"}}Total Time: 13.935162sNote that although this strategy will work with an array of items (maxItems) of any size, MongoDB Stitch Functions still have runtime limit of 90 seconds (see “Constraints”) which cannot be circumvented. If we try running the function for 90000 items, if the function runs for &gt; 90 seconds execution will be terminated:curl -w \"\\nTotal Time: %{time_total}s\\n\" \\ -H \"Content-Type: application/json\" -d '{}' \\ https://webhooks.mongodb-stitch.com/api/client/v2.0/app/cluster0-app0-abcde/service/WebWorkerFailureTest/incoming_webhook/webhook0?maxItems=90000{\"error\":\"execution time limit exceeded\",\"error_code\":\"ExecutionTimeLimitExceeded\",\"link\":\"https://stitch.mongodb.com/groups/13c415400000000000000000/apps/13c415400000000000000000/logs?co_id=13c415400000000000000000\"}Total Time: 90.311827sHappy Coding!" }, { "title": "Identifying and Reclaiming Disk Space in MongoDB", "url": "/blog/2020/03/15/identifying-and-reclaiming-disk-space-in-mongodb/", "categories": "MongoDB", "tags": "mongodb, wiredtiger, scripting", "date": "2020-03-15 16:23:38 -0400", "snippet": "A common question when it comes to MongoDB and the (default) storage engine (WiredTiger) is “Why is it after I removed a bunch of documents my free space didn’t increase”?The WiredTiger storage eng...", "content": "A common question when it comes to MongoDB and the (default) storage engine (WiredTiger) is “Why is it after I removed a bunch of documents my free space didn’t increase”?The WiredTiger storage engine maintains lists of empty records in data files as it deletes documents. This space can be reused by WiredTiger, but will not be returned to the operating system unless under very specific circumstances.The amount of empty space available for reuse by WiredTiger is reflected in the output of db.collection.stats() under the heading wiredTiger.block-manager.file bytes available for reuse.To allow the WiredTiger storage engine to release this empty space to the operating system, you can de-fragment your data file. This can be achieved using the compact command.As the db.collection.stats() command must be run one collection at a time I’ve written the following script to enhance this functionality as follows: scan all namespaces (databases + collections) include index space details support for sharded collections output to CSV/** Print storage details for all collections and indexes.* Supports sharded clusters** @author alex.bevilacqua@mongodb.com* @version 1.3* @updated 2022-11-21** History:* 1.3 - Filter out admin, local and config databases* 1.2 - Properly filter out views* 1.1 - Include Document Count / Average Object Size* 1.0 - Initial Release*/var fmt = function (bytes) { // comment this out to format the results return bytes; var sizes = [&#39;Bytes&#39;, &#39;KB&#39;, &#39;MB&#39;, &#39;GB&#39;, &#39;TB&#39;]; if (bytes == 0) return &#39;0 Byte&#39;; var i = parseInt(Math.floor(Math.log(bytes) / Math.log(1024))); return Math.round(bytes / Math.pow(1024, i), 2) + &#39; &#39; + sizes[i];}var getDetail = function (label, stats) { var detail = { name: label, count: stats.count, avgSize: stats.avgObjSize, size: stats.size, storageSize: stats.storageSize, reusableSpace: stats.wiredTiger[&quot;block-manager&quot;][&quot;file bytes available for reuse&quot;], indexSpace: stats.totalIndexSize, indexReusable: 0, }; var indexKeys = Object.keys(stats.indexDetails); for (var i = 0; i &lt; indexKeys.length; i++) { detail.indexReusable += stats.indexDetails[indexKeys[i]][&quot;block-manager&quot;][&quot;file bytes available for reuse&quot;]; } return detail;}var dbSizeReport = function (dbname) { var results = [] db.getSiblingDB(dbname).getCollectionInfos({ type: &quot;collection&quot; }, { nameOnly: true }).forEach(function(c) { var coll = db.getSiblingDB(dbname).getCollection(c.name); var s = coll.stats({ indexDetails: true }); if (s.hasOwnProperty(&quot;sharded&quot;) &amp;&amp; s.sharded) { var shards = Object.keys(s.shards); for (var i = 0; i &lt; shards.length; i++) { var shard = shards[i]; var shardStat = s.shards[shard]; results.push(getDetail(s.ns + &quot; (&quot; + shard + &quot;)&quot;, shardStat)); } } else { results.push(getDetail(s.ns, s)); } }); var totals = [0, 0, 0, 0, 0]; print([&quot;Namespace&quot;, &quot;Total Documents&quot;, &quot;Average Document Size&quot;, &quot;Uncompressed&quot;, &quot;Compressed&quot;, &quot;Reusable from Collections&quot;, &quot;Indexes&quot;, &quot;Reusable from Indexes&quot;].join(&quot;,&quot;)) for (var i = 0; i &lt; results.length; i++) { var row = results[i]; print([row.name, row.count, row.avgSize, fmt(row.size), fmt(row.storageSize), fmt(row.reusableSpace), fmt(row.indexSpace), fmt(row.indexReusable)].join(&quot;,&quot;)) totals[0] += row.size; totals[1] += row.storageSize; totals[2] += row.reusableSpace; totals[3] += row.indexSpace; totals[4] += row.indexReusable; } print([&quot;Total&quot;, &quot;&quot;, &quot;&quot;, fmt(totals[0]), fmt(totals[1]), fmt(totals[2]), fmt(totals[3]), fmt(totals[4])].join(&quot;,&quot;));}var IGNORE = [&quot;admin&quot;, &quot;local&quot;, &quot;config&quot;];db.getMongo().getDBNames().forEach(function (dbname) { if (IGNORE.indexOf(dbname) &lt; 0) { print(&quot;---------------------&quot;) print(dbname); print(&quot;---------------------&quot;) dbSizeReport(dbname); }});Running this script from a mongo shell will produce output similar to the following:---------------------admin---------------------Namespace,Uncompressed,Compressed,Reusable from Collections,Indexes,Reusable from Indexesadmin.system.keys (config),255 Bytes,36 KB,16 KB,36 KB,16 KBadmin.system.version (config),59 Bytes,20 KB,0 Byte,20 KB,0 ByteTotal,314 Bytes,56 KB,16 KB,56 KB,16 KB---------------------config---------------------Namespace,Uncompressed,Compressed,Reusable from Collections,Indexes,Reusable from Indexesconfig.actionlog (config),32 KB,40 KB,16 KB,40 KB,16 KBconfig.changelog (config),346 KB,132 KB,52 KB,96 KB,44 KBconfig.chunks (config),57 KB,52 KB,24 KB,144 KB,64 KBconfig.collections (config),431 Bytes,36 KB,16 KB,36 KB,16 KBconfig.databases (config),108 Bytes,20 KB,0 Byte,20 KB,0 Byteconfig.lockpings (config),3 KB,36 KB,16 KB,72 KB,32 KBconfig.locks (config),771 Bytes,36 KB,16 KB,108 KB,48 KBconfig.migrations (config),0 Byte,24 KB,16 KB,48 KB,32 KBconfig.mongos (config),342 Bytes,36 KB,16 KB,20 KB,0 Byteconfig.settings (config),39 Bytes,20 KB,0 Byte,20 KB,0 Byteconfig.shards (config),297 Bytes,20 KB,0 Byte,44 KB,4 KBconfig.system.sessions (shard01),99 Bytes,36 KB,16 KB,60 KB,20 KBconfig.tags (config),0 Byte,4 KB,0 Byte,24 KB,4 KBconfig.transactions (config),0 Byte,24 KB,16 KB,12 KB,4 KBconfig.version (config),83 Bytes,20 KB,0 Byte,20 KB,0 ByteTotal,441 KB,536 KB,204 KB,764 KB,284 KB---------------------test---------------------Namespace,Uncompressed,Compressed,Reusable from Collections,Indexes,Reusable from Indexestest.test1 (shard01),37 MB,37 MB,27 MB,26 MB,16 MBtest.test1 (shard02),37 MB,8 MB,52 KB,5 MB,2 MBtest.test1 (shard03),38 MB,8 MB,56 KB,5 MB,2 MBtest.ups_test (shard01),0 Byte,24 KB,16 KB,72 KB,48 KBTotal,112 MB,54 MB,27 MB,36 MB,19 MBThis output can then being imported into your favourite spreadsheet for further manipulation.Based on this sample output, the test.test1 collection on shard01 could reclaim approximately 27MB if compacted. Note that the amount of space reclaimed will not necessarily be exactly what is reported here, but is generally a good guideline as to how much space may be reclaimed." }, { "title": "MongoDB Initial Sync Progress Monitoring", "url": "/blog/2020/02/13/mongodb-initial-sync-progress-monitoring/", "categories": "MongoDB", "tags": "mongodb, replication, scripting", "date": "2020-02-13 12:34:49 -0500", "snippet": "Sometimes our replica set members fall off the oplog and the node needs to be resynced. When this happens, an Initial Sync is required, which does the following: Clones all databases except the lo...", "content": "Sometimes our replica set members fall off the oplog and the node needs to be resynced. When this happens, an Initial Sync is required, which does the following: Clones all databases except the local database. To clone, the mongod scans every collection in each source database and inserts all data into its own copies of these collections. Applies all changes to the data set. Using the oplog from the source, the mongod updates its data set to reflect the current state of the replica set.When the initial sync finishes, the member transitions from STARTUP2 to SECONDARY.Some common questions when performing an initial sync of a Replica Set Member are: How do I know if the sync is progressing? How long will this take to complete?Determining if the sync is progressing can be done by either checking the size of the dbPath of the syncing node or by running the db.adminCommand({ replSetGetStatus: 1, initialSync: 1 }) command while connected to the SECONDARY via the mongo shell.Checking the directory size of the SECONDARY that is being initial sync’ed will provide a good approximation as to how much data still remains to be copied. Note that as the WiredTiger storage engine doesn’t “release” space when documents are deleted there is a high probability that the SECONDARY will have a smaller total directory size than the sync source.The second step (after cloning) where the oplog entries are applied will also affect the overall time required to sync from the sync source.The replSetGetStatus command will produce a JSON document similar to the following. This document contains extensive details as to how the database/collection cloning is progressing, as well as any errors that have occurred during the process.{ &quot;set&quot;: &quot;replset&quot;, &quot;date&quot;: ISODate(&quot;2019-12-04T05:12:52.835Z&quot;), &quot;myState&quot;: 5, &quot;term&quot;: NumberLong(3), &quot;syncingTo&quot;: &quot;m2.example.net:27017&quot;, &quot;syncSourceHost&quot;: &quot;m2.example.net:27017&quot;, &quot;syncSourceId&quot;: 1, &quot;heartbeatIntervalMillis&quot;: NumberLong(2000), &quot;majorityVoteCount&quot;: 2, &quot;writeMajorityCount&quot;: 2, &quot;optimes&quot;: { &quot;lastCommittedOpTime&quot;: { &quot;ts&quot;: Timestamp(0, 0), &quot;t&quot;: NumberLong(-1) }, &quot;lastCommittedWallTime&quot;: ISODate(&quot;1970-01-01T00:00:00Z&quot;), &quot;appliedOpTime&quot;: { &quot;ts&quot;: Timestamp(0, 0), &quot;t&quot;: NumberLong(-1) }, &quot;durableOpTime&quot;: { &quot;ts&quot;: Timestamp(0, 0), &quot;t&quot;: NumberLong(-1) }, &quot;lastAppliedWallTime&quot;: ISODate(&quot;1970-01-01T00:00:00Z&quot;), &quot;lastDurableWallTime&quot;: ISODate(&quot;1970-01-01T00:00:00Z&quot;) }, &quot;lastStableRecoveryTimestamp&quot;: Timestamp(0, 0), &quot;lastStableCheckpointTimestamp&quot;: Timestamp(0, 0), &quot;initialSyncStatus&quot;: { &quot;failedInitialSyncAttempts&quot;: 0, &quot;maxFailedInitialSyncAttempts&quot;: 10, &quot;initialSyncStart&quot;: ISODate(&quot;2019-12-04T05:12:35.719Z&quot;), &quot;initialSyncAttempts&quot;: [], &quot;fetchedMissingDocs&quot;: 0, &quot;appliedOps&quot;: 0, &quot;initialSyncOplogStart&quot;: Timestamp(1575436355, 1), &quot;databases&quot;: { &quot;databasesCloned&quot;: 2, &quot;admin&quot;: { &quot;collections&quot;: 4, &quot;clonedCollections&quot;: 4, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:35.947Z&quot;), &quot;end&quot;: ISODate(&quot;2019-12-04T05:12:36.486Z&quot;), &quot;elapsedMillis&quot;: 539, &quot;admin.system.roles&quot;: { &quot;documentsToCopy&quot;: 12, &quot;documentsCopied&quot;: 12, &quot;indexes&quot;: 2, &quot;fetchedBatches&quot;: 1, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:35.950Z&quot;), &quot;end&quot;: ISODate(&quot;2019-12-04T05:12:36.101Z&quot;), &quot;elapsedMillis&quot;: 151, &quot;receivedBatches&quot;: 1 }, &quot;admin.system.users&quot;: { &quot;documentsToCopy&quot;: 22, &quot;documentsCopied&quot;: 22, &quot;indexes&quot;: 2, &quot;fetchedBatches&quot;: 1, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:36.101Z&quot;), &quot;end&quot;: ISODate(&quot;2019-12-04T05:12:36.252Z&quot;), &quot;elapsedMillis&quot;: 151, &quot;receivedBatches&quot;: 1 }, &quot;admin.system.keys&quot;: { &quot;documentsToCopy&quot;: 2, &quot;documentsCopied&quot;: 2, &quot;indexes&quot;: 1, &quot;fetchedBatches&quot;: 1, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:36.252Z&quot;), &quot;end&quot;: ISODate(&quot;2019-12-04T05:12:36.372Z&quot;), &quot;elapsedMillis&quot;: 120, &quot;receivedBatches&quot;: 1 }, &quot;admin.system.version&quot;: { &quot;documentsToCopy&quot;: 2, &quot;documentsCopied&quot;: 2, &quot;indexes&quot;: 1, &quot;fetchedBatches&quot;: 1, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:36.372Z&quot;), &quot;end&quot;: ISODate(&quot;2019-12-04T05:12:36.486Z&quot;), &quot;elapsedMillis&quot;: 114, &quot;receivedBatches&quot;: 1 } }, &quot;config&quot;: { &quot;collections&quot;: 2, &quot;clonedCollections&quot;: 2, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:36.486Z&quot;), &quot;end&quot;: ISODate(&quot;2019-12-04T05:12:36.863Z&quot;), &quot;elapsedMillis&quot;: 377, &quot;config.transactions&quot;: { &quot;documentsToCopy&quot;: 0, &quot;documentsCopied&quot;: 0, &quot;indexes&quot;: 1, &quot;fetchedBatches&quot;: 0, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:36.487Z&quot;), &quot;end&quot;: ISODate(&quot;2019-12-04T05:12:36.645Z&quot;), &quot;elapsedMillis&quot;: 158, &quot;receivedBatches&quot;: 0 }, &quot;config.system.sessions&quot;: { &quot;documentsToCopy&quot;: 1, &quot;documentsCopied&quot;: 1, &quot;indexes&quot;: 2, &quot;fetchedBatches&quot;: 1, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:36.645Z&quot;), &quot;end&quot;: ISODate(&quot;2019-12-04T05:12:36.863Z&quot;), &quot;elapsedMillis&quot;: 218, &quot;receivedBatches&quot;: 1 } }, &quot;test&quot;: { &quot;collections&quot;: 1, &quot;clonedCollections&quot;: 0, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:36.863Z&quot;), &quot;test.hugeindex&quot;: { &quot;documentsToCopy&quot;: 25000, &quot;documentsCopied&quot;: 9187, &quot;indexes&quot;: 2, &quot;fetchedBatches&quot;: 8, &quot;start&quot;: ISODate(&quot;2019-12-04T05:12:36.865Z&quot;), &quot;receivedBatches&quot;: 9 } } } }, &quot;members&quot;: [ { &quot;_id&quot;: 0, &quot;name&quot;: &quot;m1.example.net:27017&quot;, &quot;ip&quot;: &quot;198.51.100.1&quot;, &quot;health&quot;: 1, &quot;state&quot;: 1, &quot;stateStr&quot;: &quot;PRIMARY&quot;, &quot;uptime&quot;: 17, &quot;optime&quot;: { &quot;ts&quot;: Timestamp(1575436355, 1), &quot;t&quot;: NumberLong(3) }, &quot;optimeDurable&quot;: { &quot;ts&quot;: Timestamp(1575436355, 1), &quot;t&quot;: NumberLong(3) }, &quot;optimeDate&quot;: ISODate(&quot;2019-12-04T05:12:35Z&quot;), &quot;optimeDurableDate&quot;: ISODate(&quot;2019-12-04T05:12:35Z&quot;), &quot;lastHeartbeat&quot;: ISODate(&quot;2019-12-04T05:12:52.216Z&quot;), &quot;lastHeartbeatRecv&quot;: ISODate(&quot;2019-12-04T05:12:51.485Z&quot;), &quot;pingMs&quot;: NumberLong(0), &quot;lastHeartbeatMessage&quot;: &quot;&quot;, &quot;syncingTo&quot;: &quot;&quot;, &quot;syncSourceHost&quot;: &quot;&quot;, &quot;syncSourceId&quot;: -1, &quot;infoMessage&quot;: &quot;&quot;, &quot;electionTime&quot;: Timestamp(1575434944, 1), &quot;electionDate&quot;: ISODate(&quot;2019-12-04T04:49:04Z&quot;), &quot;configVersion&quot;: 3 }, { &quot;_id&quot;: 1, &quot;name&quot;: &quot;m2.example.net:27017&quot;, &quot;ip&quot;: &quot;198.51.100.2&quot;, &quot;health&quot;: 1, &quot;state&quot;: 2, &quot;stateStr&quot;: &quot;SECONDARY&quot;, &quot;uptime&quot;: 17, &quot;optime&quot;: { &quot;ts&quot;: Timestamp(1575436355, 1), &quot;t&quot;: NumberLong(3) }, &quot;optimeDurable&quot;: { &quot;ts&quot;: Timestamp(1575436355, 1), &quot;t&quot;: NumberLong(3) }, &quot;optimeDate&quot;: ISODate(&quot;2019-12-04T05:12:35Z&quot;), &quot;optimeDurableDate&quot;: ISODate(&quot;2019-12-04T05:12:35Z&quot;), &quot;lastHeartbeat&quot;: ISODate(&quot;2019-12-04T05:12:52.216Z&quot;), &quot;lastHeartbeatRecv&quot;: ISODate(&quot;2019-12-04T05:12:52.728Z&quot;), &quot;pingMs&quot;: NumberLong(0), &quot;lastHeartbeatMessage&quot;: &quot;&quot;, &quot;syncingTo&quot;: &quot;&quot;, &quot;syncSourceHost&quot;: &quot;&quot;, &quot;syncSourceId&quot;: -1, &quot;infoMessage&quot;: &quot;&quot;, &quot;configVersion&quot;: 3 }, { &quot;_id&quot;: 2, &quot;name&quot;: &quot;m3.example.net:27017&quot;, &quot;ip&quot;: &quot;198.51.100.3&quot;, &quot;health&quot;: 1, &quot;state&quot;: 5, &quot;stateStr&quot;: &quot;STARTUP2&quot;, &quot;uptime&quot;: 71, &quot;optime&quot;: { &quot;ts&quot;: Timestamp(0, 0), &quot;t&quot;: NumberLong(-1) }, &quot;optimeDate&quot;: ISODate(&quot;1970-01-01T00:00:00Z&quot;), &quot;syncingTo&quot;: &quot;m2.example.net:27017&quot;, &quot;syncSourceHost&quot;: &quot;m2.example.net:27017&quot;, &quot;syncSourceId&quot;: 1, &quot;infoMessage&quot;: &quot;&quot;, &quot;configVersion&quot;: 3, &quot;self&quot;: true, &quot;lastHeartbeatMessage&quot;: &quot;&quot; } ], &quot;ok&quot;: 1}Depending on the number of databases and collections being sync’ed, the size of this document can be quite large and difficult to visually parse.To improve this situation I’ve created the following script./** initialSyncProgress* @author Alex Bevilacqua &lt;alex@alexbevi.com&gt;** Can be run against a MongoDB 3.4+ mongod that is in STARTUP2 (intitial sync) state to gain some* insight into how the sync is progressing. This script WILL NOT tell you how long until the sync* is complete, but based on how the script reports progress can be used to estimate this.** usage:* mongo --quiet --eval &quot;load(&#39;initialSyncProgress.js&#39;); initialSyncProgress();&quot;*/var printPercentage = function (position, length, type) { var p = Math.round((position / length) * 100, 2); return position + &quot;/&quot; + length + &quot; &quot; + type + &quot; (&quot; + p + &quot;%)&quot;;}var msToTime = function (duration) { var milliseconds = parseInt((duration % 1000) / 100), seconds = Math.floor((duration / 1000) % 60), minutes = Math.floor((duration / (1000 * 60)) % 60), hours = Math.floor((duration / (1000 * 60 * 60)) % 24); hours = (hours &lt; 10) ? &quot;0&quot; + hours : hours; minutes = (minutes &lt; 10) ? &quot;0&quot; + minutes : minutes; seconds = (seconds &lt; 10) ? &quot;0&quot; + seconds : seconds; return hours + &quot;:&quot; + minutes + &quot;:&quot; + seconds + &quot;.&quot; + milliseconds;}var initialSyncProgress = function () { var status = db.adminCommand({ replSetGetStatus: 1, initialSync: 1 }); var dbs_cloned = status.initialSyncStatus.databases.databasesCloned; delete status.initialSyncStatus.databases.databasesCloned; var dbs = Object.keys(status.initialSyncStatus.databases); var dbs_total = dbs.length; // total time elapsed syncing databases var elapsedMillis = 0; // status message based on the position within the currently // cloning database (collections cloned of collections total) var currentlyCloningStatus = &quot;&quot;; for (var i = 0; i &lt; dbs_total; i++) { var d = status.initialSyncStatus.databases[dbs[i]]; // if the counts aren&#39;t the same either it&#39;s the database that&#39;s in progress or // hasn&#39;t started cloning yet if (d.clonedCollections &lt; d.collections) { currentlyCloningStatus = &quot;Cloning database &quot; + dbs[i]; currentlyCloningStatus += &quot; - cloned &quot; + printPercentage(d.clonedCollections, d.collections, &quot;collections&quot;); var collectionKeys = Object.keys(d); for (var j = 0; j &lt; collectionKeys.length; j++) { var c = d[collectionKeys[j]]; if (c.hasOwnProperty(&quot;documentsToCopy&quot;) &amp;&amp; (c.documentsCopied &lt; c.documentsToCopy)) { currentlyCloningStatus += &quot;\\nCloning collection &quot; + collectionKeys[j] + &quot; &quot; + printPercentage(c.documentsCopied, c.documentsToCopy, &quot;documents&quot;); } } } // only add time if there&#39;s time to record if (d.hasOwnProperty(&quot;elapsedMillis&quot;)) { elapsedMillis += d.elapsedMillis; } } print(&quot;===================&quot;) print(&quot;Initial Sync Status&quot;) print(&quot;===================&quot;) var now = new Date(); var started = status.initialSyncStatus.initialSyncStart; print(&quot;Cloning started at &quot; + started + &quot; (&quot; + msToTime(now - started) + &quot; ago)&quot;); var members = status.members; for (var i = 0; i &lt; members.length; i++) { if (members[i].stateStr == &quot;PRIMARY&quot;) { var optime = members[i].optimeDate var me = new Date(status.initialSyncStatus.initialSyncOplogStart.getTime() * 1000); print(&quot;Currently &quot; + msToTime(optime - me) + &quot; behind the PRIMARY (based on optimes)&quot;); } } if (status.initialSyncStatus.hasOwnProperty(&quot;initialSyncAttempts&quot;) &amp;&amp; status.initialSyncStatus.initialSyncAttempts.length &gt; 0) { var failures = status.initialSyncStatus.initialSyncAttempts.length; print(&quot;Cloning has already failed &quot; + failures + &quot; time(s) ...&quot;); print(&quot;Last Failure: &quot; + status.initialSyncStatus.initialSyncAttempts[failures - 1].status); } print(&quot;Copying databases for &quot; + msToTime(elapsedMillis) + &quot;. Note this updates AFTER a collection has been cloned.&quot;); print(&quot;Cloned &quot; + printPercentage(dbs_cloned, dbs_total, &quot;databases&quot;)); print(currentlyCloningStatus);}By running this against the SECONDARY from the mongo shell, a more concise representation of the initialSyncStatus document is produced:The script will also let you know if there have been any sync failures recorded, as well as what the last failure was.Hopefully you’ll find this useful when the time comes to resync one of your nodes." }, { "title": "What is MongoDB FTDC (aka. diagnostic.data)", "url": "/blog/2020/01/26/what-is-mongodb-ftdc-aka-diagnostic-dot-data/", "categories": "MongoDB", "tags": "mongodb", "date": "2020-01-26 18:14:50 -0500", "snippet": "Full Time Diagnostic Data Capture (FTDC) was introduced in MongoDB 3.2 (via SERVER-19585), to incrementally collect the results of certain diagnostic commands to assist MongoDB support with trouble...", "content": "Full Time Diagnostic Data Capture (FTDC) was introduced in MongoDB 3.2 (via SERVER-19585), to incrementally collect the results of certain diagnostic commands to assist MongoDB support with troubleshooting issues.On log rotation or startup, a mongod or mongos will collect and log: getCmdLineOpts: db.adminCommand({getCmdLineOpts: true}) buildInfo: db.adminCommand({buildInfo: true}) hostInfo: db.adminCommand({hostInfo: true})As configured by diagnosticDataCollectionPeriodMillis and defaulting to every 1 second, FTDC will collect the output of the following commands: serverStatus: db.serverStatus({tcmalloc: true}) replSetGetStatus: rs.status() collStats for the local.oplog.rs collection (mongod only) connPoolStats (mongos only)When FTDC is enabled (per diagnosticDataCollectionEnabled), the metrics.xxxxxxx files will be stored in diagnosticDataCollectionDirectoryPath which by default is the diagnostic.data directory within the systemLog.path.With SERVER-21818 (introduced in MongoDB 3.2.13) and SERVER-31400 (introduced in MongoDB 3.4.16) the diagnostic data capture scope was broadened to not only include internal diagnostic commands but system metrics as well. Depending on the host operating system, the diagnostic data may include one or more of the following statistics: CPU utilization (ex: /proc/stat) Memory utilization (ex: /proc/meminfo) Disk utilization related to performance (ex: /sys/block/*/stat) Network performance statistics (/proc/net/netstat)The metrics.xxxxxxx files in the diagnostic.data directory contain only statistics about the performance of the system and the database. They are stored in a compressed format, and are not human-readable.Just a quick note regarding privacy, regardless of the version, the data in diagnostic.data never contains: Samples of queries, query predicates, or query results Data sampled from any end-user collection or index System or MongoDB user credentials or security certificatesFTDC data contains certain host machine information such as hostnames, operating system information, and the options or settings used to start the mongod or mongos. This information may be considered protected or confidential by some organizations or regulatory bodies, but is not typically considered to be Personally Identifiable Information (PII).If you want to have a closer look at the diagnostic data collection process, you can inspect the FTDC code.FTDC StructureThere are two types of FTDC documents: a BSON metadata document, or a BSON metric chunk.Each document is made up of an _id, a type and either a doc or data field. The type field is used to identify the document type: 0: Metadata Document 1: Metric ChunkThe doc or data fields will contain “samples” in the form of:{ \"start\" : DateTime, /* Time at which all collecting started */ \"name\" : String, /* name is from name() in FTDCCollectorInterface */ { \"start\" : DateTime, /* Time at which name() collection started */ \"data\" : { ... }, /* data comes from collect() in FTDCCollectorInterface */ \"end\" : DateTime, /* Time at which name() collection ended */ }, ... /* more than 1 collector be sampled */ \"end\" : DateTime /* Time at which all collecting ended */}Samples are collected by FTDCCollectorInterface instances.Metadata Document{ \"_id\": DateTime, \"type\": 0, \"doc\": { .. } /* Samples from collectors */}On log rotation or startup, the first FTDC entry will be collected and stored. This is a BSON document that contains information sampled by running getCmdLineOpts, buildInfo and hostInfo.// example{ \"start\": DateTime, \"buildInfo\": { ... }, \"getCmdLineOpts\": { ... }, \"hostInfo\": { ... }, \"end\": DateTime}This sample will be stored in the doc field of the metadata document.Metric Chunk{ \"_id\": DateTime, \"type\": 1 \"data\": BinData(...)}During each collection interval (as configured by diagnosticDataCollectionPeriodMillis), a metric chunk will be created and a sample will be collected, compressed and stored to the data document as Binary Data.This sample can contain the results of internal commands such as serverStatus,replSetGetStatus, collStats for the local.oplog.rs collection or connPoolStats, as well as external system metrics.// example{ \"start\": DateTime, \"serverStatus\": { ... }, \"connPoolStats\": { ... }, \"systemMetrics\": { ... }, \"end\": DateTime}Decoding FTDC metrics.xxxxxxx filesFTDC files, such as the metrics.2019-10-28T19-02-23Z-00000 example file we’ll be working with below are just BSON files. As such, the bsondump utility can be used to inspect the contents:METRICS=metrics.2019-10-28T19-02-23Z-00000bsondump --quiet $METRICS | lessbsondump will default to emitting JSON, so we can interact with this using the jq utility. For example, if we only want to review the Metadata Document this could be done as follows:# bsondump &lt; 4.0bsondump --quiet $METRICS | jq -s '.[] | select( .type == 0)' | less# bsondump &gt;= 4.0bsondump --quiet $METRICS | jq -s '.[] | select( .type | .\"$numberInt\" == \"0\")' | lessWorking with Metric Chunks is a little more complicated as they are actually zlib compressed BSON documents. We’ll use the jq utility to only select the first chunk and the Ruby interpreter to decompress the zlib data. Note that the following command can be altered to navigate to other chunks (not only the first) as needed:# bsondump &lt; 4.0METRICS=metrics.2019-12-20T14-22-56Z-00000bsondump --quiet $METRICS | \\ jq -s '.[] | select( .type == 1)' | \\ jq -s 'first | .data .\"$binary\"' -Mc | \\ ruby -rzlib -rbase64 -e 'd = STDIN.read; print Zlib::Inflate.new.inflate(Base64.decode64(d)[4..-1])' | \\ bsondump --quiet# bsondump &gt;= 4.0METRICS=metrics.2019-12-20T14-22-56Z-00000bsondump --quiet $METRICS | \\ jq -s '.[] | select( .type | .\"$numberInt\" == \"1\")' | \\ jq -s 'first | .data .\"$binary\" .base64' -Mc | \\ ruby -rzlib -rbase64 -e 'd = STDIN.read; print Zlib::Inflate.new.inflate(Base64.decode64(d)[4..-1])' | \\ bsondump --quietYou eagle-eyed Rubyists will notice that we’re clipping the first 4 bytes from the binary data we’re reading from STDIN. This is to drop the header before we try to decompress the stream.If you don’t do this zlib will complain and fail:Traceback (most recent call last): 1: from -e:1:in `&lt;main&gt;'-e:1:in `inflate': incorrect header check (Zlib::DataError)The binary data has now been decompressed, and being BSON data we run it through bsondump again and voila:Hopefully this helps shed some light on what FTDC data is and what it contains. In a future post we’ll look into doing something useful with this treasure trove of telemetry our clusters are generating every 1 second or so." }, { "title": "Troubleshooting and Fixing Invariant Failure !_featureTracker on MongoDB Startup", "url": "/blog/2020/01/23/troubleshooting-and-fixing-invariant-failure-featuretracker/", "categories": "MongoDB", "tags": "troubleshooting", "date": "2020-01-23 05:34:53 -0500", "snippet": "I recently found myself troubleshooting another MongoDB startup issue due to potential corruption within a WiredTiger file. As I have previously covered this topic (see “Recovering a WiredTiger col...", "content": "I recently found myself troubleshooting another MongoDB startup issue due to potential corruption within a WiredTiger file. As I have previously covered this topic (see “Recovering a WiredTiger collection from a corrupt MongoDB installation”), I wanted to share the diagnostic and troubleshooting journey in case it helps anyone who experiences this issue in the future.To ensure I could troubleshoot this issue in isolation, I first collected a backup of the necessary files from the affected installation as follows:tar -czvf metadata.tar.gz --exclude=WiredTigerStat* WiredTiger* _mdb_catalog.wt sizeStorer.wtOnce I had this backup I extracted it to a new location, then using m to select the versions of MongoDB to use tried to startup a standalone instance to see if I could reproduce the issue:mkdir -p /tmp/reprocd /tmp/repro# move archive from earlier to the new directory firsttar xvf metadata.tar.gz# This is the version of MongoDB reported to be crashingm 3.4.18mongod --dbpath .Once the mongod started, we were able to see the failure and the process aborts (clipped log sample below).2020-01-23T03:58:19.828-0500 I CONTROL [initandlisten] db version v3.4.182020-01-23T03:58:19.828-0500 I CONTROL [initandlisten] git version: 4410706bef6463369ea2f42399e9843903b31923...2020-01-23T03:58:20.187-0500 I - [initandlisten] Invariant failure !_featureTracker src/mongo/db/storage/kv/kv_catalog.cpp 3052020-01-23T03:58:20.187-0500 I - [initandlisten]***aborting after invariant() failure2020-01-23T03:58:20.198-0500 F - [initandlisten] Got signal: 6 (Aborted).... mongod(_ZN5mongo15printStackTraceERSo+0x41) [0x55bb45c92111] mongod(+0x153F329) [0x55bb45c91329] mongod(+0x153F80D) [0x55bb45c9180d] libpthread.so.0(+0x12890) [0x7f5b7bee5890] libc.so.6(gsignal+0xC7) [0x7f5b7bb20e97] libc.so.6(abort+0x141) [0x7f5b7bb22801] mongod(_ZN5mongo17invariantOKFailedEPKcRKNS_6StatusES1_j+0x0) [0x55bb44f5b234] mongod(_ZN5mongo9KVCatalog4initEPNS_16OperationContextE+0x568) [0x55bb458db5e8] mongod(_ZN5mongo15KVStorageEngineC1EPNS_8KVEngineERKNS_22KVStorageEngineOptionsE+0x807) [0x55bb458e79f7] mongod(+0x124DFFA) [0x55bb4599fffa] mongod(_ZN5mongo20ServiceContextMongoD29initializeGlobalStorageEngineEv+0x697) [0x55bb45891627] mongod(+0x7F62AC) [0x55bb44f482ac] mongod(main+0x96B) [0x55bb44f66a6b] libc.so.6(__libc_start_main+0xE7) [0x7f5b7bb03b97] mongod(+0x86FFB1) [0x55bb44fc1fb1]----- END BACKTRACE -----Aborted (core dumped)The mongod is failing to startup successfully due to an invariant failure during KVCatalog::init. We are able to determine this as the mongod log above tells us: The MongoDB version in used (3.4.18) The path to the source file where the failure occurred (file: src/mongo/db/storage/kv/kv_catalog.cpp, line: 305)As MongoDB is open source, we can view the source for this release by going to https://github.com/mongodb/mongo/blob/r3.4.18/src/mongo/db/storage/kv/kv_catalog.cpp#L305, which will show us the following:if (FeatureTracker::isFeatureDocument(obj)) { // There should be at most one version document in the catalog. invariant(!_featureTracker); // Initialize the feature tracker and skip over the version document because it doesn't // correspond to a namespace entry. _featureTracker = FeatureTracker::get(opCtx, this, record-&gt;id); continue;}The comment preceding the invariant1 indicates that there’s only one feature document to be present in the catalog, but what’s the catalog?ls -l *catalog*-rw-r--r-- 1 alex 249856 Jan 23 03:58 _mdb_catalog.wtAs there’s only one file that contains the word “catalog” this is good a place as any to start. The _mdb_catalog is a WiredTiger file, so to interact with it directly (outside of MongoDB) we will need to use the WiredTiger command line utility, also know as wt.The documentation link for mongodb-3.4 points us to WiredTiger 2.9.2, so following the build and installation instructions we compile a wt binary with support for the snappy compressor. This is due to MongoDB’s WiredTiger storage engine using snappy as the default block compressor (see “Compression”).cd /tmp/reprogit clone git://github.com/wiredtiger/wiredtiger.gitcd wiredtigergit checkout 2.9.2sh autogen.sh# ensure you have the necessary development headers for the snappy compression# library before compiling./configure --enable-snappy &amp;&amp; makeOnce we’ve successfully build the wt utility with snappy compression we can dump our catalog to see if we can find a duplicate entry for the feature document.cd /tmp/repro# to shorten the amount of typing required, wrap the wt utility invocation in# a function we can call insteadWT() { /tmp/repro/wiredtiger/wt -v -C \"extensions=[\\\"/tmp/repro/wiredtiger/ext/compressors/snappy/.libs/libwiredtiger_snappy.so\\\"]\" $@; }# write the catalog dump out to a fileWT dump _mdb_catalog &gt; dump.datNOTE: If you receive the following error, just re-run the command.[1579773800:589375][9348:0x7fc9a8e17140], txn-recover: Recovery failed: WT_RUN_RECOVERY: recovery must be run to continuewt: WT_RUN_RECOVERY: recovery must be run to continueThis error is due to the presence of content in the journal/ that was created when we last ran the mongod.With the catalog dumped we can now search it for the feature document:grep isFeatureDoc dump.dat -B 1 -n935-\\c2\\e5936:C\\00\\00\\00\\08isFeatureDoc\\00\\01\\0ans\\00\\12nonRepairable\\00\\00\\00\\00\\00\\00\\00\\00\\00\\12repairable\\00\\01\\00\\00\\00\\00\\00\\00\\00\\00937-\\c2\\e6938:C\\00\\00\\00\\08isFeatureDoc\\00\\01\\0ans\\00\\12nonRepairable\\00\\00\\00\\00\\00\\00\\00\\00\\00\\12repairable\\00\\01\\00\\00\\00\\00\\00\\00\\00\\00INTERESTING! I’m not really sure how the catalog was able to get into a state where two feature documents exist, but since we have a dump of the catalog let’s try to remove one of those entries and then load the dump back into the catalog.As the results appear to be identical, we’ll just drop the first one and then try to load it back into the catalog.# remove lines 935-936 and overwrite the filesed -i -e '935,936d' dump.dat# drop the contents of the _mdb_catalog tableWT truncate _mdb_catalog# reload the table from the dump fileWT load -f dump.datIf the table loaded successfully the output of the command should be something like table:_mdb_catalog: 822.With a reloaded catalog, let’s try spinning up the mongod again:2020-01-23T05:24:54.911-0500 I CONTROL [initandlisten] db version v3.4.18...2020-01-23T05:24:56.247-0500 E STORAGE [initandlisten] no cursor for uri: table:SomeCollection/collection/34-13498437758539120652020-01-23T05:24:56.247-0500 F - [initandlisten] Invalid access at address: 0x582020-01-23T05:24:56.259-0500 F - [initandlisten] Got signal: 11 (Segmentation fault).SUCCESS! The mongod is still crashing as the backing files for the database don’t exist, but we should now be able to take our recovered files back to our node that was previously failing.From our recovered directory compress the following files:tar -czvf recovered.tar.gz --exclude=WiredTigerStat* WiredTiger* _mdb_catalog.wt sizeStorer.wtNote that if the mongod fails to start with the recovered files you may have to clear out the journal/ directory.Hopefully this helps someone someday ;)If you enjoyed this post and like solving these types of problems, MongoDB is hiring!1 An invariant is a condition to test, that on failure will log the test condition, source file and line of code. ↩" }, { "title": "Current Date Math in MongoDB Aggregations", "url": "/blog/2020/01/17/current-date-math-in-mongodb-aggregations/", "categories": "MongoDB, Queries & Indexing", "tags": "mongodb", "date": "2020-01-17 06:30:17 -0500", "snippet": "A challenge that I’ve had in the past while working with my data in MongoDB has been how to incorporatedate math into my aggregations.db.foo.insertMany([{ lastUpdated: new Date(new Date().setDate(n...", "content": "A challenge that I’ve had in the past while working with my data in MongoDB has been how to incorporatedate math into my aggregations.db.foo.insertMany([{ lastUpdated: new Date(new Date().setDate(new Date().getDate() - 1)) },{ lastUpdated: new Date(new Date().setDate(new Date().getDate() - 5)) },{ lastUpdated: new Date(new Date().setDate(new Date().getDate() - 9)) }]);db.foo.find();/*{ \"_id\" : ObjectId(\"5e219c6ecc99b35bb2975d9e\"), \"lastUpdated\" : ISODate(\"2020-01-16T11:37:18.522Z\") }{ \"_id\" : ObjectId(\"5e219c6ecc99b35bb2975d9f\"), \"lastUpdated\" : ISODate(\"2020-01-12T11:37:18.522Z\") }{ \"_id\" : ObjectId(\"5e219c6ecc99b35bb2975da0\"), \"lastUpdated\" : ISODate(\"2020-01-08T11:37:18.522Z\") }*/Given the 3 documents we’ve setup above, if I wanted to filter a pipeline to only $matchdocuments that are newer than 1 week old, I would have to resort to using Javascript:// compare lastUpdated to a new Javascript Date object set to// 7 days from the current datedb.foo.aggregate({ $match: { lastUpdated: { $gte: new Date(new Date().setDate(new Date().getDate() - 7)) } }});/*{ \"_id\" : ObjectId(\"5e219c6ecc99b35bb2975d9e\"), \"lastUpdated\" : ISODate(\"2020-01-16T11:37:18.522Z\") }{ \"_id\" : ObjectId(\"5e219c6ecc99b35bb2975d9f\"), \"lastUpdated\" : ISODate(\"2020-01-12T11:37:18.522Z\") }*/Now if your pipeline is running in a non-Javascript environment, the new Date() call within the pipelinewould likely throw an exception.If you’re working with MongoDB 4.2 or newer though, a new $$NOW aggregation variable is available that can be combined with existing pipeline operators to $subtract the number of milliseconds in the number of days to filter from the current date:// compare lastUpdated to the number of milliseconds in// 7 days subtracted from the currentdb.foo.aggregate({ $match: { $expr: { $let: { vars: { start: { $subtract: [\"$$NOW\", (7 * 86400000)] } }, in: { $gte: [\"$lastUpdated\", \"$$start\"] } } } }});/*{ \"_id\" : ObjectId(\"5e219c6ecc99b35bb2975d9e\"), \"lastUpdated\" : ISODate(\"2020-01-16T11:37:18.522Z\") }{ \"_id\" : ObjectId(\"5e219c6ecc99b35bb2975d9f\"), \"lastUpdated\" : ISODate(\"2020-01-12T11:37:18.522Z\") }*/I hope you find this as useful as I did. With each major release of MongoDB new features and functionalityare being introduced that reduce the “hacks” or “workarounds” we’ve had to do in the past.If you’re looking for more MongoDB tips and tricks, head on over to Asya’s Stupid Tricks With MongoDB.Let me know in the comments below if you have any questions, or if you found this useful." }, { "title": "Technical Services Engineering at MongoDB", "url": "/blog/2018/10/01/technical-services-engineering-at-mongodb/", "categories": "Personal", "tags": "mongodb, career", "date": "2018-10-01 15:39:28 -0400", "snippet": "The goal of this post is to provide a first hand account of what it means to be a Technical Services Engineer at MongoDB, as well as what the journey getting to this point has looked like for me.WH...", "content": "The goal of this post is to provide a first hand account of what it means to be a Technical Services Engineer at MongoDB, as well as what the journey getting to this point has looked like for me.WHO AM I?I have been working in Application Development and Software Engineering for nearly two decades. I started off writing desktop applications in QuickBASIC and Turbo Pascal, then eventually in VB6, VB.NET, C++ and C#. When it was time to shift focus to web development I started off with HTML/JS/CSS (as we all do :P), then in Flash/AS3, Flex, Python, Ruby/Rails and Node.js.I have been writing software since I was a kid, starting with some automation tools for my mom’s business. I then moved on to building tools to help me cheat at various games I was playing at the time, and eventually got more into emulator programming and reverse engineering. I guess you could say I’ve always loved solving problems programmatically, and especially enjoyed identifying opportunities for automation and custom tooling.This led me down an informal DevOps track, as I was finding there was a need for optimization in the infrastructure layers that my applications were deployed to. This led me deeper into Linux internals, system administration and network operations.While I was gaining these new skill-sets my primary focus was always on application development and delivery. Before coming to MongoDB I was working as a Development Lead / System Architect, but I found that my focus was always being drawn back to solving performance challenges at the infrastructure level.WHY MONGODB?I started working with MongoDB on a number of “hobby” projects around 2012. At the time I really only had experience with RDBMS’, but due to the unstructured nature of the data I was working with decided to give this new technology a whirl.I fell in love with the database almost immediately, and have since carried it forward to multiple new employers, as well as contract opportunities and consulting engagements.The low barrier to entry from a development bootstrapping perspective made it the ideal backend for proof-of-concept development through to production deployment.As a result of this increased activity with MongoDB, I found my self doing a lot more investigation into performance issues and internals (links are to blog posts of challenges I encountered and resolved).WHY TECHNICAL SERVICES?This was initially very challenging for me, as I had pre-conceived notions as to what “technical services” actually implied. The first thoughts that popped in my head were “technical support”, “client support”, “call center style support”, etc.While researching this position I came across a blog post from about six years ago by a MongoDB employee who blogged about his experience as a Support Engineer (in this two part series).I found his reasons for joining MongoDB (10gen at the time), description of what kinds of challenges the job poses on a daily basis and how there is a constant push for self improvement and continuing education to align with what I was looking for in a new opportunity.WHAT’S A TECHNICAL SERVICES ENGINEER ON PAPERTo answer this question, let’s start off by analyzing the job posting that kicked off this journey for me in the first place.So they’re looking for people that are able to solve problems and communicate clearly. This could be a call center gig after all … oh wait, experts in MongoDB related database servers, drivers, tools, services … hrm, maybe there’s a bit more to this.Architecture, performance, recovery, security, those are a lot more complex than what you would face in a traditional support role. What really sold me though was the contribute to internal projects statement, as this aligned perfectly with my desire for process improvement through custom tooling.By the time I got to this point in the job posting I was already sold. MongoDB is either trying to staff their first tier support with ridiculously over-qualified employees, or Technical Services really isn’t what I would have thought.I proceeded to fill out the application, attach my resume and cover letter and crossed my fingers.WHAT’S A TECHNICAL SERVICES ENGINEER IN PRACTICEAfter working with other TSEs for the past two months and having had an opportunity to handle some of my own cases I think I can shed a bit of light on what this role really entails.HOW IS IT A SUPPORT ROLE?A Technical Services Engineer interacts with MongoDB’s clients via a support queue. This allows incoming “cases” to be prioritized and categorized to allow engineers to quickly identify what form of subject matter expertise may be required (ex: Indexing, Replication, Sharding, Performance, Networking, etc).As a TSE you’re responsible for claiming cases from a queue and providing feedback in a timely fashion that is clear, concise and technically accurate.HOW IS IT AN ENGINEERING ROLE?Here’s the juicy part of this job. Although replying to client requests is the “deliverable” for a TSE, how you go about reproducing their issues requires a very deep understanding of MongoDB internals, software engineering, network engineering, infrastructure architecture and technical troubleshooting.Depending on the type of issue, a reproduction is likely in store. These involve recreating the environment (locally or in the cloud) to either benchmark or replicate the identified client challenge. There is a vast library of tools available to TSEs for these types of tasks, but on some occasions the right tool for the job may not exist.In these cases, you have an opportunity to write your own scripts or tools to parse logs, measure performance, record telemetry or verify a hypothesis. Although MongoDB doesn’t require TSEs to have any programming experience, for those like me that come from product engineering it’s refreshing to know there’s still an opportunity to scratch the development itch.With each case you learn more about the inner working of the database, the tools, the drivers and OS level performance.CONCLUSION?I’m leaving the closing section here as a question, as the TSE role continues to be redefined and refined as new MongoDB products come on board and new challenges present themselves.What will likely remain constant though is the need for new engineers to have the following characteristics: a passion for continuing technical education a willingness to step outside their comfort zone an interest in software engineering an interest in network operationsI encourage you to check out MongoDB’s available jobs if what I’ve described here interests you (I swear HR is not putting me up to this …) as we could use more engineers like you in our ranks :)Feel free to leave a comment below or shoot me an email at alex@alexbevi.com if you have any questions." }, { "title": "Hello MongoDB", "url": "/blog/2018/08/14/hello-mongodb/", "categories": "Personal", "tags": "mongodb, career", "date": "2018-08-14 15:31:04 -0400", "snippet": "As of August 13th, I am no longer a System Architect at DAC Group. I have a public post on LinkedIn that got some good traction, but to summarize it was time to move on.I’ve been a software enginee...", "content": "As of August 13th, I am no longer a System Architect at DAC Group. I have a public post on LinkedIn that got some good traction, but to summarize it was time to move on.I’ve been a software engineer in some capacity or another for nearly 20 years now. The position I’ve taken is as a Technical Services Engineer, which is more of a support role than an active development role.The decision to make this move wasn’t make lightly. I’ve been working hands on with code or overseeing a team of developers on a day to day basis for most of my professional career. As such, I was also involved with software engineering, and this was no different in my role as a System Architect.In that role, I was still committing code on a nearly daily basis. If not, I was performing code review, or working on a design for a new system or solution. I would consider this all to still be “hands on”, though I had found myself mired in DevOps work a lot more than I would have liked (there were not sufficient Linux Sysadmins available to assist with the type of server operations oversight that was required).The role at MongoDB isn’t a traditional “Tech Support” type of role, as it requires a strong knowledge of networking, databases, system design, programming and client services. I’ve been a fan of the MongoDB server for over 8 years now, and have brought it along with me to several new consulting opportunities as well as the full time jobs I’ve help. I believe very strongly in the quality of this product, as well as the peripheral products that they’ve developed.I think the time has come for a new adventure. This is the first step towards a new career journey with a new company, as opposed to an incremental move upwards within the same professional space." }, { "title": "Troubleshooting a MongoDB Performance Issue", "url": "/blog/2018/05/28/troubleshooting-a-mongodb-performance-issue/", "categories": "MongoDB", "tags": "mongodb, performance, troubleshooting", "date": "2018-05-28 09:14:03 -0400", "snippet": "UPDATE (2018-06-28): I actually sent a link to this article to the author of the previous blog post and in her reply she indicates that the improvements to cache management and checkpoint areas wer...", "content": "UPDATE (2018-06-28): I actually sent a link to this article to the author of the previous blog post and in her reply she indicates that the improvements to cache management and checkpoint areas were more likely to have improved my situation. Just wanted to call out how approachable the MongoDB team is even with these one-off type issues :). Thanks Sue!UPDATE (2018-06-21): As we were running MongoDB 3.0.15 while all these issues were going on it’s entirely possible that the optimizations made to the write-ahead log of WiredTiger may have also contributed to this improvement in performance :)The following is an edited excerpt from an email I sent out internally about an intermittent performance issue we’ve been experiencing for several years now. The daily processing challenges we’ve been experiencing revolved around running server-side javascript in order to produce daily reports. As our data ingestion rates rose and our data processing needs climbed, our server performance continued to degrade. This would occur regardless of the size of the VMs we would spin up.PostmortemOur MongoDB cluster is configured with three (3) servers: 1x primary (write-enabled) and 2x secondaries (read-only). These are running on Azure DS14v2 VMs with 8TB of storage (8x 1TB striped via LVM as these were the largest premium SSD-based data disks available at the time).Aside from the servers being scaled up periodically, this configuration has been constant since the inception of the product.The only major upgrade came in the form of a migration from 2.6 to 3.0 in 2015. At the time this was a major shift as it required rewriting a number of the underlying system scripts as well as introducing LRS-based storage to try and squeeze some additional performance out of the disks. Why optimize for IOPS? Because the reporting platform was designed to copy a lot of data back and forth in order to generate reports segmented by dimension (“Group”, “Company”, “Country”, “State”, “City”).This chart (48 hours sampled from 1 week ago) shows Cache Usage spiking and Replication Lag spiking. The cache spikes occur as new writes trigger index activity, which invalidates (dirties) cached memory and causes cache eviction.This slows down the speed at which the secondaries can request data from the primary, which spikes the lag. When the secondaries request more data, it would lock up the primary, which in turn affected the primary server’s ability to ingest new content and write it to disk. The read/write buffers back up and new write requests are throttled.Note — As of MongoDB 4.0, non-blocking secondary reads have been added to address these types of latency issues.This type of cascading failure was almost exclusively seen when a large batch process was being run in the morning directly on the primary mongod instance in the mornings..This chart (48 hours sampled from 2 weeks ago) shows similar behaviour. The vertical lines show points at which we were forced to restart instances or cycle the primary server in order to recover resources.You’ll notice that cache usage hits a certain point on the primary (left) server after which we have to kill the instance. The replication lag on the secondaries is also inconsistent, which would lead us to believe that the consumption rates from the primary are being affected by either network performance or disk performance.In the absence of dedicated DevOps, DBAs or Infrastructure Engineers, the development teams have spent a significant amount of time learning to tune and troubleshoot this installation. Due to lack of specialization though occasionally issues may be misdiagnosed.We completed a significant upgrade on Tuesday that brings our cluster up to mongodb-server 3.4.15 (from 3.0.15). The 3.0 series was first introduced in March 2015, with an end of life of February 2018. As no further security updates are being released, we’ve been coordinating tests with the product development teams for the past 12 months in order to prepare for a major upgrade.This involved the deprecation of client-side javascript calls, as well as rewriting several map/reduce operations as aggregation pipelines to ensure when the transition happened there was no sudden outage.Now that we’ve been running 3.4 in production for a few days I checked the same 48 hour sample and found something interesting …The cache usage has remained steady since we turned the instances on. The replication lag also hasn’t gone much higher than a minute in the past few days (this could creep up to over an hour in the past!).These samples include report generation for all products as well, so they represent the same load. We’ll have to continue to monitor this, but the initial results seem to show that a lot of the pain we’ve been suffering through may have stemmed from outdated software.The way the review report is generated is still extremely inefficient, but if we continue seeing results like this for the foreseeable future then the urgency of redesigning that product drops and can be properly managed.Here are some lessons we learned as a result of this investigation:Measure Everything — Without proper telemetry in place, not only is it difficult to identify negative trends, but it’s almost impossible to showcase the success of any change or action.Understand Your Tech — Whether you’re using hosted, provisioned, on-premise, containerized, PAAS or some other solution as part of the application architecture, make sure you really understand how to use it, and how to support it. When MongoDB was introduced to the project it was done so to fill a specific need. Once that need was filled, resourcing discussions surrounding maintainability and support should likely have been prioritized.Document Everything — As discoveries are made, write them down and share them. Knowledge sharing is even more important when you’re dealing with issues that go beyond the standard requirements of “application development”.Ask For Help — When it becomes necessary to step outside your comfort zone to solve a problem, a fresh perspective can be welcome.Hopefully this journey benefits someone else in a similar situation." }, { "title": "Setting up domain forwarding in hover.com", "url": "/blog/2018/04/25/setting-up-domain-forwarding-in-hover-dot-com/", "categories": "Other, Configuration", "tags": "hover, dns", "date": "2018-04-25 15:27:08 -0400", "snippet": "I’ve known for a long time that when you navigate to my domain directly at alexbevi.com that you would be redirected to a Hover placeholder page.I’ve meant to add a domain redirect for a long time ...", "content": "I’ve known for a long time that when you navigate to my domain directly at alexbevi.com that you would be redirected to a Hover placeholder page.I’ve meant to add a domain redirect for a long time but just never got around to it … until now.If you log into your Hover control console at https://www.hover.com/control_panel/domain/&lt;your domain&gt;, you can just add the forward from the Mangage Forwards section. Click Create a Forward Select from the dropdown list Enter the full url (http://…) you would like requests to your domain to go to Click Save ForwardAfter about 15 minutes this will be active and all requests to your domain will redirect to the url you’ve selected." }, { "title": "Redmine Plugin Extension and Development Still In Demand", "url": "/blog/2017/10/03/redmine-plugin-extension-and-development-still-in-demand/", "categories": "Writing", "tags": "redmine, book", "date": "2017-10-03 20:38:12 -0400", "snippet": "It’s been a while since I last wrote about Redmine Plugin Extension and Development so I thought I’d give a quick update.I have been sharing sales numbers whenever possible as a way of (hopefully) ...", "content": "It’s been a while since I last wrote about Redmine Plugin Extension and Development so I thought I’d give a quick update.I have been sharing sales numbers whenever possible as a way of (hopefully) encouraging other authors to see that there is some money out there, even for obscure niche topics.   Ebook Print Q2/2017 17 12 Q1/2017 16 8 This isn’t a super lucrative endeavour, but considering I published this book three years ago and it’s still in demand, I can’t complain.If I had more time on my hands I might look into writing about something a bit more “in demand”, like Docker or .NET Core (random topics I’m currently interested in :P)." }, { "title": "Turning an old Android Phone into a Plex Media Server + PVR", "url": "/blog/2017/09/22/turning-an-old-android-phone-into-a-plex-media-server/", "categories": "Plex", "tags": "android, linux, plex", "date": "2017-09-22 21:55:53 -0400", "snippet": "This is possibly a solution to a problem no one other than me has, but once in a while I like to challenge myself to see if something ridiculous is possible.This time around I wanted to see if I co...", "content": "This is possibly a solution to a problem no one other than me has, but once in a while I like to challenge myself to see if something ridiculous is possible.This time around I wanted to see if I could take my old Moto X (2014) and use it as a Plex Media Server.I didn’t want to just see if I could install Plex though; I wanted to see if it would be possible to actually run a PMS instance along with Sonarr to download content automatically for shows I’m interested in.This introduced a couple of challenges, as Sonarr would need to be run using Mono (as it’s a .NET project), I’d need a Bittorrent client to actually download the content, and I’d likely need to be running a Jackett service to allow Sonarr to process request through sites like The Pirate Bay.Finally, as the Moto X is an ARM device, all of our software will need to be capable of running on an ARM platform.Note that this process (with some minor changes) can be used to get a Plex + Sonarr + Jackett + Filebot + Transmission setup done on any Linux distribution. I just thought it would be fun to do it using a phone.Setup the PhoneThis was the easiest part, as it simply required rooting the device so we could install SuperSU.With a factory reset instance and root access, the next step was to install BusyBox and Linux Deploy.Once both are installed from the Play Store, run Busy Box and select Install. You should only have to run this once, as it just installs a handful of Unix tools that Linux Deploy will need for the next step.Linux Deploy will require a bit more configuration as you need to tweak it a bit to suit your needs.First, select the repository you’d like. For my phone I went with ubuntu-lxde_arm, as this would setup an Ubuntu 16.04 LXDE distro along with VNC and SSH. Those will come into play when we want to start setting up Sonarr and Plex.Under the properties for the repository (the settings icon in the lower right next to the Stop button) I set the Installation type to Directory. This setting allows use to have access to all available storage on our device. The first time I tried this I used the default of File, which only gave me 2GB of storage to play with. Once the base installation was done, there wasn’t enough space left for me to do much. I tried tweaking the Image Size (MB) setting and starting over but it didn’t make a difference. Your milage may vary, so feel free to try other configurations.Next, from the top-right menu you need to Install the selected repository. This will do the basic installation of the Linux distribution you selected and pre-configure it for SSH and VNC access.Once installed, click on the Start button to boot up the image. After a lot of terminal scrolling, you’ll see something along the lines of &lt;&lt;&lt; start on the last line and the text will stop. This is our signal to start setting up our software.We’ve now got Linux running alongside Android on our phone. Let’s SSH into it and continue. The credentials are available under the repository properties. You can change these prior to installation or just use the defaults that are generated.Setting Up PlexThere isn’t an official ARM distribution of Plex, but thanks to some great work by Jan Friedrich, we can easily patch the Plex NAS distribution for ARM or ARM64.sudo apt-get install -y fakeroot gitgit clone https://github.com/uglymagoo/plexmediaserver-installercd plexmediaserver-installermkdir ../plex-installer; git archive master | tar -x -C ../plex-installer/cd ..; fakeroot dpkg-deb --build plex-installer ./This will create a Debian package that can be installed directly. For me, it produced plexmediaserver-installer_1.9.1.4272-b207937f1-2_armhf.deb, so installation was done using:sudo dpkg -i plexmediaserver-installer_1.9.1.4272-b207937f1-2_armhf.debThen, to start up your Plex Media Server:start_pmsYou can verify it by going to http://phone-ip:32400/web/index.html.I’m not going to set anything up here yet as we first need our media source available, which we’ll be doing next.Setting Up Sonarr and JackettSonarr can be installed from a repository list, so we’ll just set that up on our phone. Sonarr will also require Mono to be installed, which luckily for us is also available from Ubuntu compiled for armhf.sudo apt-get install -y libmono-cil-dev mono-devel libcurl4-openssl-devsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys FDA5DFFCsudo echo \"deb http://apt.sonarr.tv/ master main\" | sudo tee /etc/apt/sources.list.d/sonarr.listsudo apt-get updatesudo apt-get install -y nzbdroneBefore we configure Sonarr, we’ll setup Jackett. Jackett can be used to connect a torrent search site as a Torznab feed to Sonarr.For the purposes of this test I just setup The Pirate Bay, but Jackett supports a number of sites that may better suit your needs ;)To start Jackett, we’ll need to run it using Mono:mono --debug ~/Jackett/JackettConsole.exeJackett will be available at http://phone-ip:9117. Once you’ve configured a search site in Jackett, you can start Sonarr and begin configuration.mono --debug ~/Jackett/JackettConsole.exeSonarr will start up at http://phone-ip:8989. First we’ll setup an Indexer (from Settings -&gt; Indexers). This is where we’ll plug in the details we configured in Jackett. You’ll need exact url from Jackett, as well as the API key. This information is all easy to find from the Jackett interface.Next, setup a TV show from the Series section. Once added we’ll need to configure way to download content automatically when Sonarr detects a new episode. This is done under Settings -&gt; Download Client.Create a new Transmission client, give it a name and leave the default settings.The only issue now though is that we don’t have Transmission setup …Setting Up Transmission and FilebotBoth of these are going to be a bit challenging, as Filebot (which we’ll be using to move and organize our downloads) needs Java, and Transmission requires a display server (can’t be run from SSH).First, let’s install whatever we can from our existing SSH session:sudo echo \"deb http://ppa.launchpad.net/webupd8team/java/ubuntu precise main\" | sudo tee -a /etc/apt/sources.listsudo echo \"deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu precise main\" | sudo tee -a /etc/apt/sources.listsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys EEA14886sudo apt-get updatesudo apt-get install -y curl oracle-java8-installer transmissioncurl -L -O https://downloads.sourceforge.net/project/filebot/filebot/FileBot_4.7.9/filebot_4.7.9_armhf.debsudo dpkg -i filebot_4.7.9_armhf.debNOTE: if you get an error installing the key via apt-key, try the following:sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886In order to use Filebot, we’ll need a script we can call from Transmission. The script I’m working with is as follows:#!/bin/bashfilebot -non-strict -script fn:amc --output /home/android/Media --log-file /home/android/amc.log --action move --conflict auto --def music=n --def subtitles=n --def artwork=y --def backdrops=n --def clean=y \"seriesFormat= /home/android/Media/TV/{n}/{fn}\" \"ut_dir=$TR_TORRENT_DIR/$TR_TORRENT_NAME\" \"ut_kind=multi\" \"ut_title=$TR_TORRENT_NAME\"This script assumes that (a) your home directory is /home/android, and (b) you will be storing your media for Plex at /home/android/Media/TV. Feel free to modify the values to fit your particular installation.Let’s save this script as transmission-postprocess and make it executable using chmod +x transmission-postprocess.Next, we’ll need to actually VNC into our Linux distro. This is due to Transmission being a GTK application so we’ll need to interact with it in a desktop environment.From the desktop, launch Transmission and go to Edit -&gt; Preferences. We want to do two things now: Setup Remote Access Under Downloading setup the script we created above to be called when a torrent completesPutting It All Togetheropen in new tab to see full sizeGoing back to our Plex Media Server url, we can now setup a library that points to our downloaded media directory.When Sonarr detects a new episode, it will send the torrent link it finds from our TPB indexer to Transmission, and on completion Filebot will analyze the file and move it to the appropriate series/season folder for the show that was downloaded.Enjoy!" }, { "title": "Just Finished - Warcraft Adventures", "url": "/blog/2016/11/29/just-finished-warcraft-adventures/", "categories": "Gaming, Just Finished", "tags": "adventure, pc", "date": "2016-11-29 03:36:22 -0500", "snippet": "Although I’m a huge fan of the point-and-click adventure genre, I’ve been primarily focusing on covering 16-bit RPGs in my spare time. I’ve still got a sizeable backlog to wade through, but when th...", "content": "Although I’m a huge fan of the point-and-click adventure genre, I’ve been primarily focusing on covering 16-bit RPGs in my spare time. I’ve still got a sizeable backlog to wade through, but when the news dropped that Warcraft Adventures: Lord of the Clans had been leaked, I couldn’t resist.Although no one really got a taste of this game until 2010 when MAN-biker posted some content to Youtube, I’d been inquiring with Blizzard as early as 2008. I still have a copy of the email, so I’ve trimmed it down a bit and shared it below.From: alexbevi@gmail.com To: web.support@blizzard.comSent: 12/19/2008 11:58:55 AMSubject: [en]Warcraft -- OtherThis isn&#39;t really a support question, and I have no idea how to route thisproperly, but the question I want to ask is as follows:Over the years, I&#39;m sure there have been numerous attempts by fans to get Blizzard to revive the Warcraft Adventures project. The statement made when it was cancelled was that it didn&#39;t meet the quality requirements that Blizzard was trying to keep for it&#39;s users; and over the years, those decisions have resulted in excellent titles.Regarding Warcraft Adventures though, it&#39;s been 10 years since this project was cancelled. I don&#39;t have any inside information into the level of completion the project was at, but if it was &#39;near&#39; completion, couldn&#39;t this be released to the open-source community to be added to a project such as ScummVM? The benefit here would be positive PR for Blizzard, as well as renewed interest in their catalog of games to a number of new users.I don&#39;t represent the ScummVM team, but I&#39;d be interested to know what you guys think. -Alex===============================================================================From: &lt;petern.support@blizzard.com&gt;To: &lt;alexbevi@gmail.com&gt;Subject: Re: [en]Warcraft -- OtherDate: Mon, 22 Dec 2008 18:49:29 -0800Greetings Alex,Thank you for emailing the Blizzard technical support department in regards to your question. I know the same question has been asked by others in the past and unfortunately there just is not information in regards to the game that I have available to give besides what is already available on the internet. Unfortunately I have no way to request any further information about the game or items such as art resources or code from the game. The best place I can suggest to post such a request is on our suggestions forum at http://forums.battle.net/board.html?forumId=12016&amp;sid=3000. This is by employees that have more information, and more access to information than we do here in technical support, if there is a large enough interest in anything then those ideas are passed along to those that may be able to do more. I cant guarantee anything unfortunately but this is the best location I can suggest to try for such a request.Regards,Peter N.Technical supportBlizzard Entertainmenthttp://www.blizzard.com/supportIf you reply, please include all previous text and files related to this e-mail. Please note the Tech support email system does not support read receipts.My hope was to get access to the source code for the ScummVM team to be able to incorporate as a custom engine.Unfortunately (though not unsurprisingly), Blizzard wouldn’t release the code.Now, once word got out that the complete game had been released (not legally I know …), I had to grab it and give it a shot.Being a game from the late 90’s, I didn’t expect it to just work. My primary machines run Linux, so I first tried to run this through wine, which didn’t end up working reliably.I then built a Windows XP VM using VirtualBox. This managed to runthe game with no issues.StoryAs per the Gamepedia article:Basically, after the Dark Portal was destroyed and the rift between the worlds was destroyed, you had a large group of orcs that were trapped on Azeroth. And over the course of the next few years, the humans, being merciful in their ways, instead of hunting down and eradicating these orcs, granted them land areas where they could live as long as they lived within the confines of societal expectations. Basically, they were put on these reservations or camps. And because they were made to live in a way that was very contrary to their basic nature, a lot of the spirit and fire that defines them as a culture was drained out of them. And so what you found yourself with was an orc society in Azeroth of forced passivity, not forced through violence but forced through situation.Although down on their luck, the orcs in Warcraft Adventures were supposed to experience a rebirth, thanks to the leadership of Thrall.Our storyline followed an orc baby that was taken from a battle scene where his parents were slain and raised by a human lieutenant, Blackmoore, with the intention of raising him with human ideals but being able to use him to control the orcs. Definitely someone who does not fit into the general stereotype of noble humans. He was a self-serving, dark human character who wanted to raise this orc, Thrall, our central character in the game, and use him to control and command the orcs and then raise them as his own private army. Thrall, though he’s raised in captivity by humans to serve their will, still has some fire within him that he can’t deny, so he rebels against his human owners. He escapes the compound where he’s being held, and then over the course of the game, what we do is follow his adventures. As he discovers more about himself and the orcs and what it means to be an orc, so does the player. As you go through the game, you meet some familiar faces from the games, some in retirement, some trying to lead an underground resistance, and you learn of what happened to the Frost Wolf Clan, which was the clan that Thrall’s father Durotan was a part of. You learn that Durotan, Blackhand and Doomhammer were three blood brothers, and that his clan Frost Wolf was sent into the Dwarf Highlands in the mountains. They were exiled there by a plotting Ner’zhul, when he was pulling the strings in the background behind Doom Hammer and Black Hand, because he knew that Durotan was a threat.GameplayWhen I first started playing WCA, it was a nice trip down memory lane. The gameplay mechanics really seemed to borrow heavily from Full Throttle, which was one of my favourite point-and-click adventures in the ’90s.Right-clicking brings up a graphical command menu that lets you talk, look or use.You can interact with the environment based on designated hotspots (which are identified when hovered by showing a label).Thrall will usually try to respond with a funny comment whenever possible, which makes it more compelling to click around and see what he has to say about his surroundings.PuzzlesAs with most adventure games of this era, the puzzles are predominantly fetch-quests and inventory manipulation quests. Again, if you grew up with these games this type of mechanic is old hat, but if you’re new to the genre, this can seem a bit tedious.I found that the puzzles were a bit on the easy side (until the last 10 minutes of the game). This could just be because I’m used to these sorts of games, but until I need to dig around in the dragon’s stomach for the pig’s jetpack, everything was straightforward and I didn’t feel the need to look to Google for the answer.OverallAs a throwback to ’90s point-and-click adventures, I’d say WCA is worth playing. If you enjoyed the genre, this is a no brainer, and if you’ve never played this type of game, this is a pretty easy entry.The version of the game that was leaked is pretty close to being completed. I didn’t notice any issues or major omissions.The quality of the FMVs are pretty low compared to other games from the late ’90s. The pacing of the game isn’t too slow and the story is interesting enough to make you want to see it through to the end.Playing Warcraft Adventures actually made me want to revisit some other adventure games. I get the urge to play through Day of the Tentacle every 4-5 years or so, and I think it’s been almost that long …" }, { "title": "Summer RPG Update 2016", "url": "/blog/2016/09/05/summer-update-2016/", "categories": "Writing", "tags": "jrpg, rpg, snes, gba", "date": "2016-09-05 14:08:29 -0400", "snippet": "I don’t really have much progress to report for the last month, so I thought I’d throw out a few of the games I’ve been working through to see if I can generate any interest in the next article ;)F...", "content": "I don’t really have much progress to report for the last month, so I thought I’d throw out a few of the games I’ve been working through to see if I can generate any interest in the next article ;)First off, I’ve been playing through Golden Sun for the GBA.I really like the psyenergy system and how you use it for puzzle solving. It makes this game a bit more of an ARPG along the same lines as Lufia 2.I’ve made it as far as Kraken, but I’m finding beating him is proving to be more difficult that I’d expected. Probably going to have to back out and grind for a while, which I don’t have a lot of time for …Next, I’m pretty sure I’ve given up on the 7th Saga. I’m still at Telaine with a party at level 17, but I’m pretty sure that to progress, there is grinding in my future.Similar to the situation I’m having above with Golden Sun, i just need to set many hours aside to level up in order to proceed, but with the amount of time I’ve got these days to devote to these games, that may never happen.Finally, I’ve decided to bite the bullet and dive into Super Mario RPG. This, along with Final Fantasy Tactics is one of those games that I’ve heard nothing but good things about for the last 20 years, but have never bothered to investigate myself.I’m currently at level 10 with 3 star pieces recovered and have just rescued Princess Toadstool and had her join the party.I’m really enjoying the story and gameplay, but since this was developed by Square, I’m not surprised that this is right up my alley :PI’m most likely going to be able to finish this game off, and then circle back around to Golden Sun, as I really want to beat that game.Now, assuming I can make some progress on those two games, I’d like to start getting some feedback on where to take this series next.There are a number of games that I’ve finished previously that I didn’t write about. Some of the more notable ones would be: Mother 3 Final Fantasy VIII Final Fantasy IX Chrono TriggerThen there’s the list of games that I really want to tackle, but since they all require a sizable time investment I’m not sure which to hit up: Xenogears Breath of Fire 3 Shin Megami Tensei: Persona 3 - I’d likely play the PSP release Danganronmpa - I started this a while back but never finished it Shin Megami Tensei 2 - There’s a great fan translation of the Super Famicom release Final Fantasy IV: After Years Vagrant StoryIf you’ve got any insight into which I should tackle next, feel free to leave me a comment below ;)" }, { "title": "Just Finished - Dragon Quest I", "url": "/blog/2016/07/27/just-finished-dragon-quest-i/", "categories": "Gaming, Just Finished", "tags": "rpg, jrpg, snes", "date": "2016-07-27 22:26:01 -0400", "snippet": "I actually finished this game around the end of June, but haven’t really had a chance to write about it until now. I’m currently sitting out by the lake at the cottage and figured it was about time...", "content": "I actually finished this game around the end of June, but haven’t really had a chance to write about it until now. I’m currently sitting out by the lake at the cottage and figured it was about time :PThe last time I played through Dragon Quest was likely in the late 80’s. I’m pretty sure I either heard about it, or got a copy of it through Nintendo Power, and like most kids in North America at the time, this would have been my first introduction to what would become known as JRPGs.I decided to go with the SNES re-release of this title for my playthrough as I wanted some updated graphics and music. Although the SNES version was never officially localized for North America, there is an excellent fan translation available.It threw me off a bit when they refer to the legendary hero as Roto (as opposed to Erdrick), but I’m assuming this is a better translation than what we got in the 80’s so that’s probably what the name should have been in the first place.Dragon Quest was essentially the first JRPG. As a result, there are a lot of concepts introduced here that would be adapted and refined by other titles and series over the years. This means that some parts of this game feel a bit rough or unbalanced.I originally played the NES version of this title, and have fond memories of just how brutally hard it was. This game introduced me to what I would come to know as “grinding”, as you couldn’t progress through the game unless your character was sufficiently leveled up in order to tackle the monsters in the areas you were exploring.This meant walking back and forth and fighting random monsters.Enemy encounter rates are high. Very high. I think the SNES version actually optimized this a bit, but the rates are still high. This is useful for grinding, but gets tedious when you want to explore, or really need to get back to a town to heal.The story is pretty simple.You’re the descendant of the legendary hero, and have been tasked with rescuing the princess and defeating the Dragon Lord.To defeat the Dragon Lord, you need to get three pieces of equipment of the legendary hero which are scattered across the world.Beat the dragon. Save the princess. Beat the Dragon Lord. Done.Gameplay is pretty straightforward. You start off in the castle, where the king explains your quest. You can talk to people in the castle, or leave and move on to the nearest town.Here you’ll get a bit more info, or can buy supplies.I think it’s kind of cool that the first time you leave the castle you can see the Dragon Lord’s castle on the world map. This gives you your target right away and you know what you’re working towards.You interact with NPCs and the world via a command menu. One of the nice additions to the SNES version is that you no longer have to use a STAIRS command to go up and down stairs; just walk over them.As you roam around the overworld and dungeouns, you’ll get into random encounters. You interact with these using a command menu as well, where you can select to either attack, use magic, use an item, or run.Dragon Quest is a pretty short game. Being one of the first of this genre it introduced a lot of gameplay elements that would shape the genre, but when you look at it as a standalone title, there’s not all that much there.This isn’t meant to be a knock at the game. I had a great time playing through it, but that playthrough was only a few hours.I used an emulator to do this, so most of my level grinding was done in fast-forward, so it felt even faster to get through this title.If you’re playing the “classics”, I’d definitely recommend this title. Playing these early games gives you more of an appreciation for the later RPGs, as well as giving you a feel for how it all started.Have you had a chance to play this title? How do you think it holds up nowadays, especially with the release of the mobile ports? Have any DQ memories you want to share? Let me know in the comments." }, { "title": "Just Gave Up On - Shadowrun (Genesis)", "url": "/blog/2016/07/26/just-finished-shadowrun-genesis/", "categories": "Gaming, Just Gave Up On", "tags": "rpg, genesis", "date": "2016-07-26 05:25:39 -0400", "snippet": "UPDATE: 2016-09-05 - It was pointed out on the Reddit thread and in the comments below that I might have used a cheat which prevented finishing the game. I didn’t think I’d used one, but I think wh...", "content": "UPDATE: 2016-09-05 - It was pointed out on the Reddit thread and in the comments below that I might have used a cheat which prevented finishing the game. I didn’t think I’d used one, but I think while doing research for the article i may have been testing it out :(Note that you don’t really need to cheat in this game to get more Nuyen; just grind ;)Thanks to those who responded. It’s worth noting that if you’re going to play this game, avoid the cheat menu as it will bite you in the ass.As I work my way through the 8-bit and 16-bit games I grew up with, there are (and will continue to be) those that I just can’t get through. Shadowrun for the Sega Genesis turned out to be one of those titles.I’ve made it all the way to the final boss, and though he doesn’t beat me, I empty all my clips (and my runner’s clips) into him and just can’t seem to kill him and trigger the endgame sequence.This is extremely frustrating as I’ve effectively finished this game, but if I can’t beat the last boss I can’t in good conscience call this a “Just Finished” article.Regardless, I’ve gone through enough of this game to be able to write about it, and based on my experience, recommend it to other players looking for some good retro cyberpunk action-RPG gaming.This iteration of Shadowrun doesn’t resemble the Super Nintendo’s version of this title at all.First off, you begin by selecting the type of player you want to run the game as: Samurai, Decker or Street Shaman. Each of these classes will give you different starting abilities and stats.I chose to run as a Decker, as I wanted to get an idea as to how the matrix sequences differed between the games (plus I’m a programmer and felt drawn to this class :P).The story starts off with you at a hotel trying to track down some information about what happened to your brother. You interact with characters in this game through conversation trees (select A, B, C) in order to advance the story or get clues.A good chunk of the game is presented through these conversation views.Once you leave the hotel, you start on your adventure into the world of Shadowrun.Your character can walk around the initial city and go in and out of various buildings in order to trigger additional conversations with NPCs. It’s a good idea to get a lay of the land as you’ll be visiting each of these building repeatedly once you start shadowrunning, so it’s worth taking the time to explore.Some buildings will warn you that they contain ghouls. These buildings are where you can go to do some level grinding, or if you’re on a run that has you clearing out ghouls, earn some money.To level grind, you’ll just be earning karma points which can be applied to the various stats your character has. These stats affect your weapons skills, magic skills, tech/computer skills or your aptitude with various weapon classes.Although I started out as a Decker, I found myself maxing out my weapon skills early on to make the game a lot easier to progress through. It turns out there really isn’t any demand for low level Deckers, so I didn’t really have an opportunity to do any Matrix runs until much later in the game.On the subject of shadowruns, the way you earn money (nuyen) in this game is by taking on various jobs (runs) from Mr. Johnson’s. There are one or more Mr. Johnson’s in each city you visit and they offer you work. Initially the types of work are either “escort to \", \"take package to \" or \"clear out ghouls from abandoned building\".I found that the easiest runs to make money at early on where the ghoul runs. You basically just go into the building, find some ghouls and start running in circles. They’ll just follow you so you can pick them off one at a time.Although this became tedious pretty quickly, you get money and karma and can level up your weapon stats in order to make repeating this easier.The escort runs are easy, and if you took the time to get an idea where each target is, it’s even easier to do. It just takes time and the payout isn’t that great.Eventually you’ll earn enough money to buy some info about where to go next and you have the opportunity to travel to a new city.There are a handful of cities you can visit in this game, and each one offers it’s own set of challenges, as well as opportunities for more advanced runs.You’ll also meet other shadowrunners in bars and clubs throughout the game. These runners will give you more information about shadowrunning, the story, as well as background about the Shadowrun world.These characters can also be hired to assist you on your runs. Each one has two rates; a single run rate or a lifetime rate.I found I didn’t really hire anyone until I hit the endgame as I didn’t really need the help. If you’re playing through the game as a shaman or samurai and don’t bother upgrading your computer skills, it may be easier to hire a good decker than to upgrade your own stats and get a cyberdeck.You’ll meet new Mr. Johnson’s who offer more difficult missions that involve traveling between the various cities in order to do more difficult quests such as infiltrating corporations to either steal secrets or extract employees, or running the matrix.Both of these advanced run types offer their own challenges and require abilities to be leveled up in order to proceed.When infiltrating corporations, you’ll need to be able to have electronics skills in order to be able to break maglocks, and you’ll want your weapon skills upgraded so as to be able to shoot guards without attracting more.The infiltration jobs have you going through multiple floors of these buildings, with each floor having guards to deal with, security cameras to avoid and wall safes to plunder.It helps to have computer skills when doing these runs as you can hack the computers to disable cameras, turn off alarms and open doors.This takes us to decking and matrix running.Unlike the SNES version of this game, the Genesis version actually has a bit more depth to the matrix.You’ll need to have computer skills first, then also have computer software for fighting with subsystems and identifying and avoiding various threats.The fighting itself is pretty repetitive, but the concept is fun.There are various types of components within a matrix system. You have to hack each of these as you move deeper into the system. The end goal is usually to hit a data store and download or delete a file.One way to easily earn money is to hack into a system, download the maximum number of data files then try to sell them to a contact in [city]. Near the end of the game i was able to get upwards of 10K for some files, which made upgrading my equipment a lot easier to do ;)Although I’ve been a huge fan of the SNES version of this game since I was a kid, I think I may have enjoyed the Genesis incarnation more. The music wasn’t nearly as good, but the story was well laid out, there were a lot of options on how to progress and it didn’t feel nearly as linear.The addition of the classes gave you a sense of depth and replay-ability, though chances are you could just max out all stats for any player type and have same experience no matter how you started the game.I also really enjoyed the shadowrunning aspect of this game. In the SNES version you could hire runners to help you get through the game, but you didn’t actually participate in shadowruns for profit.If you haven’t given this game a shot, I would highly recommend it. It turned out to be a lot of fun, and an excellent addition to the limited field of cyberpunk games out there.If you have any recommendations for other games in this genre I should check out, or if you want to share your opinion on this or the SNES title, please feel free to leave me a comment.Gallery" }, { "title": "Extracting Best ROM from GoodTools Generated ROM Sets", "url": "/blog/2016/07/25/extracting-best-rom-from-goodtools-generated-rom-sets/", "categories": "Scripting", "tags": "linux, roms", "date": "2016-07-25 15:17:38 -0400", "snippet": "As a kid of the 80’s, I have fond memories of all the old 8-bit and 16-bit consoles that I grew up with.Although it’s easy enough to find ROMs, I tend to find myself going for the GoodTools generat...", "content": "As a kid of the 80’s, I have fond memories of all the old 8-bit and 16-bit consoles that I grew up with.Although it’s easy enough to find ROMs, I tend to find myself going for the GoodTools generated sets more often than not as they’re considered “complete”.This is kind of ridiculous as I don’t speak Japanese, which constitutes the vast majority of the contents of these sets.Even though most emulators support compressed ROM sets, I’d prefer to just have the English ROMs available on their own in one place.As a programmer, I thought “How can I do this in Linux?”, but more specifically, “how do I do this from the command line directly?”.# extract best rom to directory# best contains !7z e \"*.7z\" -o../../ *[!]*.* -r# purge all non US/Europeanfind . -type f ! -name '*(U)*' ! -name '*(E)*' -delete# purge duplicates where a (U) exists alongiside an (E)for f in *\"(E)\"*; do us=`echo $f | sed -r 's/\\(E\\)+/\\(U\\)/g'`; if [ -e \"$us\" ]; then echo \"FOUND $us - removing $f\"; rm \"$f\"; fi; doneIf you find yourself with compressed ROM sets and you want to just grab the English ones, this might just come in handy ;)" }, { "title": "Redmine Knowledgebase 3.2.0 Released", "url": "/blog/2016/06/22/redmine-knowledgebase-3-dot-2-released/", "categories": "Redmine", "tags": "redmine", "date": "2016-06-22 21:33:34 -0400", "snippet": "I haven’t been very actively involved with this plugin or the Redmine community as a whole lately, but it would seem there is a very active user-base still logging bugs and enhancing this project.Y...", "content": "I haven’t been very actively involved with this plugin or the Redmine community as a whole lately, but it would seem there is a very active user-base still logging bugs and enhancing this project.You can grab a copy of the release on GitHub.I’m pushing out version 3.2.0 of the plugin thanks to the efforts of some very dedicated community members, who I’d like to highlight below:Thanks to Frederico Camara: updating acts_as_rated to work with Redmine 3.2.xThanks to Eduard Kuleshov: getting this plugin supported in Redmine 3.0.xThanks to Axel Kämpfe: getting this plugin supported in Redmine 3.1.x and 3.2.xHUGE thanks to Rob Spearman for basically taking over the project and pushing it forward:New Configuaration options Show articles without tabs Show attachments before article content Show thumbnails for articles in lists Show breadcrumbs for articles in listsNew permissions Article history will only show up if have view permission optional permission for users to manage just their own articles. (#306)Layout Sort Tags on the index page Added authored view so users can find articles by author easilyBug Fixes article view counts not updating (#304) top rated list not valid (#305) ActiveRecord::StaleObjectError (Attempted to destroy a stale object: KbArticle) (#300) Error when generating a PDF of an article with pictures (#308) 500 Internal Server Error - if DELETE category but it’s have subcategory (#293)Note that this is a preliminary release as there is one bug in here that I haven’t squashed.When trying to search, you’re prompted with a failure similar to:Started GET \"/search?utf8=%E2%9C%93&amp;q=stuff\" for 127.0.0.1 at 2016-06-23 01:14:06 +0000Processing by SearchController#index as HTML Parameters: {\"utf8\"=&gt;\"✓\", \"q\"=&gt;\"stuff\"} Current user: admin (id=1)Completed 500 Internal Server Error in 632ms (ActiveRecord: 173.6ms)NoMethodError (undefined method `where' for #&lt;Hash:0x000000096b0d60&gt;): lib/plugins/acts_as_searchable/lib/acts_as_searchable.rb:93:in `search_result_ranks_and_ids' lib/redmine/search.rb:127:in `block in load_result_ids' lib/redmine/search.rb:125:in `each' lib/redmine/search.rb:125:in `load_result_ids' lib/redmine/search.rb:115:in `block in load_result_ids_from_cache' lib/redmine/search.rb:114:in `load_result_ids_from_cache' lib/redmine/search.rb:99:in `result_ids' lib/redmine/search.rb:70:in `result_count' app/controllers/search_controller.rb:65:in `index' lib/redmine/sudo_mode.rb:63:in `sudo_mode'I’m pretty sure this has to do with how we’re setting up acts_as_searchable in the kb_article model. Any suggestions welcome ;)" }, { "title": "What's Up - May/June 2016", "url": "/blog/2016/06/14/whats-up-may-slash-june-2016/", "categories": "Writing", "tags": "anime, games", "date": "2016-06-14 20:38:02 -0400", "snippet": "It’s been a while since I posted the Seiken Densetsu 3 review, so I figured as it might be a while before I get around to posting more content, I’ll just throw up a quick update.GamingI’m currently...", "content": "It’s been a while since I posted the Seiken Densetsu 3 review, so I figured as it might be a while before I get around to posting more content, I’ll just throw up a quick update.GamingI’m currently working my way through three games. After finishing the Shadowrun play-through for SNES, I got some good feedback about the Genesis version. I decided to give that a go, and I’ve currently made it to the last boss, but am having no luck beating him.I’m pretty sure I’m going to have to save up some nuyen and grind it out with some better runners in order to be able to beat Thon. This is a bit of a pain in the ass, and is the reason I ended up abandoning Terranigma previously.I really enjoyed that game, but since I’ve got very limited time decided to move on to the next challenge.By challenge, I meant CHALLENGE. I decided to give The 7th Saga another shot, and have now officially made it MUCH further than I ever have before.This game was alway brutally hard for me when I was a kid, and it still is. You’ve got limited inventory for healing items, everything is expensive and the enemies are generally overpowered.On top of that, it takes forever to level up, and when you do it doesn’t really make all that much of a difference.I’ve made it as far as Telaine and have the Wind, Water and Star runes. Still a ways to go, but I don’t know that I’ll continue …Finally, since I haven’t played through this since the late ’80s, I’m diving back into Dragon Quest. I’m going to be playing through the fan-translated SNES port, which I think is the superior port (I don’t like the mobile remakes of these games …).I think I should have this one done relatively soon as it’s not a very long game, and although it’s a bit grind-heavy, I’ve got 8x fast-forward on my emulator :PAnimeJust a quick update on the series I’ve burned through recently.Log Horizon - Season 1This reminded me or SAO a bit, but with a lot less “Asuna! Kirito! \", and a lot more \"Lens flare from glasses\" and \"Pushing glasses back into place\".Story was enjoyable. Characters were interesting. I’d recommend it.Darker than Black - Season 1+2REALLY enjoyed this. Good action. Interesting plot. Good dialog. I’d recommend it.One Punch Man - Season 1I LOVE this. Saitama is OP AF!!! The story is a bit goofy, but the characters are interesting and the plot keeps you interested. Would totally recommend this.I think that’s all for now." }, { "title": "Just Finished - Seiken Densetsu 3", "url": "/blog/2016/04/19/just-finished-seiken-densetsu-3/", "categories": "Gaming, Just Finished", "tags": "jrpg, snes, rpg", "date": "2016-04-19 14:30:20 -0400", "snippet": "Seiken Densetsu 3 (聖剣伝説3) is the third installment in the Mana series. It’s the sequel to Secret of Mana, which is the entry into the series that most North American gamers would be familiar with. ...", "content": "Seiken Densetsu 3 (聖剣伝説3) is the third installment in the Mana series. It’s the sequel to Secret of Mana, which is the entry into the series that most North American gamers would be familiar with. Once again, I played through this game on my phone using the fantastic Snes9x EX. Please support this dev as he provides a fantastic product. For some reason though, most of the screenshots I took came out stretched. This hasn’t happened before, and I’ve been playing a couple other games that this isn’t happening for either, so I’ll chalk this up to bad luck : NOTE I ran these screenshots through pngcrush (ls *.png | while read line; do pngcrush -ow -brute $line; done) to get the size down a bit ;)I’m leaving these images in here as they were part of my “journey”, though hopefully they don’t deter anyone from playing this game, as the game is fantastic.First off, if you’re a fan of Secret of Mana, the initial “feel” of the game will be familiar, as will the art style and sound.Whereas SoM had you gradually encounter the two other playable characters at set points in the game, SD3 has you selecting your 3 main characters before you even begin.I decided to go with Duran, Carlie and Kevin (something about a werewolf named Kevin made me chuckle :P). Duran’s story starts off with the castle where he lives being invaded by a wizard who almost kills him. The wizard lets Duran live, so naturally Duran has to set off on a quest to find and defeat the wizard.On this journey, he’ll run into the other characters I selected (Kevin and Carlie), and each of their stories will mix into Duran’s. This is actually a really cool element of this game as it gives it a lot of replay value. No matter what combination of characters you pick, their stories will all overlap. Also, you’ll run into all the other characters you didn’t pick, so every character makes an appearance throughout the game.You travel around the game mostly on foot initially, but eventually new methods of travel will open up. As with SoM, you can get to certain areas via Cannon Travel (though there’s a couple of fetch-quests before you can actually take advantage of this). There’s also a ship, a weird turtle thing, and eventually Flammie the dragon.The combat system is reminiscent of SoM as well, though this time around it feels a lot more “button mash-ey”. I found that I just spammed the B button throughout most of this game and didn’t really use any strategy during combat; aside from healing periodically.You get experience by killing enemies, which will allow you to level up your characters. Leveling up lets you increase various stats, that should make your characters perform better in future battles.As you make your way through the game, you’ll start finding these pillars that are guarded by stronger enemies. These are the Mana stones, and apparently contain God-beasts that will be awakened if you shatter the stones.These stones can also be used to change your character’s class. This is another cool aspect of this game, as it give the leveling system a bit more flavour. You can change classes twice in this game; once at level 18, and again at level 38. Each of these classes is aligned with either “Light” or “Dark”, and gives the character new skills/improvements.The game can feel a bit “grindy” due to the need to constantly trudge back and forth between areas, and there are enemies on each screen. These battles are all avoidable, but you’ll want to level up for the boss fights, and eventually the God Beasts.While you’re trekking though, you get to enjoy some really good background music. If you like Secret of Mana, you’ll love Seiken Densetsu 3. You can have a listen over at YouTube to get an idea what’s in store for you ;)The artwork is also top notch. It’s really a shame this game never made it to North America as this is possibly one of the best looking SNES games out there. The background design, character design and monster sprites are all very, very well done. There is a lot of variety to the various regions of the world.Once you’ve shattered all the Mana stones, you unleash the God Beasts. Their position is shown on the world map, and they can be fought in any order, but it’s a good idea to start from the top left of the map and work your way through them sequentially.I learned this the hard way as I only play this game on occasion, and ended up forgetting where I was. This resulted in A LOT of backtracking to find the last couple of beasts.I thoroughly enjoyed playing this game and would highly recommend this to anyone out there that likes JRPGs, action RPGs and 8/16-bit gaming in general. The graphics, music and gameplay are all very well done, and the story lines are interesting, even if there is occasionally some cheesy dialog :PI finished this game over a month ago, so I’m still pretty slow cranking out these reviews …I’m currently working my way through The 7th Saga for SNES since I clearly need more pain and frustration in my life :P When that game is annoying me, I’m also working on Sega Genesis version of Shadowrun.That game was recommended by some of my readers on Reddit so I figured I had to give it a whirl. So far I’m actually really enjoying it, and it is VERY different the the SNES game of the same name.Hopefully I’ll have more for you in the near(er) futureGallery" }, { "title": "Redmine Plugin Extension and Development is Apparently Still Relevant", "url": "/blog/2016/03/23/redmine-plugin-extension-and-development-is-apparently-still-relevant/", "categories": "Writing", "tags": "redmine, book", "date": "2016-03-23 20:59:24 -0400", "snippet": "For the first five quarters that this book was out in the wild, I was posting publication numbers any time I got a royalty statement from Packt Publishing for Redmine Plugin Extension and Developme...", "content": "For the first five quarters that this book was out in the wild, I was posting publication numbers any time I got a royalty statement from Packt Publishing for Redmine Plugin Extension and Development.I haven’t been very active in the Redmine community in the past year or so as I’ve been a bit busy with work, contracts and life, but after seeing the last batch of sales figures, it seems people are still interested in this topic.   Ebook Mini Subscription Packtlib Mini Print Book Mini Subscription (3rd Party Mini) Q4/2015 30 8 18 3 Q3/2015 24 3 15 4 Q2/2015 30 2 18 3 Q1/2015 90 0 25 2 Q4/2014 33 2 31 2 Q3/2014 25 2 24 4 Q2/2014 73 1 33 2 Q1/2014 18 3 18 0 As was made clear with the above info, there is steady demand for this book, so there must be quite a few plugin authors out there looking to get started with the Redmine platform.I’m going to try to get my knowledgebase and dropbox plugins updated in the next couple of months, but if there is anything Redmine-related people might be interested, please leave your comments below ;)" }, { "title": "Just Finished - Shadowrun", "url": "/blog/2016/02/23/just-finished-shadowrun/", "categories": "Gaming, Just Finished", "tags": "rpg, snes", "date": "2016-02-23 20:20:11 -0500", "snippet": "I had this game finished around Christmas, but I find my motivation for writing these articles is starting to languish. I’m hoping to get that spark back with the next couple of titles i’ll be tack...", "content": "I had this game finished around Christmas, but I find my motivation for writing these articles is starting to languish. I’m hoping to get that spark back with the next couple of titles i’ll be tackling, even though Shadowrun was one of my favourite SNES games of all time.You wake up on a slab in the morgue. You don’t know who you are or how you got there. That’s pretty much all you’ve got to go on when you start this game for the first time.The intro shows you being gunned down, then a shapeshifter casts a spell on you and takes off. After getting out of the morgue, you run into “Dog”, who appears to be a shamanistic totem.This was the first 10 minutes of this game, and I remember playing this back in ‘94 and being hooked at this point.Shadowrun is a cyberpunk action-RPG for the Super Nintendo. You control Jake Armitage, a courier who has something important uploaded to his head computer.You move around in an isometric fashion. You can interact with certain people by talking to them, or occasionally giving them a key item from your inventory.When talking to NPCs, some words will appear in bold. These can then be used as keywords in future conversations. These NPC interactions are how the story is progressed, and you’ll spend a fair amount of time in this menu over the course of the game.Once you learn the Hiring keyword, you can also try to hire additional shadowrunners. When they join your team, they will fight along side you … for a while. If you’ve increased your leadership levels, they may stay longer.Leveling up is done by spending karma when you rest on a bed (which also acts as a save point). You can spend your karma to boost various stats, or to level up your spells once you learn magic.Karma is gained by fighting, which is done by moving a set of cross-hairs across the screen, and when placed over an enemy, mashing the A button. You then fire, wait, fire, wait … etc.The combat in this game is pretty simple. Just stand in one place and fire. Once you’re in combat mode, you can move your character, so there’s not really any strategy (other than healing on occasion).The same system is used for spells and items. You select the (spell/item) from the menu, the cursor becomes a hand icon, and you hover over your target and press a button again.Navigating the screen using the D-pad can be a bit cumbersome; especially when fighting, but you get used to it after a while.The game seems to be made up of a series of fetch quests that are used to get you from NPC to NPC in order to uncover more keywords, to then be used with other NPCs to trigger more fetch quests. Oh, and move the story along as well ;)The game is split up into 3 main areas that you can navigate to by train, one area you get to by boat, and the volcano you get to by helicopter. These areas are all opened up as the story moves along and you slowly uncover who was responsible for trying to geek you at the beginning of the game.As you fight your way through the various areas, you’ll find out that you were a courier with sensitive information in your head computer. You have a datajack in your head which is not working, so you can’t access your brain by computer in order to get this information.When you go find a street doc to fix your datajack, it triggers a cortex bomb that then needs to be defused. After this is done, you get some information about what’s in your head and you can start decking (hacking) into computers.The decking segments are actually pretty dumb, but I love them. This is really what has kept me going back to this game time and again over the years, as I just really loved the concept.You’re physically wired into this machine, and have to battle various security countermeasures that have been installed in these systems. The battles are done by guessing when there is a hidden enemy on a grid and pressing the A button. Based on an RNG value and your computer skill level, you’ll either miss or destroy the enemy.That’s it. That’s all there is to it. I still love it though :PDepending on why you were hacking in, you’ll either trigger an elevator or get part of a data file. The data files are plot devices that give you bits of the story, or contain phone numbers you can use from vidphones throughout the game.After you’ve finally fought your way through the various sections of the game, you finally get to face off against Drake. He’s apparently the one behind you almost being whacked. You beat him up, head back to Akimi building, fight your way up a handful of floors (fighting and decking) and finally find the AI you need to destroy.Once done, things explode, and you can close the books on this adventure.The game alludes to there being a potential sequel in the works, but this never materialized. There was another Shadowrun title for Sega Genesis, but this was a completely different game. I might play through this and do a write up one day, but I’ve tried playing it a couple times over the years and could never get into it (if you think I should, or want to share an opinion, there is a comments section below … :P).Overall I still love this game. This is where I was introduced to cyberpunk as a kid, and I even went as far as getting the Shadowrun books (Never Deal with a Dragon) that this game was very loosely based on.The music was also a fantastic complement to the dark atmosphere of the game. 3AM On A Summer Night might actually be one of my favourite tracks from the 16-bit era that wasn’t composed by Nobuo Uematsu ;)If you’re revisiting the SNES and looking for something you may have overlooked during the mid ’90s, pick this up and give it a spin.Next up, Seiken Densetsu 3. This one will probably take me a while to complete, then a longer while to actually do a write up on.Gallery" }, { "title": "Recovering a WiredTiger collection from a corrupt MongoDB installation", "url": "/blog/2016/02/10/recovering-a-wiredtiger-collection-from-a-corrupt-mongodb-installation/", "categories": "MongoDB", "tags": "mongodb, wiredtiger, data-corruption, troubleshooting", "date": "2016-02-10 20:38:38 -0500", "snippet": " April, 1 2019: I’ve received a LOT of feedback on this article since it was published. I would like to point out that although the methods described here may still work, MongoDB introduced a --re...", "content": " April, 1 2019: I’ve received a LOT of feedback on this article since it was published. I would like to point out that although the methods described here may still work, MongoDB introduced a --repair flag in 4.0.3 that simplifies this process significantly. I would recommend reading their “Recover a Standalone after an Unexpected Shutdown” tutorial to see if it applies to your recovery scenario.Recently at work, we experienced a series of events that could have proven to be catastrophic for one of our datasets. We have a daily process that does daily cleanup, but relies on the presence of control data that is ETL’d in from another process.The secondary process failed, and as a result, everything was “cleaned” … aka, we purged an entire dataset.This data happens to be on a 5 node replicaset (primary-secondary-secondary-arbiter-hidden), and the hidden node died over the holidays and I waited too long to recover it, so it was unable to ever catch up to the primary (always stuck in a RECOVERING state).My incredible foresight (… laziness … ) resulted in us having a backup of the data ready to be extracted from the out of sync hidden node. All we had to do was start up mongod … right?2016-01-29T21:06:05.180-0500 I CONTROL ***** SERVER RESTARTED *****2016-01-29T21:06:05.241-0500 I CONTROL [initandlisten] MongoDB starting : pid=1745 port=27021 dbpath=/data 64-bit host=xxx2016-01-29T21:06:05.241-0500 I CONTROL [initandlisten] db version v3.0.82016-01-29T21:06:05.241-0500 I CONTROL [initandlisten] git version: 83d8cc25e00e42856924d84e220fbe4a839e605d2016-01-29T21:06:05.241-0500 I CONTROL [initandlisten] build info: Linux build3.ny.cbi.10gen.cc 2.6.32-431.3.1.el6.x86_64 #1 SMP Fri Jan 3 21:39:27 UTC 2014 x86_64 BOOST_LIB_VERSION=1_492016-01-29T21:06:05.241-0500 I CONTROL [initandlisten] allocator: tcmalloc...2016-01-29T21:06:05.315-0500 W - [initandlisten] Detected unclean shutdown - /data/mongod.lock is not empty.2016-01-29T21:06:05.315-0500 W STORAGE [initandlisten] Recovering data from the last clean checkpoint.2016-01-29T21:06:05.324-0500 I STORAGE [initandlisten] wiredtiger_open config: create,cache_size=13G,session_max=20000,eviction=(threads_max=4),statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),2016-01-29T21:06:05.725-0500 E STORAGE [initandlisten] WiredTiger (0) [1454119565:724960][1745:0x7f2ac9534bc0], file:WiredTiger.wt, cursor.next: read checksum error for 4096B block at offset 6799360: block header checksum of 1769173605 doesn't match expected checksum of 41760847832016-01-29T21:06:05.725-0500 E STORAGE [initandlisten] WiredTiger (0) [1454119565:725067][1745:0x7f2ac9534bc0], file:WiredTiger.wt, cursor.next: WiredTiger.wt: encountered an illegal file format or internal value2016-01-29T21:06:05.725-0500 E STORAGE [initandlisten] WiredTiger (-31804) [1454119565:725088][1745:0x7f2ac9534bc0], file:WiredTiger.wt, cursor.next: the process must exit and restart: WT_PANIC: WiredTiger library panic2016-01-29T21:06:05.725-0500 I - [initandlisten] Fatal Assertion 28558Aw crap. I could not for the life of me get the node back up and running. Since this was a replica-set member, I thought maybe if I just copied the failing file from the (working) primary it would just work. Apparently that’s not the way MongoDB or WiredTiger works :P. Back to the drawing board.I could see that my data directory contained a bunch of collection-*.wt and index-*.wt files, so I assumed these were the WiredTiger collection and index files. These are binary files so grep-ing didn’t help me identify the collection I needed.I wanted to next see if I could just copy the collection’s backing file directly to a new (working) MongoDB installation, so I started up a new mongod, created a new collection with a document in it, then copied over any collection-*.wt file to see what would happen.Guess what … didn’t work.Identify the WiredTiger collection’s backing fileSince we had access to a working node, plus the collection hadn’t been dropped (just purged), I thought maybe the files on each node would be the same. I logged into the primary via the shell to get some info from my collection.db.getCollection('borkedCollection').stats(){ \"ns\" : \"production.borkedCollection\", \"count\" : 0, \"size\" : 0, \"storageSize\" : 1138688, \"capped\" : false, \"wiredTiger\" : { \"metadata\" : { \"formatVersion\" : 1 }, \"creationString\" : \"allocation_size=4KB,app_metadata=(formatVersion=1),block_allocation=best,block_compressor=snappy,cache_resident=0,checkpoint=(WiredTigerCheckpoint.5149=(addr=\\\"01808080808080c0b081e40ebe4855808080e3113fc0e401417fc0\\\",order=5149,time=1454966060,size=21078016,write_gen=119495)),checkpoint_lsn=(224134,44112768),checksum=on,collator=,columns=,dictionary=0,format=btree,huffman_key=,huffman_value=,id=178668,internal_item_max=0,internal_key_max=0,internal_key_truncate=,internal_page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=1MB,memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=0,prefix_compression_min=4,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,value_format=u,version=(major=1,minor=1)\", \"type\" : \"file\", \"uri\" : \"statistics:table:collection-7895--1435676552983097781\", \"LSM\" : { //... }, \"block-manager\" : { //... }, \"btree\" : { // ... }, // ... }, \"nindexes\" : 4, \"totalIndexSize\" : 1437696, \"indexSizes\" : { \"_id_\" : 212992, // ... }, \"ok\" : 1}That \"uri\" : \"statistics:table:collection-7895--1435676552983097781\" entry looked promising.I started hunting for a way to extract the data from this file without having to “mount” the file in another MongoDB installation, as I assumed this was not possible. I stumbled across a command line utility for WiredTiger that happened to have a ‘salvage’ command.Salvaging the WiredTiger collectionIn order to use the wt utility, you have to build it from source. Being comfortable in Linux, this was not daunting ;)wget http://source.wiredtiger.com/releases/wiredtiger-2.7.0.tar.bz2tar xvf wiredtiger-2.7.0.tar.bz2cd wiredtiger-2.7.0sudo apt-get install libsnappy-dev build-essential./configure --enable-snappymake Adding support for Google’s snappy compressor when building WiredTiger will save you some errors that I initially encountered when trying to salvage the data.Now that I had a wt utility, I wanted to test it out on the collection file. It turns out that you need additional supporting files before you can do this. Once I’d copied over the necessary files, my working directory (called mongo-bak) looked like this:-rw-r--r-- 1 root root 4738772992 Feb 9 14:06 collection-2657--1723320556100349955.wt-rw-r--r-- 1 root root 1155072 Feb 9 14:05 _mdb_catalog.wt-rw-r--r-- 1 root root 26935296 Feb 9 14:05 sizeStorer.wt-rw-r--r-- 1 root root 95 Feb 9 14:05 storage.bson-rw-r--r-- 1 root root 46 Feb 9 14:04 WiredTiger-rw-r--r-- 1 root root 495 Feb 9 14:04 WiredTiger.basecfg-rw-r--r-- 1 root root 21 Feb 9 14:04 WiredTiger.lock-rw-r--r-- 1 root root 916 Feb 9 14:04 WiredTiger.turtle-rw-r--r-- 1 root root 10436608 Feb 9 14:04 WiredTiger.wtNow, from the directory where we compiled WiredTiger, we started salvaging the collection:./wt -v -h ../mongo-bak -C \"extensions=[./ext/compressors/snappy/.libs/libwiredtiger_snappy.so]\" -R salvage collection-2657--1723320556100349955.wtYou know it’s working if you see output along the lines of:WT_SESSION.salvage 639400which I believe is just counting up the number of documents recovered. Once the operation has completed, it will have overwritten the source *.wt collection file with whatever it could salvage.The only issue is that you still can’t load this into MongoDB yet.Importing the WiredTiger collection via dump/load into MongoDBIn order to get the data into MongoDB, first we need to generate a dump file from the WiredTiger collection file. This is done using the wt utility:./wt -v -h ../data -C \"extensions=[./ext/compressors/snappy/.libs/libwiredtiger_snappy.so]\" -R dump -f ../collection.dump collection-2657--1723320556100349955This operation produces no output, so you’ll just have to sit tight and wait a while. You can always watch ls -l in another console if you want to make sure it’s working ;)Once completed, you’ll have a collection.dump file, but this still can’t be loaded directly into MongoDB. You can however, using the wt utility one more time, load the dump back into a WiredTiger collection.First, let’s startup a new mongod instance that we can try this out on.mongod --dbpath tmp-mongo --storageEngine wiredTiger --nojournalNext, let’s connect to this instance via the mongo shell and create a new collection:use Recoverydb.borkedCollection.insert({test: 1})db.borkedCollection.remove({})db.borkedCollection.stats()I’ve created a new db called Recovery, and inserted/removed a document so the collection’s backing file would be generated. You can use the stats() method to get the collection name, but since we’re only using one collection, it’s easy enough to find just using ls.Now we’re going to take the backing file name of the collection we just created and use that to load our WiredTiger dump file:./wt -v -h ../data -C \"extensions=[./ext/compressors/snappy/.libs/libwiredtiger_snappy.so]\" -R load -f ../collection.dump -r collection-2-880383588247732034Note that we drop the .wt extension from the collection file above. Also, the -h flag needs to point to the directory where our mongod has it’s dbPath. Finally, mongod should not be running.This operation also provides a progress indicator showing how much data has been loaded:table:collection-4--4286091263744514813: 1386220Once completed, we can start mongod back up, shell in and have a look:$ mongoMongoDB shell version: 3.2.1connecting to: testMongo-Hacker 0.0.9laptop(mongod-3.2.1) test&gt; show dbsRecovery → 0.087GBlocal → 0.000GBlaptop(mongod-3.2.1) test&gt; use Recoveryswitched to db Recoverylaptop(mongod-3.2.1) Recovery&gt; show collectionsborkedCollection → 0.000MB / 88.801MBlaptop(mongod-3.2.1) Recovery&gt; db.borkedCollection.count()0WTF? The size looks right, but there are no documents???laptop(mongod-3.2.1) Recovery&gt; db.borkedCollection.find({}, {_id: 1}){ \"_id\": ObjectId(\"55e07f3b2e967329c888ac74\")}{ \"_id\": ObjectId(\"55e07f3b2e967329c888ac76\")}...{ \"_id\": ObjectId(\"55e07f402e967329c888ac85\")}Fetched 20 record(s) in 29ms -- More[true]Well that’s promising, but the collection still hasn’t been properly restored yet.Restoring the MongoDB collection to a usable stateThis final part is pretty straightforward, as we’re just going to do a mongodump, followed by a mongorestore. The mongodump will fail if you’re using a version of MongoDB &lt; 3.2, as 3.2 is built against WiredTiger 2.7. I initially tested this using MongoDB 3.0.9 and the dump operation just returned 0 results.$ mongodump2016-02-10T22:04:00.580-0500 writing Recovery.borkedCollection to2016-02-10T22:04:03.579-0500 Recovery.borkedCollection 2682192016-02-10T22:04:06.579-0500 Recovery.borkedCollection 3406552016-02-10T22:04:09.579-0500 Recovery.borkedCollection 4967872016-02-10T22:04:12.579-0500 Recovery.borkedCollection 6708942016-02-10T22:04:15.579-0500 Recovery.borkedCollection 7785392016-02-10T22:04:18.579-0500 Recovery.borkedCollection 8485252016-02-10T22:04:21.579-0500 Recovery.borkedCollection 9912772016-02-10T22:04:24.579-0500 Recovery.borkedCollection 11477182016-02-10T22:04:27.579-0500 Recovery.borkedCollection 11876002016-02-10T22:04:30.579-0500 Recovery.borkedCollection 13536652016-02-10T22:04:33.579-0500 Recovery.borkedCollection 13762552016-02-10T22:04:33.681-0500 Recovery.borkedCollection 13862202016-02-10T22:04:33.682-0500 done dumping Recovery.borkedCollection (1386220 documents)$ mongorestore --drop2016-02-10T22:05:51.959-0500 using default 'dump' directory2016-02-10T22:05:51.960-0500 building a list of dbs and collections to restore from dump dir2016-02-10T22:05:52.307-0500 reading metadata for Recovery.borkedCollection from dump/Recovery/borkedCollection.metadata.json2016-02-10T22:05:52.330-0500 restoring Recovery.borkedCollection from dump/Recovery/borkedCollection.bson2016-02-10T22:05:54.962-0500 [#.......................] Recovery.borkedCollection 32.8 MB/569.8 MB (5.8%)2016-02-10T22:05:57.962-0500 [###.....................] Recovery.borkedCollection 80.5 MB/569.8 MB (14.1%)2016-02-10T22:06:00.962-0500 [#####...................] Recovery.borkedCollection 131.5 MB/569.8 MB (23.1%)2016-02-10T22:06:03.962-0500 [#######.................] Recovery.borkedCollection 178.5 MB/569.8 MB (31.3%)2016-02-10T22:06:06.962-0500 [#########...............] Recovery.borkedCollection 230.1 MB/569.8 MB (40.4%)2016-02-10T22:06:09.962-0500 [###########.............] Recovery.borkedCollection 271.6 MB/569.8 MB (47.7%)2016-02-10T22:06:12.962-0500 [#############...........] Recovery.borkedCollection 320.6 MB/569.8 MB (56.3%)2016-02-10T22:06:15.962-0500 [###############.........] Recovery.borkedCollection 366.3 MB/569.8 MB (64.3%)2016-02-10T22:06:18.962-0500 [#################.......] Recovery.borkedCollection 414.9 MB/569.8 MB (72.8%)2016-02-10T22:06:21.962-0500 [###################.....] Recovery.borkedCollection 464.8 MB/569.8 MB (81.6%)2016-02-10T22:06:24.962-0500 [#####################...] Recovery.borkedCollection 504.0 MB/569.8 MB (88.4%)2016-02-10T22:06:27.962-0500 [#######################.] Recovery.borkedCollection 554.5 MB/569.8 MB (97.3%)2016-02-10T22:06:29.082-0500 [########################] Recovery.borkedCollection 569.8 MB/569.8 MB (100.0%)2016-02-10T22:06:29.082-0500 restoring indexes for collection Recovery.borkedCollection from metadata2016-02-10T22:06:29.104-0500 finished restoring Recovery.borkedCollection (1386220 documents)2016-02-10T22:06:29.104-0500 doneNow that we’ve dumped and reloaded the collection yet again, we can shell back in and validate that our recovery attempt has succeeded:$ mongoMongoDB shell version: 3.2.1connecting to: testMongo-Hacker 0.0.9laptop(mongod-3.2.1) test&gt; show dbsRecovery → 0.099GBlocal → 0.000GBlaptop(mongod-3.2.1) test&gt; use Recoveryswitched to db Recoverylaptop(mongod-3.2.1) Recovery&gt; show collectionsborkedCollection → 569.845MB / 88.594MBlaptop(mongod-3.2.1) Recovery&gt; db.borkedCollection.count()1386220BOOYA! Everything is back and properly accessible.The mongorestore could actually have been done to the primary node in order to recover the data for production purposes. Once that’s done, just recreate the necessary indexes and you’re back in business." }, { "title": "Identifying failing system.js functions in MongoDB", "url": "/blog/2016/02/10/identifying-failing-system-dot-js-functions-in-mongodb/", "categories": "MongoDB", "tags": "mongodb, javascript, scripting, troubleshooting", "date": "2016-02-10 15:17:56 -0500", "snippet": "laptop(mongod-3.2.1) test&gt; db.loadServerScripts()2016-02-10T15:18:42.322-0500 E QUERY [thread1] SyntaxError: unterminated string literal :DB.prototype.loadServerScripts/&lt;@src/mongo/shell/d...", "content": "laptop(mongod-3.2.1) test&gt; db.loadServerScripts()2016-02-10T15:18:42.322-0500 E QUERY [thread1] SyntaxError: unterminated string literal :DB.prototype.loadServerScripts/&lt;@src/mongo/shell/db.js:1158:9DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1DB.prototype.loadServerScripts@src/mongo/shell/db.js:1157:5@(shell):1:12016-02-10T15:18:42.323-0500 E QUERY [thread1] Error: SyntaxError: unterminated string literal :DB.prototype.loadServerScripts/&lt;@src/mongo/shell/db.js:1158:9DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1DB.prototype.loadServerScripts@src/mongo/shell/db.js:1157:5@(shell):1:1 :DB.prototype.loadServerScripts/&lt;@src/mongo/shell/db.js:1158:9DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1DB.prototype.loadServerScripts@src/mongo/shell/db.js:1157:5@(shell):1:15:17:56Occasionally we’ll run into these scenarios where we need to load the system.js functions into the global context, but for whatever reason one (or more) scripts are borked.I created on that essentially looks like the following to illustrate this point.var thisFunctionShouldFail = function() { return \"Fail}When you try to execute a db.loadServerScripts() call, the entire process will fail as there is a malformed script.This is a major pain in the ass when you have large background processes that rely heavily on internal system scripts.In order to address this, we wrote a small script that you can run against any database to validate the internal scripts:var testSystemJs = function() { var coll = db.system.js; coll.find({}, {_id: 1}).forEach(function(doc) { try { var func = coll.findOne({_id: doc._id}); eval(func.value); } catch (ex) { print(\"LOAD_ERROR: \" + doc._id); } });}Now if you run the above, it will give you a bit more context into the failures you may have ;)laptop(mongod-3.2.1) test&gt; testSystemJs()2016-02-10T15:52:13.086-0500 E QUERY [thread1] SyntaxError: unterminated string literal :testSystemJs/&lt;@(shell):1:190DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1testSystemJs@(shell):1:66@(shell):1:1LOAD_ERROR: thisFunctionShouldFail2016-02-10T15:52:13.088-0500 E QUERY [thread1] SyntaxError: unterminated string literal :testSystemJs/&lt;@(shell):1:190DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1testSystemJs@(shell):1:66@(shell):1:1LOAD_ERROR: thisFunctionShouldAlsoFailI’m testing this on a mongo 3.2.1 system, but this method should be applicable to older releases as well." }, { "title": "Just Finished - Final Fantasy V", "url": "/blog/2015/11/07/just-finished-final-fantasy-v/", "categories": "Gaming, Just Finished", "tags": "jrpg, gba, rpg", "date": "2015-11-07 19:15:57 -0500", "snippet": "I’ve been waiting write this review for a very long time. I love the Final Fantasy series, and although I got the original back in ‘90, I didn’t really fall in love with the series until I played I...", "content": "I’ve been waiting write this review for a very long time. I love the Final Fantasy series, and although I got the original back in ‘90, I didn’t really fall in love with the series until I played IV for the SNES. Then I played VI. Then I was REALLY hooked.Since I didn’t grow up in Japan, FF IV and VI were numbered II and III when I first got my hands on them. This means that I didn’t know there was a Final Fantasy V; at least not until much later.The game starts off with the king of Tycoon heading off to check on the wind crystal, only to watch it shatter. The hero Bartz (Butz when I played the fan translation :P) sees a meteorite fall from the sky and heads off to investigate. Bartz and his Chocobo go to check it out, and find an old man who’s suffering from amnesia.The king’s daughter, Lenna, heads out to find her father and runs into Bartz and Galuf. They team up and the adventure begins.Once at the wind shrine, they find that the crystal has shattered and within each shard is the power of a job class.This introduces you to the job ability system. If you’ve ever had a chance to play Final Fantasy III, you’ve see this system in action, but in FFV, it’s done to perfection :)Each character can be assigned to a job class. As they defeat enemies, they gain ability points, which levels up the job class. As the classes level up, you gain abilities associated with that class.These abilities can then be applied to a character regardless of the class they’re assigned to. For example, you can give a white mage black magic abilities (if you’ve leveled up a black mage for that character).This gives you a lot of control over how you build your characters as the game progresses.The storytelling in this game is very well done. Although the plot is pretty standard Final Fantasy fare (crystals, amnesia, evil emperor, main characters dying), the writing was pretty good and the character emotion pop-ups added a nice touch as well.I enjoyed the elements of comedy they tried to incorporate into some of the scenarios and characters. After playing this game, Gilgamesh is probably my new favourite non-player character in the series. This might also have to do with Clash on the Big Bridgebeing my favourite song from this game ;)The battle system is the same as in FFIV (Active Time Battle system). You can fight, use items, or up to 2 abilities you’ve learned from gaining AP and leveling up your classes.I didn’t find the battles overly difficult during this play-through, but I’ve been focusing a lot on grinding in the last few games I’ve covered, so this could just be a result of me over leveling.So as you progress through the game, you discover that the ultimate evil, Exdeath, was banished to your world by the four warriors of dawn. Now you and your party need to become the new warriors of dawn to fight him again.You have to get the ultimate weapons for each character class, you can go on some side quests to get some powerful summons and eventually go after Exdeath.There is one fight in Exdeath’s tower that I just couldn’t win, and that was Shinryu. I didn’t bother to upgrade my equipment after a while as I just hoarded my money to use Gil Toss against everybody. As a result, Shinryu just decimated me every time I tried … so I just gave up. I found that magic and summons in this game proved to be effective until the very end. This is different than my experience with FF II, where you bust your ass to get Ultima, then it does like 900 damage to enemies (even after being leveled up a few times : ). I thought the story was really compelling, and I found the whole alternate worlds overlapping “thing” to be a nice twist, though it felt a bit like what happened in FF III. Actually, just like FF III, you even have to race a chocobo around the world to get an item …I teared up (just a bit) when Galuf’s flame was extinguished by Exdeath. I was a bit worried that since this game follows FF IV it was possible they would do the whole “everyone who died is magically back to fight again for no reason” that happened with Yang, Cid, Edward, Tellah … etc. Fortunately (?), this wasn’t the case and it’s actually a very emotional point in the story.I would strongly recommend any fan of JRPGs spend some time on this title. If I were ranking my favourite FFs now, Final Fantasy V would probably show up in the top 5 :)As for where I’m going from here, I’m currently about 6-8 hours into Pokémon Emerald. I’m sort of enjoying it, though it feels like a watered down version of Earthbound (just my initial impression).I’m also about 4-5 hours into Tales of Destiny, but I’m not really enjoying this title and likely won’t go back to it.Feel free to leave me some feedback on whether or not I should continue the above, or if you’ve got a recommendation for the next title I tackle.Cheers!" }, { "title": "October Anime Update", "url": "/blog/2015/10/20/october-anime-update/", "categories": "Anime", "tags": "", "date": "2015-10-20 21:20:16 -0400", "snippet": "I finally got around to adding an Anime section to this blog where I can track my Plex library.Since this is the first post, I figure I’ll just list of what I’ve worked my way through in the past s...", "content": "I finally got around to adding an Anime section to this blog where I can track my Plex library.Since this is the first post, I figure I’ll just list of what I’ve worked my way through in the past six months or so and what’s currently on deck.FinishedThese are the series that I’ve recently completed. Fullmetal Alchemist: Brotherhood Darker than Black (Season 1) Sword Art Online (Season 1) Attack on Titan Deadman WonderlandOn DeckWhat I’m either currently watching, or planning on hitting up soon. Serial Experiments Lain Legend of Korra (I liked Avatar, so figured … why not :P) Death Note Big OFutureBased on what I’m currently interested in, I’ll probably dip my toes in periodically to see if I should pay closer attention to these series soon. Elfen Lied Ergo Proxy Technolyze Log Horizon Accel World" }, { "title": "Just Finished - Lufia 2", "url": "/blog/2015/09/23/just-finished-lufia-2/", "categories": "Gaming, Just Finished", "tags": "snes, jrpg, rpg", "date": "2015-09-23 20:04:59 -0400", "snippet": "This review has been a long time coming; a VERY long time coming. I heard about Lufia 2: Rise of the Sinistrals when I was still in high school, and have been meaning to play it ever since.Back the...", "content": "This review has been a long time coming; a VERY long time coming. I heard about Lufia 2: Rise of the Sinistrals when I was still in high school, and have been meaning to play it ever since.Back then, I was just starting to get into JRPGs (though I probably just thought they were RPGs at the time) and was completely infatuated with anything that Square or Enix could crank out (except for Earthbound, which is still one of my all-time faves).In the decade(s) since my teens, I’ve picked up Lufia 2 a good dozen times, but have never really given it a fair shake. This time around though, since I’m doing this “Just Finished” series, I thought I’d finally sit down and do a full playthrough.For anyone that has not played this game: PLAY IT! Lufia 2 is now in the top 10 of my favourite 16-bit JRPGs of all time. Hopefully after reading my synopsis here, anyone that was on the fence will also take the plunge (you’ll thank me later!).OverviewLufia 2 (aka. Estpolis Denki II, aka. エストポリス伝記II) is a hybrid of traditional JRPG and Action RPG. You control one to four characters at a time throughout the adventure, with enemy encounters being handled via fade-away to a turn-based combat system.It turns out you (Maxim) are destined to fight the Sinistrals, so you embark on a journey to save the world. Cliché? Yup :PDon’t get me wrong, I love a good “destined to save the world” story, but after playing a handful of JRPGs in succession, I was getting a bit numb to the same story over and over again.I decided to give this a fair chance though, and went along with it. I had never played the original Lufia for more than 5-10 minutes, but I do remember the intro segment where you have the four heroes already in the Fortress of Doom getting ready to fight the Sinistrals.I’m going to skip around a bit here because I just want to cover one thing, which is the end of the game … first. The final battle with Daos for Maxim, Artea, Guy and Selan is the EXACT scene from the first game. This made me REALLY want to go back and play the first game, as I now had a connection with the characters.Also, I thought I was a really cool transition as it tied the two games together seamlessly.CombatEnemy encounters on the overworld are random. As you’re walking around, suddenly the screen will zoom in and boom, it’s go time! For anyone familiar with JRPGs, this is pretty common.Unlike traditional RPGs, Lufia 2 doesn’t have “random” encounters in dungeons. All enemies are visible as you explore the dungeons, and though most will chase you, a lot of these battles can be avoided if you so choose.Once a battle is initiated, as opposed to a text-based menu of Fight | Magic | Item | Run as you might see in some games, you’re presented with an icon cross (reminiscent of Breath of Fire).Aside from the standard Attack, Item and Magic options, there is also and Abilities option for each party member. This additional entry gives each character access to various offensive or defensive abilities that are linked to their weapons and armour. This adds another dimension to battles as you have to keep track of what skills are linked to your equipment (don’t just hit ‘optimize’ each time) as weaker items may provide added benefits against certain enemies.PuzzlesWhat I really enjoyed about this game from the moment I entered the first dungeon was the puzzles. Littered throughout each dungeon are puzzles that you need to solve in order to open a door and proceed.Generally, these puzzles aren’t all that complex, but they break up the dungeons a bit so it doesn’t feel monotonous as you basically do the same things over and over in order to progress the story.As you move further through the game, you also collect various key items, that can be used in order to help solve the various puzzles (bombs, arrows, hook shot, etc.)These add another dimension to the puzzles that keeps them interesting. I honestly didn’t find most puzzles to be very challenging, but I will admit to having to use a walkthrough for at least 2 of them … (thankfully there are excellent guides on rpgclassics for just these instances where you need a tiny bit of help)PartyYour party is made up of Maxim, and at various points Tia, Guy, Selan, Dekar and Arty.Tia is from your home village and follows you on the initial leg of the quest. You think she’s going to end up being Maxim’s love interest, but that changes once Selan enters the story ;)Maxim and Selan fall in love, get married, and have a baby. This all happens around the mid-point of the game, acting as a bit of an intermission. Once they have the kid, they decide to go back out and fight so that the world is safe for their baby (and everyone else … I guess).Guy joins as he’s a good fighter and wants to help defeat the Sinistrals. Dekar’s reasons are basically the same.Capsule MonstersYour party can also include capsule monsters. These are optional encounters throughout the game that if you seek out result in an additional character being able to join you in battle.Each monster has an elemental affiliation, and can level up with you as you fight enemies. If the capsule monster dies in battle though, they respawn for the next fight, so there’s no need to worry about their health (you can’t heal them). They do tend to run from a lot of battles automatically though, which can be annoying.Capsule monsters can also be made stronger by leveling them up through feeding them weapons, armour and items.This got a bit annoying as I had to hear a lot of “Yuk”s from the monsters while trying to find the right items to feed them.MusicI just quickly wanted to touch on the music in this game. For a JRPG not composed by Nobuo, the soundtrack is pretty solid.If you’ve got a minute, check out a couple of songs below to get a taste of what’s in store for you ;) Final Battle Peace of Mind The Lost World Battle Theme 3In case you’re wondering, composition credit goes to Yasunori Shiono.Ancient CaveThis is a completely optional dungeon, but if you’re going to play Lufia 2, you HAVE to try and clear it ;)The ancient cave is 99 randomly generated floors, and you start the cave at level 1 (no matter where you actually are in the game). As you level up and descend further into the cave, you find various treasures and spells that you can only use within the cave.If you warp out, you lose all experience and items you’ve found; aside from items in blue chests (which are few).Once you make it to the last level of the cave, you fight the Master Jelly. This is a brutal fight, as you only have 3 turns in order to beat him, and if you don’t, he just runs and that’s the end of it (you have to warp out).Note that I basically cheated to beat him. If you can kill your entire party before he runs, you’ll actually win the fight.I did this, but I didn’t find all the Iris Treasures in the cave so it didn’t really matter.SummaryAs great as you probably think I think this game is by now, it is not without its faults.First off, this game is extremely linear. The plot progresses as follows: Go to tower to find Climb tower and fight enemy Go back to town to learn where next town is Go to shrine to advance to next townSure you get a boat at a certain point and it “opens” up the map, but you still basically go from town to town and tower to tower without much variety.Also, it may be that I’m just a bit more experienced in the genre and know how to level grind, but I found this game to be very, very easy. By the final dungeon, every battle was essentially the same (the final boss included): fight, fight, valor, fight.All in all though, I really enjoyed this game. Although the story seemed like it was going to be pretty basic at the outset turned out the be really compelling.There was actually quite a bit of character development. I think this is why at the end when you realize you’re basically setting up the intro of Lufia I, you really want to go back and play that game to see if the story continues to be as strong.I’ll probably give this game another run through in a few years, as it really was enjoyable. I tend to do this with RPGs i enjoy as I forget specifics of the story over the years and enjoy rediscovering them. Final Fantasy IV and VI have had this treatment now probably a good 4-5 times each ;)" }, { "title": "Fifth Royalty Statement: This time for profit!", "url": "/blog/2015/08/04/fifth-royalty-statement/", "categories": "Writing", "tags": "redmine, book", "date": "2015-08-04 20:49:59 -0400", "snippet": "I’ve been meaning to write this post for a long time, as I officially got the royalty statement at the end of June, but I’ve been a bit busy with life, contracts and the occasional 16-bit era JRPG ...", "content": "I’ve been meaning to write this post for a long time, as I officially got the royalty statement at the end of June, but I’ve been a bit busy with life, contracts and the occasional 16-bit era JRPG …As of this quarter, Redmine Plugin Extension and Development is officially earning me money!Technically, the last statement was in the black, however since the terms laid out by Packt Publishing only pays out royalties greater than £75, those earnings were rolled into this quarter.I’ve updated the table below with the numbers from the current quarter, but I’d really like to thank everyone who’s bought a copy of this book for supporting the Redmine project, and the vibrant plugin development community as well.   Ebook Mini Subscription Packtlib Mini Print Book Mini Subscription (3rd Party Mini) Q1/2014 18 3 18 0 Q2/2014 73 1 33 2 Q3/2014 25 2 24 4 Q4/2014 33 2 31 2 Q1/2015 90 0 25 2 If you’re just venturing onto this blog after having bought the book, please feel free to leave a comment on any of the posts, or just shoot me a message :)" }, { "title": "Just Finished - Final Fantasy II", "url": "/blog/2015/07/22/just-finished-final-fantasy-ii/", "categories": "Gaming, Just Finished", "tags": "gba, jrpg, rpg", "date": "2015-07-22 13:21:52 -0400", "snippet": "I’m excited to finally be able to review this game, as it’s the direct sequel to the game that got me into RPGs way back in 1990-91.When I say Final Fantasy II, I literally mean the second game in ...", "content": "I’m excited to finally be able to review this game, as it’s the direct sequel to the game that got me into RPGs way back in 1990-91.When I say Final Fantasy II, I literally mean the second game in the franchise; not the SNES translation of Final Fantasy IV ;)I actually played a bit of this game in the late 90’s using the Demiforce translation patch (back in the good ol’ NESticle days :D), but didn’t make it very far.For the actual play-through, I decided to go with the Dawn of Souls version for GBA, as this includes updated graphics, an “official” translation and an updated soundtrack.Note that since I’m reviewing a remake, I’m not sure if some of the gameplay elements below were available in the original. If you happen to have actually completed the original NES version and spot an error, please let me know ;)StoryThe story is pretty standard JRPG fare. The Emperor of Palamecia is trying to take over the world. He summons some monsters that destroy the home town of Firion, Maria, Guy and Leon. The first three are rescued by a rebel faction and taken to another town (where they join the rebellion).Leon goes missing. The party wants to find him. Off they go.You’ll meet various characters along the way that will join your party and fight with you. (SPOILERS! they usually die by sacrificing themselves so you can continue the quest …)As you go along, you find out more about the Emperors plan, fight with the Dark Knight who turns out the be Leon, have him join up with you and eventually make your way down to the castle of hell (aka Pandaemonium) to fight the Emperor.What's CoolFFII introduces a couple of interesting gameplay elements that were not reused in any future entries in the series.First, key character conversations allowed you to Learn keywords, which you could then Ask other key characters about. This mechanic helps advance the story as you have to ask various characters about plot points you learn as you go, and they then point you in the direction of your next objective.FFII has multiple modes of travel on the overworld. As with the original, there are ships and an airship, but FFII introduces the chocobo.There is also a bunch of “cut-scenes” added to this incarnation (which would not have been available in the original NES version). These are nice as they aren’t excessive or long, but help advance the plot.Instead of leveling up your characters by fighting and gaining EXP, FFII opted for a system where the more you use a skill, the better you get at it.For example, the more Firion attacks with a sword, the higher his sword level would go. The more Maria uses the Cure spell, the higher her magic level … and Cure spell level would go.This also applied to taking damage. The more damage a player took, the higher their strength and HP levels would go.I thought this was a very interesting mechanic, and I’m not really sure why this didn’t make it into any future games.What's Not CoolFor some reason, this installation in the series decided to introduce an Inn system that charged based on how much healing you required. This proved to be a bit annoying until you realize it’s a LOT cheaper to just use cure spells to get everyone’s HP back up, THEN rest at the inn ;)Another annoyance is that throughout the game, in any dungeon you visit, there are doors that lead to empty rooms. LOTS of doors that lead to empty rooms. What’s annoying is that these rooms have higher encounter rates, so the first step you take to head back towards the exit generally results in a battle.The battles aren’t necessarily difficult, it’s just a pain in the ass to have to wade through all these useless rooms.On battles in general, once again I’m happy to be playing this game using an emulator, as the 8x fast-forward was my best friend.The encounter rates in this game seem to be pretty high in most areas, and the fights are kind of sluggish. I found that for most non-boss fights, I’d just hit fast-forward and hold down the A button (thankfully, you don’t have to mash A in this game; just holding it down works :D).SummaryOverall, I really enjoyed the game. The story (I feel) was much better than that of the original, or even FF III (not FF VI, which is still my favourite installment in the series!)I really liked the leveling system, and found that since I didn’t run from many fights, I didn’t have to game the system by just attacking my own party members to boost their HP/defense/stamina stats.I’m glad I jumped back into the Final Fantasy series for a bit; especially with a game I actually haven’t beaten before. Whenever I sit down with FF, it’s usually either IV, VI or VII.I’m thinking the next Final Fantasy I tackle will likely be V, as I haven’t previously finished this (though I’ve gotten close). Not sure when that’ll happen, because I’m currently balls-deep in Lufia II ;)" }, { "title": "Just Finished - Breath of Fire 2", "url": "/blog/2015/06/29/just-finished-breath-of-fire-2/", "categories": "Gaming, Just Finished", "tags": "snes, jrpg, rpg", "date": "2015-06-29 13:17:02 -0400", "snippet": "Following on the heels of my playthrough of the original Breath of Fire, I’ve just finished Breath of Fire II.Originally released for the Super Nintendo Entertainment System in 1994, BoF2 is the di...", "content": "Following on the heels of my playthrough of the original Breath of Fire, I’ve just finished Breath of Fire II.Originally released for the Super Nintendo Entertainment System in 1994, BoF2 is the direct sequel to the original. Although the protagonist is named the same (Ryu) as in the original, I don’t think he’s supposed to be the same person (the story takes place hundreds of years after the original).You start off as a kid in a small town with your buddy Bow. You go visit the outskirts of town where there is a dragon sleeping and blocking a gate. You’re attacked by a demon named Barubary, who knocks the two out then disappears.The story picks up 10 years later with the two trying to make it as rangers and taking odd jobs. You take a job trying to find someone’s lost pet, and from there, the game begins.You save the pet (pig) in an old house where a hermit lives. This house introduces an interesting dynamic to the game as this becomes the location known as TownShip. As you make your may through the game, various people join your town or can be hired to improve it.You can recruit builders, weapon/item shop keepers and magicians that teach you new skills and abilities. Also as your party grows, they can be stashed here in a big house and you can go talk to them at any point to get clues as to what needs to be done next.The battle system is the same as the original, and most JRPGs of the era; enemies to the left, characters to the right. A menu system controls your actions, which should all be well known by now (Attack, Defend, Magic, Item, Run).As you progress, you will learn that Ryu is a member of the Dragon Clan (as was the case in the original BoF as well), and can learn to summon various dragons.In BoF2, you don’t need to venture too far off track in order to get the dragons. You actually get the majority of them in groups of three, which is convenient.You meet a whole cast of characters that can join your party along the way, and each one has a special overworld ability that helps you progress the story.For example, Sten can reach across some cliffs to grab a pole and drag the party across, Katt can smash rocks to open up pathways, Rand can roll into a ball and get you across vast stretches of the overworld without being attacked.This last one is convenient as the random enemy encounter rate is pretty high in this game. I’m really thankful I was playing this on an emulator that supported fast-forwarding, as the majority of the battles I just auto-fought at 800x speed ;)There are also multiple ways to get around the world. You can walk, ride on a whale, ride on a giant bird, or fly TownShip around. This last one, I didn’t actually get to do as I killed the old guy hooked up to the machine on one of the quests :(.While playing, depending on the decisions you make, you can affect the type of ending you’ll get. Like some other RPGs (ex. Chrono Trigger), there are multiple endings you can get.I didn’t get the best ending, but I didn’t get the worst ending either (I chose to go into the gate and fight the end boss).BoF2 has a lot more “cut scenes” than the original did. I’m not sure if this was the case on the SNES version, but as I was playing the GBA remake, this stood out for me.The cut-scenes are just still images, but they do provide a nice break from the gameplay and tend to actually contribute to the story at hand (as opposed to just being randomly inserted).The underlying story of the game is about a religious cult (St. Eva) that is sending souls to the ultimate evil. Once you infiltrate the cult and discover this plot, you can go to the underworld to fight Detheven, but first you have to beat the demon from the beginning of the game: Barubary.You can choose to fight Barubary alone, or as a group. I believe the choice you make here influences the ending you get, but I’m not positive …I went it alone, and found it to be a very long, difficult fight. You basically have to attack once, heal once, repeat.When I’d knocked off about 3000 points of damage, I finally summoned a dragon to finish him off. This was the recommended strategy as the first few times I fought I used my dragon right away and had my ass kicked shortly thereafter. This walkthrough really helped ;)The final battle with Detheven didn’t prove to be all that difficult, and once you defeat him and the credits roll, Ryu chooses to take the form of a dragon and sacrifice himself to seal off the doorway to the underworld.I quite enjoyed this game, though I did find the battle system to be excruciatingly slow. Without fast-forward on, I doubt I would have finished this game. Also, after a battle completes there is a delay before you can move your characters again, which also made the game feel sluggish.Overall though, I would recommend this to any JRPG fan as the story is solid, the gameplay is fun, and if you like, you can take a break from the action and go fishing for treasure.For my next playthrough, I’m working on Final Fantasy II: Dawn of Souls. Shouldn’t be too much longer now, as I’m already at the Jade Passage, which I believe is the final section of the game.Did you like Breath of Fire II? Which is your favourite of the series? What should I play next? Is my writing getting any better? Let me know in the comments below if you have any feedback.Cheers!Gallery" }, { "title": "Submit Content to Reddit because people WILL see it", "url": "/blog/2015/06/12/submit-content-to-reddit-because-people-will-see-it/", "categories": "Writing", "tags": "reddit", "date": "2015-06-12 09:15:44 -0400", "snippet": "I just wanted to share a quick story as someone who occasionally publishes stuff (both in print and on the internets).I’ve never really given Reddit a fair chance. Every time I would go there, it s...", "content": "I just wanted to share a quick story as someone who occasionally publishes stuff (both in print and on the internets).I’ve never really given Reddit a fair chance. Every time I would go there, it seemed like the front page just had random links to news stories, or links to pictures.I didn’t really bother to dive in and actually look for content, or bother to checkout the subreddits to see what sorts of communities I might be more inclined to interact with.Yesterday, I took a couple of my articles from this blog and submitted them (here, here and here) to see what sort of feedback/engagement I might see.Generally, I see anywhere from 5-25 users a day pass through here. That all changed very, VERY quickly once I submitted my content.Within 12 hours of posting, I’d managed to get up to ~ 900 unique users, and by the end of the day, it was over 1000! (over 9000 would have been sweeter :P)I had some really great conversations around my content, and got some useful feedback as well.I’ve posted content to Twitter, LinkedIn, Facebook and elsewhere, but never really got any good engagement. After this experience though, I know the first place I’ll be dropping my new articles, and am even slightly more motivated to get them completed as I know what I can expect from the community :)Please let me know if there are other communities you’ve found that generate good conversations around your content." }, { "title": "Just Finished - Breath of Fire", "url": "/blog/2015/06/10/just-finished-breath-of-fire/", "categories": "Gaming, Just Finished", "tags": "snes, rpg", "date": "2015-06-10 08:34:06 -0400", "snippet": "My original plan for my next playthrough was going to be Daganronpa, but after making it to chapter 5, I got caught up in an phone upgrade bricking (at least Wind ended up replacing my phone) fiasc...", "content": "My original plan for my next playthrough was going to be Daganronpa, but after making it to chapter 5, I got caught up in an phone upgrade bricking (at least Wind ended up replacing my phone) fiasco and lost all my progress.I didn’t really feel like starting all over again, so I started Lufia II and the original [Breath of Fire](http://en.wikipedia.org/wiki/Breath_of_Fire_(video_game).I ended up sticking with Breath of Fire, though I want to get through both Lufia and Lufia II eventually ;)I wasn’t really planning on writing about this game, but decided at the last minute, so all my screenshots are from the very end ;)I played the Game Boy Advance remake on my phone using GBA.emu. Although I’m perfectly capable of compiling from source and side-loading the APK, I bought the version on the Play store as the developer deserves the support.The story starts off the main character, Ryu’s hometown. He is one of the last members of the Light Dragon Clan, and the Dark Dragon Clan has just come to eradicate them.When they set the town ablaze, Ryu’s sister Sara turns everyone to stone and ventures out alone to stop the invasion. She is captured, and the game begins with you setting out to fight the Dark Dragons and get her back.As you make your way through the game, you meet up with additional character that add a new twist to how you solve puzzles. For example, Bo can hunt animals that appear randomly on the overworld map, Karn can open locked doors, Ox can smash certain walls, Gobi can swim under water and Nina eventually morphs into a bird.I thought this angle was pretty cool, as it makes the game more of an Action-RPG (ala Zelda or Secret of Mana) than a traditional Final Fantasy style JRPG.Each character has a unique skill that is required to progress the story at some point, so you feel more attached to them than supporting characters from certain other series.The combat system is a traditional turn-based, menu-driven, pick your attacks and wait for the enemy implementation. If you’ve played any other JRPG from this era, you’ll feel right at home.The main quest progresses in a pretty linear fashion. You’re trying to find a bunch of Goddess keys that were used to lock up Tyr long ago, but the Dark dragons are trying to find them as well to let Tyr out to wreak havoc on the world.You get these keys, fight members of the Dark Dragons, meet characters, unlock more of the map and eventually have to fight Tyr once and for all.There are side-quests as well that you have to do if you want to learn dragon techniques for Ryu, or fusion techniques for Karn.If you go to the trouble of getting all the Dragon equipment and finding the final dragon form, Agni, you basically have a Win Stick(tm) for the rest of the game (I highly recommend this :P).SummaryThis game wasn’t overly difficult, and really didn’t take that long to complete.The learning curve wasn’t very steep, but this could just be due to having played a lot of other games from the JRPG, Action RPG genres previously.Since the majority of games that I play nowadays are from Square Enix, it was nice to run through something from another publisher.I really enjoyed this game, and am currently working my way through the sequel, Breath of Fire II.Assuming I don’t brick my phone again, hopefully I’ll have a review for that next :D." }, { "title": "Fourth Royalty Statement", "url": "/blog/2015/05/14/fourth-royalty-statement/", "categories": "Writing", "tags": "redmine, book", "date": "2015-05-14 11:34:45 -0400", "snippet": "It’s been over a year since this book was published, and I didn’t think it would see the type of success it has.Based on this last statement, I’m officially eligible to start collecting royalties o...", "content": "It’s been over a year since this book was published, and I didn’t think it would see the type of success it has.Based on this last statement, I’m officially eligible to start collecting royalties off this book.This will have to wait until next quarter as Packt only pays out if the total proceeds exceed £75; which the USD$78 I earned last quarter doesn’t ;)Redmine is a very popular open source project management platform, but in order to extend it users need to be familiar with not only the Ruby language, but the Ruby on Rails framework as well.This can be quite the barrier to entry for new developers, as they need to have a fair amount of knowledge coming in.Based on the sales figures I have access to, as well as feedback and inquiries I’ve received over the last year, it’s safe to say that the Redmine community is thriving :)I’ve updated the sales chart with my last royalty statement:   Ebook Mini Subscription Packtlib Mini Print Book Mini Subscription (3rd Party Mini) Q1/2014 18 3 18 0 Q2/2014 73 1 33 2 Q3/2014 25 2 24 4 Q4/2014 33 2 31 2 If you’re one of the people that bought the book, please leave a comment and let me know what you thought.I’m in the process of updating the knowledgebase plugin that served as the focal point of the book to be compatible with Redmine 3.0 and beyond. If you have any ideas for new features, or feel I should get cracking on the bug fixes, log an issue :)" }, { "title": "Asylum Engine Update", "url": "/blog/2015/04/08/asylum-engine-update/", "categories": "ScummVM", "tags": "scummvm, sanitarium, asylum", "date": "2015-04-08 09:31:14 -0400", "snippet": "It’s been a number of years since I had a chance to look at this project, but recently I updated the codebase to the latest version (sync with upstream ScummVM) and found that the videos no longer ...", "content": "It’s been a number of years since I had a chance to look at this project, but recently I updated the codebase to the latest version (sync with upstream ScummVM) and found that the videos no longer worked.It turned out to be a pretty simple fix (see commit), but it rekindled my interest in the project.I added some additional debug output and cleaned up the script debug loop so that there wasn’t as much spamming of commands that were waiting for another event.Also, thanks to @xesf, we can now actually proceed up and down stairs via fixes he made to the script processor.TODObetter pathfindingThe path-finding in the original engine is a lot smarter and more forgiving than our implementation.inventory managementWe can currently pick up an item and remove the graphic from the screen, but there’s no way to retrieve/use items once collected.clean up graphical glitchesThere are still a number of clipping issues, as well as incorrect behaviour when Max is standing still (wrong animation might play).fix encountersEncounters load, but you can’t interact with keywords yet.save/loadThere was quite a bit of work done on this, but we still need to verify that everything is being restored properly on load." }, { "title": "Block Retry using Powershell", "url": "/blog/2015/02/06/block-retry-using-powershell/", "categories": "Scripting", "tags": "powershell", "date": "2015-02-06 09:54:28 -0500", "snippet": "I’ve been doing a lot of Powershell scripting lately, and one of the features I’ve really been pining for is the ability to apply some form of retry logic to either a function or a block.Take the f...", "content": "I’ve been doing a lot of Powershell scripting lately, and one of the features I’ve really been pining for is the ability to apply some form of retry logic to either a function or a block.Take the following sample:function RandomlyFail{ $rnd = Get-Random -minimum 1 -maximum 3 if ($rnd -eq 2) { throw \"OH NOES!!!\" } $Host.UI.WriteLine(\"W00t!!!\")}Depending on what the random number we get is, we’ll get one of two scenarios:# SuccessRandomlyFailW00t!!!# FailureRandomlyFailOH NOES!!!At line:62 char:9+ throw \"OH NOES!!!\"+ ~~~~~~~~~~~~~~~~~~ + CategoryInfo : OperationStopped: (OH NOES!!!:String) [], RuntimeException + FullyQualifiedErrorId : OH NOES!!!Now, if this happened to be part of a larger script and we didn’t have everything wrapped in a try..catch block, execution could potentially stop there.Since Powershell supports closures, we can write a function that accepts a script block as a parameter.&lt;#This function can be used to pass a ScriptBlock (closure) to be executed and returned.The operation retried a few times on failure, and if the maximum threshold is surpassed, the operation fails completely.Params: Command - The ScriptBlock to be executed RetryDelay - Number (in seconds) to wait between retries (default: 5) MaxRetries - Number of times to retry before accepting failure (default: 5) VerboseOutput - More info about internal processing (default: false)Examples:Execute-With-Retry { $connection.Open() }$result = Execute-With-Retry -RetryDelay 1 -MaxRetries 2 { $command.ExecuteReader() }#&gt;function Execute-With-Retry { [CmdletBinding()] param( [Parameter(ValueFromPipeline,Mandatory)] $Command, $RetryDelay = 5, $MaxRetries = 5, $VerboseOutput = $false ) $currentRetry = 0 $success = $false $cmd = $Command.ToString() do { try { $result = &amp; $Command $success = $true if ($VerboseOutput -eq $true) { $Host.UI.WriteDebugLine(&quot;Successfully executed [$cmd]&quot;) } return $result } catch [System.Exception] { $currentRetry = $currentRetry + 1 if ($VerboseOutput -eq $true) { $Host.UI.WriteErrorLine(&quot;Failed to execute [$cmd]: &quot; + $_.Exception.Message) } if ($currentRetry -gt $MaxRetries) { throw &quot;Could not execute [$cmd]. The error: &quot; + $_.Exception.ToString() } else { if ($VerboseOutput -eq $true) { $Host.UI.WriteDebugLine(&quot;Waiting $RetryDelay second(s) before attempt #$currentRetry of [$cmd]&quot;) } Start-Sleep -s $RetryDelay } } } while (!$success);}Now, if we retrofit our sample above:Execute-With-Retry -RetryDelay 1 -VerboseOutput $true { RandomlyFail }Failed to execute [ RandomlyFail ]: OH NOES!!!DEBUG: Waiting 1 second(s) before attempt #1 of [ RandomlyFail ]Failed to execute [ RandomlyFail ]: OH NOES!!!DEBUG: Waiting 1 second(s) before attempt #2 of [ RandomlyFail ]Failed to execute [ RandomlyFail ]: OH NOES!!!DEBUG: Waiting 1 second(s) before attempt #3 of [ RandomlyFail ]Failed to execute [ RandomlyFail ]: OH NOES!!!DEBUG: Waiting 1 second(s) before attempt #4 of [ RandomlyFail ]W00t!!!DEBUG: Successfully executed [ RandomlyFail ]The inspiration for this comes from an excellent article by Pawel Pabich." }, { "title": "Just Gave Up On - Terranigma", "url": "/blog/2015/01/16/just-finished-terranigma/", "categories": "Gaming, Just Gave Up On", "tags": "snes, rpg", "date": "2015-01-16 09:26:52 -0500", "snippet": "As someone in their thirties with a family, my personal gaming time is somewhat limited. As a result, I need to be a bit more selective with what I choose to sink any free time into; and grinding i...", "content": "As someone in their thirties with a family, my personal gaming time is somewhat limited. As a result, I need to be a bit more selective with what I choose to sink any free time into; and grinding is not where I want that time to go. I’ll get back to this shortly ;)I want to start off by saying I really enjoyed Terranigma. It’s a fun 3rd person action-rpg (similar to Link to the Past or Secret of Mana) that concludes the Quintet trilogy started by Soul Blazer and Illusion of Gaia.I was a big fan of Soul Blazer (less of Illusion of Gaia) so I was really looking forward to Terranigma. The gameplay is similar (real-time battle) and the themes are similar (restoring the world).You start off by opening a funky box, which releases Yomi. I don’t really know what he is, but the box he’s in acts as your inventory system, as well as various settings and status pages. This is kind of innovative as it makes interacting with your items, weapons, armour and stats a bit more interesting.First, you need to save your town, which gets frozen; except for the one guy who conveniently can explain how to unfreeze everyone.You go off to five different towers in order to “free” some of the villagers (kind of like you do in the dungeons of Soul Blazer …) who can then help you proceed through the remaining towers.You save the town, say goodbye to your lifelong friend and jump into an abyss that transports you to the Lightside. There you continue resurrecting the world by going through towns and dungeons and killing baddies. Pretty simple … right?WRONG!I got stuck twice since I tried doing this without a walkthrough. The first place was Louran, since I forgot to pick up the scarf you needed in order to prove to the dog you knew his owner. I was stuck here for a very, very long time …Second is Sylvan Castle in Spain. This is where I’ve officially given up. I made it through the castle with no issues, but once you get to the boss, she just decimates you.Some walkthroughs say your should be at level 21, some say 25. I’m at level 18. Grinding in this game takes a very long time, and I think I’m just going to cut my losses here and move on.If this were a turned-based RPG, I could blindly button mash my way up a few levels. Oh well." }, { "title": "Third Royalty Statement", "url": "/blog/2014/12/22/third-royalty-statement/", "categories": "Writing", "tags": "redmine, book", "date": "2014-12-22 08:11:36 -0500", "snippet": "I’d love to say Christmas came early this year, but it looks like sales of Redmine Plugin Extension and Development didn’t quite hit the numbers needed to actually result in a royalty cheque. Ah we...", "content": "I’d love to say Christmas came early this year, but it looks like sales of Redmine Plugin Extension and Development didn’t quite hit the numbers needed to actually result in a royalty cheque. Ah well, there’s always Q1 of 2015 :DI’ve added the latest numbers to those from my previous post in order to show the running total of units sold this year.   Ebook Mini Subscription Packtlib Mini Print Book Mini Subscription (3rd Party Mini) Q1/2014 18 3 18 0 Q2/2014 73 1 33 2 Q3/2014 25 2 24 4 The main reason I’ve been sharing these numbers (without sharing that actual “numbers”) is to show prospective authors what types of returns they can potentially expect.The book I wrote falls into the “minibook” category, so the returns are lower than a full-on “book” (I think &lt; 100 pages is a minibook), but at this rate, 2015 might result in a non-negative royalty statement that I can share :)Happy Holidays Everyone!!!" }, { "title": "Git Push to GitHub and Bitbucket", "url": "/blog/2014/12/19/git-push-to-github-and-bitbucket/", "categories": "Scripting", "tags": "git, github, bitbucket", "date": "2014-12-19 13:05:35 -0500", "snippet": "I just want to start off by saying how much I LOVE Git. I’ve been working with it for a number of years now (coming from Subversion and sharing code on Google Code and SourceForge) and have fully e...", "content": "I just want to start off by saying how much I LOVE Git. I’ve been working with it for a number of years now (coming from Subversion and sharing code on Google Code and SourceForge) and have fully embraced GitHub as the “victor” (IMHO) for both online source control and collaborative development.The one “downside” to GitHub though is that you don’t have the ability to manage a private repository for free. They do offer reasonable hosting plans, but I generally use private repos for client work or other professional backups.Bitbucket on the other hand offers unlimited private repositories. They limit the collaborative features you have access to, but if you’re strictly mirroring or backing up, that’s not an issue.In order to configure your repository to push to both, all you need to do is:1) create a new repository on Bitbucket2a) edit the .git/config of your local repository2b) add a second url entry under the same remote as you’re already pushing to3) now to initialze the Bitbucket remote execute git push origin -u --all.This will attempt to push all branches to the remote named origin. Since the version on GitHub is already up to date, the Bitbucket version will be initialized and all changes will be pushed.Now, whenever you issue a git push command, both remote repositories will receive the changesets!" }, { "title": "Vote ScummVM for SourceForge POTM", "url": "/blog/2014/11/17/vote-scummvm-for-sourceforge-potm/", "categories": "ScummVM", "tags": "scummvm", "date": "2014-11-17 08:50:15 -0500", "snippet": "Although I haven’t used my SourceForge account in a number of years, when I started working with ScummVM, that’s where the code and issue tracker were, so that’s where the development community was...", "content": "Although I haven’t used my SourceForge account in a number of years, when I started working with ScummVM, that’s where the code and issue tracker were, so that’s where the development community was as well.They’ve since moved to GitHub, but SourceForge has included them in the running for Project of the Month, January 2015.If you’ve still got an account, or just feel like creating one to show them some love, head on over to the voting page for POTM and reply with VOTE: scummvm.They’ve got my vote ;)" }, { "title": "Just Finished - Robotrek", "url": "/blog/2014/10/17/just-finished-robotrek/", "categories": "Gaming, Just Finished", "tags": "rpg, snes", "date": "2014-10-17 09:00:00 -0400", "snippet": "I was planning on writing this review a while ago, but it turns out I nuked my phone and lost my progress so I had to start all over again. I originally started this on 2014-09-26, but I kind of lo...", "content": "I was planning on writing this review a while ago, but it turns out I nuked my phone and lost my progress so I had to start all over again. I originally started this on 2014-09-26, but I kind of lost my motivation … and I started playing Terranigma, which is pretty awesome so far.In my ongoing series of articles about old games that I never got around to playing, today I’m going to tackle Robotrek.Robotrek is a silly RPG that revolves around “getting to the bottom” of a situation involving a group calling themselves the “hackers” who are disrupting the peace of your home town of Rococo.The story follows your character, who wants to be an inventor, and right off the bat learns how to make robots. As you progress through the game, you’ll eventually be able to build three robots, which can be swapped in an out during battle.These robots can be upgraded with better weapons and armour by reading books you find throughout the game.You’ll also learn how to make items and some useful tools that will help you solve various puzzles.Since you’re an inventor in this game, one of your main tasks is to combine items you find in order to potentially make new items. Although this is useful a couple times, I found I didn’t really need to use it beyond the first part of the game where you make the drill.You can sometimes combine scraps with each other to produce weapons, or combine scraps with weapons to produce upgraded weapons, but only trial and error yield results. I couldn’t find any information in the game that helped lead you towards proper combinations (and I wanted to avoid walkthroughs).Battles in this game are pretty straightforward. You equip weapons to your robots, then use them in battle either directly, or you can program sequences that allow you to run multiple attacks in succession. This approach can kill enemies faster, but will require you to charge up for longer, and therefore be vulnerable for longer as well.The fights are extremely repetitive, and it doesn’t help that there only appears to be a single battle theme that loops constantly. I played most of the game with the volume off due to this :POnce you get to the volcano and find the Sword 4 in a chest, the random battles become exceedingly easy (for at least the next 3/4 of the game). This weapon is just ridiculously overpowered at this point and you’ll find yourself just spamming it in order to quickly proceed through the story.Boss battles tend to be very dependent on what level you’re at when you enter the fight. Most battles I had to redo at least once as I found I was far too low of a level to have a chance.I found this to be the most true when I got to De Rose. This was the boss I reached during my last playthrough, and after grinding for a good 2 hours, I found I just couldn’t beat her, gave up and moved on to another game.This time around, I actually checked a walkthrough, which recommended I be at about level 50 before entering the fight. I was at level 37. I was going to have to grind … and grinding in this game is TEDIOUS. I ended up grinding to about level 44 before beating her, but this took a good 4-5 hours.The story progresses: the Hackers leader (a crab) is defeated, Dr. Einst’s plot is foiled, a new Hackers leader emerges (Blackmore), the story jumps back and forwards in time, a spaceship emerges so you can visit another planet and you end up having to save the world.The final battle is against Gateau (the “food”-naming convention for characters and places is used throughout the game), who had something to do with using the Tetron to build a weapon or something (I can’t remember the exact story; play the game for a better description :P).He takes two forms, but isn’t overly difficult to defeat at this point.Overall, this game was pretty easy. The story progressed in a very linear fashion, and most puzzles were pretty straightforward to solve.It was fun though, and though I probably won’t replay this game again in the future, I’m glad that I went through it at least once.My screenshots are below, though it seems that the emulator I was using (Snes9x EX on my Moto G) overwrote some of my earlier images. Oh well. Enjoy :PGallery" }, { "title": "Gallery Tag for Octopress", "url": "/blog/2014/10/07/gallery-tag-for-octopress/", "categories": "Ruby", "tags": "octopress, jekyll", "date": "2014-10-07 08:31:08 -0400", "snippet": "While working on my post about finishing Robotrek, I found that I had taken a lot more screenshots than would fit nicely with the amount of copy I intended to write.I hope to be done the just finis...", "content": "While working on my post about finishing Robotrek, I found that I had taken a lot more screenshots than would fit nicely with the amount of copy I intended to write.I hope to be done the just finished … article soon-ish …Since I didn’t really want to discard any, I figured I’d just throw the execss screenshots into a carousel or gallery.I found an excellent example, but it didn’t quite produce the results I was looking for.I ended up forking the gist in order to implement a Gallery tag for Jekyll/Octopress that could work with the Galleria jQuery plugin.Now I can build a gallery from my extra screenshots by simply adding{% galleria %}/images/robotrek/Robotrek_(USA).000.png/images/robotrek/Robotrek_(USA).001.png/images/robotrek/Robotrek_(USA).002.png/images/robotrek/Robotrek_(USA).003.png/images/robotrek/Robotrek_(USA).004.png/images/robotrek/Robotrek_(USA).005.png/images/robotrek/Robotrek_(USA).006.png/images/robotrek/Robotrek_(USA).007.png/images/robotrek/Robotrek_(USA).008.png/images/robotrek/Robotrek_(USA).009.png/images/robotrek/Robotrek_(USA).010.png{% endgalleria %}to my post, which will produce the following gallery:" }, { "title": "Second Royalty Statement", "url": "/blog/2014/09/22/second-royalty-statement/", "categories": "Writing", "tags": "redmine, book", "date": "2014-09-22 11:45:14 -0400", "snippet": "Although I haven’t really had much to write about lately, I just got another royalty update from Packt Publishing about Redmine Plugin Extension and Development so I wanted to share the latest sale...", "content": "Although I haven’t really had much to write about lately, I just got another royalty update from Packt Publishing about Redmine Plugin Extension and Development so I wanted to share the latest sales figures … sort of :PIn the table below, I’ve added the latest numbers to those from my previous post in order to show the running total of units sold this year.   Ebook Mini Subscription Packtlib Mini Print Book Mini Subscription (3rd Party Mini) Q1/2014 18 3 18 0 Q2/2014 73 1 33 2 Once again, I’ve decided to remove the actual sales numbers from the statement, but I left in TOTAL ROYALTIES TO BE PAID IN PERIOD section. I did this to show the progress from time of publication to now in terms of how much of the advance has been consumed by ACTUAL sales.At this rate, it’s possible I’ll see some additional money out of this title by the time 2014 is through .. which would be very exciting.I’m sure there are others out there who have been approached by technical publications and asked to write about some topic or other. I’m sure the numbers I’m posting here will convince you it’s not worth quitting your job and writing full time, but it looks like it could become profitable over time.If you’ve had anything published and are seeing a decent return on your “investment”, please share in the comments ;)" }, { "title": "Just Finished - Fire Emblem: The Sacred Stones", "url": "/blog/2014/08/06/just-finished-fire-emblem-the-sacred-stones/", "categories": "Gaming, Just Finished", "tags": "rpg, gba", "date": "2014-08-06 17:58:55 -0400", "snippet": "I’ve played a lot of RPGs over the years (JRPGs mostly) but have somehow managed to hear little to nothing about the Fire Emblem series.My focus was mostly on the Final Fantasy series, as well as o...", "content": "I’ve played a lot of RPGs over the years (JRPGs mostly) but have somehow managed to hear little to nothing about the Fire Emblem series.My focus was mostly on the Final Fantasy series, as well as other Square-Enix properties so it’s possible this is why :PThis post isn’t really meant to be a review; it’s more just my thoughts on the game right after I finished it as (a) I didn’t take any notes, and (b) I started this playthrough over a year ago.I’m using this as an opportunity to get back into writing a bit more frequently, so I apologize in advance if the subject matter isn’t covered very well, or if the writing is all over the place.I managed to complete the game in about 19 hours, so at least I managed to finish over an hour a month (which by gaming standards is terrible, but I’m married with kids and have a job so back off! :P).I played through on my Nexus 7 using GBA.emu (which I would highly recommend buying to support the author), as I don’t have an actual GBA (and prefer emulated games for convenience).The game is turn-based tactical combat. You can buy equipment for your team members at various shops, and there is a “world map” that ties the various locations together. This makes is seem as though you’re playing a board game.The story is pretty straightforward: a dark force was sealed with a bunch of magic stones which were then distributed for safe keeping. Now someone wants to get these stones and re-awaken the dark force.The goals of each map are either to defeat an enemy or capture a location. Along the way there are various NPC characters that you can either fight or try to persuade to join you.Fight outcome is determined purely mathematically (stats, resources, weapon selection, character skill level, enemy skill level, terrain) and is presented as 2 avatars on the map with health guages underneath that deplete based on the result of the last “turn” of combat.For some special battles, the characters are shown as their avatar representations and the fights are animated. These fights are generally plot advancement battles.Once of the main differentiating factors between Fire Emblem and other RPG series is “permanent death”. Once a character dies on the battle field, there is no way to revive them. This can make the game extremely difficult as you can potentially advance to a point where you cannot continue as you don’t have enough players to clear a map without dying.I found myself abusing save states as I didn’t want my characters to die, and I didn’t want to restart campaigns once I’d progressed beyond a certain point (usually about 10 minutes …).Overall I quite enjoyed the game. The story isn’t terrible and there are enough characters to keep the campaigns interesting if you want or, if you’re like me you stick with a core group of characters that end up being ridiculously over-leveled by the end.Once you finish the game, the final credits roll an epilogue unfolds that gives some final info about the characters that survived throughout the game. The characters that died in battle are also listed, and it summarizes where they died as well, which I thought was a nice touch.I will likely take a stab at another entry in this series and take a bit more time (in game, not another year :P) to explore as I missed A LOT.Once I finished the game I realized there was an entire Extras section where you can manage teams and practice battles (for points I think). Also I had a rogue with a members card and didn’t notice any of the “?” shops until after beating the last area.I also didn’t spend much time in the optional dungeons, though I did use upgrade my characters to new classes whenever possible :) The next time I tackle a Fire Emblem title though I think I’ll try to (a) explore more, (b) do more optional/side quests, (c) NOT restart campaigns once they’re underway.I’m currently also midway through a playthrough of Robotrek which I’ve been working on-again-off-again for about a year as well, so i may try to finish that off for the next Just Finished™ post.If anyone has suggestions for an RPG, leave a comment (currently I’m sticking with NES to PSP as the platform range)." }, { "title": "Pipe Command Output to Logentries", "url": "/blog/2014/07/16/pipe-command-output-to-logentries/", "categories": "Linux", "tags": "linux, logentries", "date": "2014-07-16 12:06:55 -0400", "snippet": "I’ve been using Logentries for a couple of months now to manage variou project logs and have found it to be extremely convenient.If you want to retrofit a crontab or any other process to use the Lo...", "content": "I’ve been using Logentries for a couple of months now to manage variou project logs and have found it to be extremely convenient.If you want to retrofit a crontab or any other process to use the Logentries Token-TCP type log, just do the following:COMMAND | while read -r line; do echo \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee $line\" &gt; /dev/tcp/data.logentries.com/80; doneWhere aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee is your log token, and COMMAND is whatever your like.For example:lsblk | while read -r line; do echo \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee $line\" &gt; /dev/tcp/data.logentries.com/80; doneHopefully this helps someone other than myself :)" }, { "title": "Adding LinkedIn Share Support to Octopress", "url": "/blog/2014/07/15/adding-linkedin-share-support-to-octopress/", "categories": "Ruby", "tags": "octopress", "date": "2014-07-15 15:55:14 -0400", "snippet": "I was looking for a plugin or an include to easily add LinkedIn support to the share options for my posts.A quick Google search came up dry, so I figured I’d just roll my own.In order to accomplish...", "content": "I was looking for a plugin or an include to easily add LinkedIn support to the share options for my posts.A quick Google search came up dry, so I figured I’d just roll my own.In order to accomplish this, all that is required is to edit source/_includes/post/sharing.html:diff --git a/source/_includes/post/sharing.html b/source/_includes/post/sharing.htmlindex e32500d..0bdd01e 100644--- a/source/_includes/post/sharing.html+++ b/source/_includes/post/sharing.html@@ -5,6 +5,12 @@ {% if site.google_plus_one %} &lt;div class=\"g-plusone\" data-size=\"{{ site.google_plus_one_size }}\"&gt;&lt;/div&gt; {% endif %}+ {% if site.linkedin_share %}+ &lt;script src=\"//platform.linkedin.com/in.js\" type=\"text/javascript\"&gt;+ lang: en_US+ &lt;/script&gt;+ &lt;script type=\"IN/Share\" data-url=\"{{ site.url }}{{ page.url }}\" data-counter=\"right\"&gt;&lt;/script&gt;+ {% endif %} {% if site.facebook_like %} &lt;div class=\"fb-like\" data-send=\"true\" data-width=\"450\" data-show-faces=\"false\"&gt;&lt;/div&gt; {% endif %}The change above adds a snippit of Javascript that passes the current site.url and page.url in a similar fashion as is done for Twitter.The Javascript source was generated using LinkedIn’s Share Plugin Generator, so if the format of the one I’ve chosen isn’t to your liking, you can generate another.Just in case you want to easily toggle this functionality, I’ve also set it up to check for a configuration value in your site’s _config.yml:diff --git a/_config.yml b/_config.ymlindex f8f825c..2dce55b 100644--- a/_config.yml+++ b/_config.yml@@ -104,4 +104,7 @@ google_analytics_tracking_id: xxx # Facebook Like facebook_like: true+# LinkedIn Share+linkedin_share: trueNow, the next time your site is regenerated, a LinkedIn share button will be available.This same method could be used to add pretty much any other social sharing sites you may want, as most should provide some form of widget generator." }, { "title": "First Royalty Statement", "url": "/blog/2014/07/15/8748os-first-royalty-statement/", "categories": "Writing", "tags": "redmine, book", "date": "2014-07-15 14:38:35 -0400", "snippet": "Just a quick update on Redmine Plugin Extension and Development. I received my first royalty statement today, which basically just indicates that not enough books have sold to exceed the advance I ...", "content": "Just a quick update on Redmine Plugin Extension and Development. I received my first royalty statement today, which basically just indicates that not enough books have sold to exceed the advance I received initially.(I probably didn’t need to redact anything from the image, but it seemed appropriate.)The deal I had with Packt was that they’d front me $600 against future sales, of which my cut is (I believe) ~ 16% of the list price.Still a ways to go (sales-wise) before I would consider this a “profitable” venture, but it’s nice to see that there is at least some initial interest.I was approached by Packt a little while back to author another book, but based on the amount of time I invested in this one as well as the sales figures I’m sharing here, I don’t think it would be a good choice :PShould I consider writing another book? Any thoughts out there from my fellow denizens of the interwebs?" }, { "title": "Subversion + Ruby on Heroku via Buildpacks", "url": "/blog/2014/07/07/subversion-plus-ruby-on-heroku-via-buildpacks/", "categories": "Heroku", "tags": "redmine, heroku, subversion", "date": "2014-07-07 09:28:38 -0400", "snippet": "Back in 2012, I wrote a post about Redmine and Subversion on Heroku that involved some funky hacks in order to build a working svn binary that could be uploaded as part of your Git payload.This can...", "content": "Back in 2012, I wrote a post about Redmine and Subversion on Heroku that involved some funky hacks in order to build a working svn binary that could be uploaded as part of your Git payload.This can be done a lot more cleanly by taking advantage of Heroku Buildpacks.All of the instructions I provided in my previous post can now be distilled into a single buildpack install command.If we want to leverage an existing solution, we just add the Subversion Buildpack to our app.The main “gotcha” here is that we can only have ONE buildpack defined at a time, which is problematic because our app’s environment (Ruby, PHP, Node .. etc) is managed as a buildpack (see https://devcenter.heroku.com/articles/buildpacks for more details).In order to get around this limitation though, we can leverage another buildpack, known as the heroku-buildpack-multi buildpack which allows us to define multiple buildpacks.For our purposes, we’re going to add Subversion and Ruby to our app:$ cd /path/to/app$ heroku config:add BUILDPACK_URL=https://github.com/ddollar/heroku-buildpack-multi.git$ cat .buildpackshttps://github.com/cucumber-ltd/heroku-buildpack-subversion.githttps://github.com/heroku/heroku-buildpack-ruby.git$ git commit .buildpacks -m \"Add Buildpacks\"$ git push heroku masterThis has helped me with my Redmine deployment on Heroku. Did it help you at all? Did I get something wrong. Let me know ;)" }, { "title": "Troubleshooting a Mongoid Connection Issue", "url": "/blog/2014/06/23/troubleshooting-a-mongoid-connection-issue/", "categories": "MongoDB", "tags": "mongodb, mongoid, rails, ruby", "date": "2014-06-23 10:27:58 -0400", "snippet": "I’ve been struggling with an issue for a while now regarding Mongoid connections just freezing when the host is not on localhost. I’ve tried posting this on stack overflow and GitHub, but haven’t r...", "content": "I’ve been struggling with an issue for a while now regarding Mongoid connections just freezing when the host is not on localhost. I’ve tried posting this on stack overflow and GitHub, but haven’t really gotten anywhere.I’m now trying to dive into the issue using GDB directly, which I’m starting off following the NewRelic blog post about debugging hung Ruby processes.First, once the process is frozen, I connect using:sudo gdb -p &lt;pid&gt;and then tried to get some info from the C-level backtraces via:(gdb) t a a btThis resulted in a lot of information that didn’t really help me (at least not immediately):Thread 8 (Thread 0x7f032a5e1700 (LWP 9001)):#0 0x00007f0329c0ffbd in poll () at ../sysdeps/unix/syscall-template.S:81#1 0x00007f032a0d1d60 in timer_thread_sleep (gvl=0x1ed8f28) at thread_pthread.c:1381#2 thread_timer (p=0x1ed8f28) at thread_pthread.c:1456#3 0x00007f032990c182 in start_thread (arg=0x7f032a5e1700) at pthread_create.c:312#4 0x00007f0329c1d30d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111Thread 7 (Thread 0x7f032a4c0700 (LWP 9002)):#0 sem_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/sem_wait.S:85#1 0x00007f032495b75d in v8::internal::LinuxSemaphore::Wait() () from gems/ruby-2.1.1/extensions/x86_64-linux/2.1.0/therubyracer-0.12.1/v8/init.so#2 0x00007f032486104c in v8::internal::RuntimeProfiler::WaitForSomeIsolateToEnterJS() () from gems/ruby-2.1.1/extensions/x86_64-linux/2.1.0/therubyracer-0.12.1/v8/init.so#3 0x00007f032495b968 in v8::internal::SignalSender::Run() () from gems/ruby-2.1.1/extensions/x86_64-linux/2.1.0/therubyracer-0.12.1/v8/init.so#4 0x00007f032495b86e in v8::internal::ThreadEntry(void*) () from gems/ruby-2.1.1/extensions/x86_64-linux/2.1.0/therubyracer-0.12.1/v8/init.so#5 0x00007f032990c182 in start_thread (arg=0x7f032a4c0700) at pthread_create.c:312#6 0x00007f0329c1d30d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111Thread 6 (Thread 0x7f0321988700 (LWP 9008)):#0 0x00007f0329c14c33 in select () at ../sysdeps/unix/syscall-template.S:81#1 0x00007f032a0d8cdb in rb_fd_select (n=&lt;optimized out&gt;, readfds=&lt;optimized out&gt;, writefds=&lt;optimized out&gt;, exceptfds=&lt;optimized out&gt;, timeout=&lt;optimized out&gt;) at thread.c:3321#2 0x00007f032a0d9139 in native_fd_select (th=&lt;optimized out&gt;, timeout=0x7f0321987550, exceptfds=0x0, writefds=0x0, readfds=0x7f0321987810, n=12) at thread_pthread.c:1007#3 do_select (timeout=0x7f0321987550, except=0x0, write=0x0, read=0x7f0321987810, n=12) at thread.c:3436#4 rb_thread_fd_select (max=max@entry=12, read=read@entry=0x7f0321987810, write=write@entry=0x0, except=except@entry=0x0, timeout=timeout@entry=0x7f03219877e0) at thread.c:3582#5 0x00007f0329f96150 in select_internal (fds=0x7f0321987810, tp=0x7f03219877e0, except=&lt;optimized out&gt;, write=&lt;optimized out&gt;, read=&lt;optimized out&gt;) at io.c:8232#6 select_call (arg=arg@entry=139651425269744) at io.c:8302#7 0x00007f0329f67d97 in rb_ensure (b_proc=b_proc@entry=0x7f0329f95e30 &lt;select_call&gt;, data1=data1@entry=139651425269744, e_proc=e_proc@entry=0x7f0329f926a0 &lt;select_end&gt;, data2=data2@entry=139651425269744) at eval.c:850#8 0x00007f0329f92810 in rb_f_select (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, obj=&lt;optimized out&gt;) at io.c:8651#9 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f0321a88f20, th=0x96c91c0) at vm_insnhelper.c:1470#10 vm_call_cfunc (th=0x96c91c0, reg_cfp=0x7f0321a88f20, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#11 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x96c91c0, initial=initial@entry=0) at insns.def:1028#12 0x00007f032a0bb5ec in vm_exec (th=0x96c91c0) at vm.c:1304#13 0x00007f032a0be89f in vm_invoke_proc (th=th@entry=0x96c91c0, proc=0x96c9690, self=133280920, defined_class=117512440, argc=0, argv=0x7f1b228, blockptr=blockptr@entry=0x0) at vm.c:788#14 0x00007f032a0be8da in rb_vm_invoke_proc (th=th@entry=0x96c91c0, proc=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, blockptr=blockptr@entry=0x0) at vm.c:807#15 0x00007f032a0d475d in thread_start_func_2 (th=th@entry=0x96c91c0, stack_start=&lt;optimized out&gt;) at thread.c:535#16 0x00007f032a0d4a9b in thread_start_func_1 (th_ptr=0x96c91c0) at thread_pthread.c:803#17 0x00007f032990c182 in start_thread (arg=0x7f0321988700) at pthread_create.c:312#18 0x00007f0329c1d30d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111Thread 5 (Thread 0x7f0321786700 (LWP 9009)):#0 pthread_cond_timedwait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_timedwait.S:238#1 0x00007f032a0d27ff in native_cond_timedwait (ts=&lt;optimized out&gt;, mutex=&lt;optimized out&gt;, cond=&lt;optimized out&gt;) at thread_pthread.c:352#2 native_sleep (th=0x96c9870, timeout_tv=0x7f03217857c0) at thread_pthread.c:1061#3 0x00007f032a0d55fa in sleep_timeval (th=0x96c9870, tv=..., spurious_check=spurious_check@entry=1) at thread.c:1046#4 0x00007f032a0d57ba in rb_thread_wait_for (time=...) at thread.c:1115#5 0x00007f0329ff27d0 in rb_f_sleep (argc=1, argv=0x7f0321787038) at process.c:4193#6 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f0321886f70, th=0x96c9870) at vm_insnhelper.c:1470#7 vm_call_cfunc (th=0x96c9870, reg_cfp=0x7f0321886f70, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#8 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x96c9870, initial=initial@entry=0) at insns.def:1028#9 0x00007f032a0bb5ec in vm_exec (th=0x96c9870) at vm.c:1304#10 0x00007f032a0be89f in vm_invoke_proc (th=th@entry=0x96c9870, proc=0x96c9d90, self=133279880, defined_class=117256360, argc=0, argv=0x7f1afd0, blockptr=blockptr@entry=0x0) at vm.c:788#11 0x00007f032a0be8da in rb_vm_invoke_proc (th=th@entry=0x96c9870, proc=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, blockptr=blockptr@entry=0x0) at vm.c:807#12 0x00007f032a0d475d in thread_start_func_2 (th=th@entry=0x96c9870, stack_start=&lt;optimized out&gt;) at thread.c:535#13 0x00007f032a0d4a9b in thread_start_func_1 (th_ptr=0x96c9870) at thread_pthread.c:803#14 0x00007f032990c182 in start_thread (arg=0x7f0321786700) at pthread_create.c:312#15 0x00007f0329c1d30d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111Thread 4 (Thread 0x7f0321584700 (LWP 9010)):#0 0x00007f0329c14c33 in select () at ../sysdeps/unix/syscall-template.S:81#1 0x00007f032a0d8cdb in rb_fd_select (n=&lt;optimized out&gt;, readfds=&lt;optimized out&gt;, writefds=&lt;optimized out&gt;, exceptfds=&lt;optimized out&gt;, timeout=&lt;optimized out&gt;) at thread.c:3321#2 0x00007f032a0d9139 in native_fd_select (th=&lt;optimized out&gt;, timeout=0x0, exceptfds=0x0, writefds=0x0, readfds=0x7f0321583810, n=11) at thread_pthread.c:1007#3 do_select (timeout=0x0, except=0x0, write=0x0, read=0x7f0321583810, n=11) at thread.c:3436#4 rb_thread_fd_select (max=max@entry=11, read=read@entry=0x7f0321583810, write=write@entry=0x0, except=except@entry=0x0, timeout=timeout@entry=0x0) at thread.c:3582#5 0x00007f0329f96150 in select_internal (fds=0x7f0321583810, tp=0x0, except=&lt;optimized out&gt;, write=&lt;optimized out&gt;, read=&lt;optimized out&gt;) at io.c:8232#6 select_call (arg=arg@entry=139651421059056) at io.c:8302#7 0x00007f0329f67d97 in rb_ensure (b_proc=b_proc@entry=0x7f0329f95e30 &lt;select_call&gt;, data1=data1@entry=139651421059056, e_proc=e_proc@entry=0x7f0329f926a0 &lt;select_end&gt;, data2=data2@entry=139651421059056) at eval.c:850#8 0x00007f0329f92810 in rb_f_select (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, obj=&lt;optimized out&gt;) at io.c:8651#9 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f0321684f20, th=0x96c9f10) at vm_insnhelper.c:1470#10 vm_call_cfunc (th=0x96c9f10, reg_cfp=0x7f0321684f20, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#11 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x96c9f10, initial=initial@entry=0) at insns.def:1028#12 0x00007f032a0bb5ec in vm_exec (th=0x96c9f10) at vm.c:1304#13 0x00007f032a0be89f in vm_invoke_proc (th=th@entry=0x96c9f10, proc=0x96ec480, self=133126360, defined_class=118126960, argc=0, argv=0x7f1adf0, blockptr=blockptr@entry=0x0) at vm.c:788#14 0x00007f032a0be8da in rb_vm_invoke_proc (th=th@entry=0x96c9f10, proc=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, blockptr=blockptr@entry=0x0) at vm.c:807#15 0x00007f032a0d475d in thread_start_func_2 (th=th@entry=0x96c9f10, stack_start=&lt;optimized out&gt;) at thread.c:535#16 0x00007f032a0d4a9b in thread_start_func_1 (th_ptr=0x96c9f10) at thread_pthread.c:803#17 0x00007f032990c182 in start_thread (arg=0x7f0321584700) at pthread_create.c:312#18 0x00007f0329c1d30d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111Thread 3 (Thread 0x7f0321180700 (LWP 9472)):#0 0x00007f0329c1007f in __GI_ppoll (fds=fds@entry=0x7f032117bb90, nfds=nfds@entry=1, timeout=&lt;optimized out&gt;, timeout@entry=0x0, sigmask=sigmask@entry=0x0) at ../sysdeps/unix/sysv/linux/ppoll.c:56#1 0x00007f032a0d9b28 in rb_wait_for_single_fd (fd=fd@entry=15, events=events@entry=1, tv=tv@entry=0x0) at thread.c:3656#2 0x00007f032a0da044 in rb_thread_wait_fd_rw (read=1, fd=fd@entry=15) at thread.c:3495#3 rb_thread_wait_fd (fd=fd@entry=15) at thread.c:3506#4 0x00007f0329f9693f in rb_io_wait_readable (f=15) at io.c:1092#5 0x00007f0329f97cee in io_bufread (ptr=0x7f03042c3440 \"\", len=36, fptr=0x7f0310054420) at io.c:2035#6 0x00007f0329f97e24 in bufread_call (arg=arg@entry=139651416833616) at io.c:2071#7 0x00007f0329f67d97 in rb_ensure (b_proc=b_proc@entry=0x7f0329f97e10 &lt;bufread_call&gt;, data1=data1@entry=139651416833616, e_proc=e_proc@entry=0x7f032a04f900 &lt;rb_str_unlocktmp&gt;, data2=&lt;optimized out&gt;) at eval.c:850#8 0x00007f032a05e8db in rb_str_locktmp_ensure (str=&lt;optimized out&gt;, func=func@entry=0x7f0329f97e10 &lt;bufread_call&gt;, arg=arg@entry=139651416833616) at string.c:2004#9 0x00007f0329f9ad06 in io_fread (fptr=0x7f0310054420, size=36, offset=0, str=139650673747040) at io.c:2085#10 io_read (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, io=35833440) at io.c:2816#11 0x00007f032a0bfae1 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127e770, th=0x7f030c0455c0) at vm_insnhelper.c:1470#12 vm_call_cfunc (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127e770, th=0x7f030c0455c0) at vm_insnhelper.c:1560---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---#13 vm_call_method (th=0x7f030c0455c0, cfp=0x7f032127e770, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1754#14 0x00007f032a0b7b9c in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:1050#15 0x00007f032a0bb5ec in vm_exec (th=th@entry=0x7f030c0455c0) at vm.c:1304#16 0x00007f032a0c2cff in invoke_block_from_c (defined_class=79426520, cref=0x0, blockptr=0x0, argv=0x7f032117c2e8, argc=1, self=72558880, block=&lt;optimized out&gt;, th=0x7f030c0455c0) at vm.c:732#17 vm_yield (argv=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, th=&lt;optimized out&gt;) at vm.c:763#18 rb_yield_0 (argv=0x7f032117c2e8, argc=1) at vm_eval.c:938#19 rb_yield (val=&lt;optimized out&gt;) at vm_eval.c:948#20 0x00007f0329f1d4bd in rb_ary_collect (ary=139650673749800) at array.c:2684#21 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127ea90, th=0x7f030c0455c0) at vm_insnhelper.c:1470#22 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f032127ea90, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#23 0x00007f032a0b8385 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:999#24 0x00007f032a0bb5ec in vm_exec (th=th@entry=0x7f030c0455c0) at vm.c:1304#25 0x00007f032a0c2cff in invoke_block_from_c (defined_class=80043920, cref=0x0, blockptr=0x0, argv=0x7f032117c6c8, argc=1, self=139650673695240, block=&lt;optimized out&gt;, th=0x7f030c0455c0) at vm.c:732#26 vm_yield (argv=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, th=&lt;optimized out&gt;) at vm.c:763#27 rb_yield_0 (argv=0x7f032117c6c8, argc=1) at vm_eval.c:938#28 rb_yield (val=&lt;optimized out&gt;) at vm_eval.c:948#29 0x00007f0329f15fa2 in rb_ary_each (array=139650673713080) at array.c:1792#30 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127f170, th=0x7f030c0455c0) at vm_insnhelper.c:1470#31 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f032127f170, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#32 0x00007f032a0b8385 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:999#33 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304#34 0x00007f032a0be89f in vm_invoke_proc (th=0x7f030c0455c0, proc=proc@entry=0x7f0304886b90, self=139650673713320, defined_class=78042600, argc=argc@entry=0, argv=argv@entry=0x7f0321182020, blockptr=0x0) at vm.c:788#35 0x00007f032a0be8da in rb_vm_invoke_proc (th=&lt;optimized out&gt;, proc=proc@entry=0x7f0304886b90, argc=argc@entry=0, argv=argv@entry=0x7f0321182020, blockptr=&lt;optimized out&gt;) at vm.c:807#36 0x00007f0329f6e550 in proc_call (argc=0, argv=0x7f0321182020, procval=139650673713160) at proc.c:734#37 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127f300, th=0x7f030c0455c0) at vm_insnhelper.c:1470#38 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f032127f300, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#39 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:1028#40 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304#41 0x00007f032a0be89f in vm_invoke_proc (th=th@entry=0x7f030c0455c0, proc=0x9676cb0, self=139650673658200, defined_class=79743600, argc=1, argv=argv@entry=0x7f032117d0a0, blockptr=0x0) at vm.c:788#42 0x00007f032a0bea45 in vm_call_bmethod_body (argv=0x7f032117d0a0, ci=0x7f032117d280, th=0x7f030c0455c0) at vm_insnhelper.c:1592#43 vm_call_bmethod (th=th@entry=0x7f030c0455c0, cfp=cfp@entry=0x7f032127f7b0, ci=ci@entry=0x7f032117d280) at vm_insnhelper.c:1607#44 0x00007f032a0bfc1e in vm_call_method (th=th@entry=0x7f030c0455c0, cfp=cfp@entry=0x7f032127f7b0, ci=ci@entry=0x7f032117d280) at vm_insnhelper.c:1774#45 0x00007f032a0bf691 in vm_call_opt_send (th=0x7f030c0455c0, reg_cfp=0x7f032127f7b0, ci=0x7f032117d280) at vm_insnhelper.c:1657#46 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:1028#47 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304#48 0x00007f032a0bec82 in vm_yield_with_cref (cref=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, th=0x7f030c0455c0) at vm.c:755#49 yield_under (under=&lt;optimized out&gt;, self=&lt;optimized out&gt;, values=139650673660760) at vm_eval.c:1531#50 0x00007f032a0bfae1 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127f9e0, th=0x7f030c0455c0) at vm_insnhelper.c:1470#51 vm_call_cfunc (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127f9e0, th=0x7f030c0455c0) at vm_insnhelper.c:1560#52 vm_call_method (th=0x7f030c0455c0, cfp=0x7f032127f9e0, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1754#53 0x00007f032a0b8385 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:999#54 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304#55 0x00007f032a0be89f in vm_invoke_proc (th=0x7f030c0455c0, proc=proc@entry=0x7f030010a480, self=83224000, defined_class=87087200, argc=argc@entry=2, argv=argv@entry=0x7f0321181c90, blockptr=0x0) at vm.c:788#56 0x00007f032a0be8da in rb_vm_invoke_proc (th=&lt;optimized out&gt;, proc=proc@entry=0x7f030010a480, argc=argc@entry=2, argv=argv@entry=0x7f0321181c90, blockptr=&lt;optimized out&gt;) at vm.c:807#57 0x00007f0329f6e550 in proc_call (argc=2, argv=0x7f0321181c90, procval=139651130810120) at proc.c:734#58 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127fa80, th=0x7f030c0455c0) at vm_insnhelper.c:1470#59 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f032127fa80, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#60 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:1028#61 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304#62 0x00007f032a0be89f in vm_invoke_proc (th=0x7f030c0455c0, proc=proc@entry=0x7f030010a560, self=87087800, defined_class=87087760, argc=argc@entry=1, argv=argv@entry=0x7f0321181c38, blockptr=0x0) at vm.c:788#63 0x00007f032a0be8da in rb_vm_invoke_proc (th=&lt;optimized out&gt;, proc=proc@entry=0x7f030010a560, argc=argc@entry=1, argv=argv@entry=0x7f0321181c38, blockptr=&lt;optimized out&gt;) at vm.c:807#64 0x00007f0329f6e550 in proc_call (argc=1, argv=0x7f0321181c38, procval=139651130809960) at proc.c:734#65 0x00007f032a0bfae1 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127fb20, th=0x7f030c0455c0) at vm_insnhelper.c:1470#66 vm_call_cfunc (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127fb20, th=0x7f030c0455c0) at vm_insnhelper.c:1560#67 vm_call_method (th=0x7f030c0455c0, cfp=0x7f032127fb20, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1754#68 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:1028#69 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304#70 0x00007f032a0be89f in vm_invoke_proc (th=0x7f030c0455c0, proc=proc@entry=0x7f030010a720, self=87087800, defined_class=87087760, argc=argc@entry=1, argv=argv@entry=0x7f0321181be0, blockptr=0x0) at vm.c:788#71 0x00007f032a0be8da in rb_vm_invoke_proc (th=&lt;optimized out&gt;, proc=proc@entry=0x7f030010a720, argc=argc@entry=1, argv=argv@entry=0x7f0321181be0, blockptr=&lt;optimized out&gt;) at vm.c:807#72 0x00007f0329f6e550 in proc_call (argc=1, argv=0x7f0321181be0, procval=139651130809560) at proc.c:734#73 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032127fbc0, th=0x7f030c0455c0) at vm_insnhelper.c:1470#74 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f032127fbc0, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#75 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:1028#76 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---#77 0x00007f032a0be89f in vm_invoke_proc (th=0x7f030c0455c0, proc=proc@entry=0x7f0304529c40, self=156831200, defined_class=50567600, argc=argc@entry=1, argv=argv@entry=0x7f0321181990, blockptr=0x0) at vm.c:788#78 0x00007f032a0be8da in rb_vm_invoke_proc (th=&lt;optimized out&gt;, proc=proc@entry=0x7f0304529c40, argc=argc@entry=1, argv=argv@entry=0x7f0321181990, blockptr=&lt;optimized out&gt;) at vm.c:807#79 0x00007f0329f6e550 in proc_call (argc=1, argv=0x7f0321181990, procval=139650673670240) at proc.c:734#80 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f0321280070, th=0x7f030c0455c0) at vm_insnhelper.c:1470#81 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f0321280070, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#82 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:1028#83 0x00007f032a0bb5ec in vm_exec (th=th@entry=0x7f030c0455c0) at vm.c:1304#84 0x00007f032a0c2cff in invoke_block_from_c (defined_class=76547880, cref=0x0, blockptr=0x0, argv=0x7f032117ee08, argc=1, self=34415960, block=&lt;optimized out&gt;, th=0x7f030c0455c0) at vm.c:732#85 vm_yield (argv=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, th=&lt;optimized out&gt;) at vm.c:763#86 rb_yield_0 (argv=0x7f032117ee08, argc=1) at vm_eval.c:938#87 rb_yield (val=&lt;optimized out&gt;) at vm_eval.c:948#88 0x00007f0329f15fa2 in rb_ary_each (array=139650673629520) at array.c:1792#89 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f03212801b0, th=0x7f030c0455c0) at vm_insnhelper.c:1470#90 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f03212801b0, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#91 0x00007f032a0b8385 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:999#92 0x00007f032a0bb5ec in vm_exec (th=th@entry=0x7f030c0455c0) at vm.c:1304#93 0x00007f032a0c2924 in invoke_block_from_c (defined_class=80036240, cref=0x0, blockptr=0x0, argv=0x7f032117f1c8, argc=1, self=120753520, block=&lt;optimized out&gt;, th=0x7f030c0455c0) at vm.c:732#94 vm_yield (argv=0x7f032117f1c8, argc=1, th=0x7f030c0455c0) at vm.c:763#95 rb_yield_0 (argv=0x7f032117f1c8, argc=1) at vm_eval.c:938#96 catch_i (tag=&lt;optimized out&gt;, data=data@entry=0) at vm_eval.c:1772#97 0x00007f032a0b4b4a in rb_catch_protect (t=&lt;optimized out&gt;, func=func@entry=0x7f032a0c26c0 &lt;catch_i&gt;, data=data@entry=0, stateptr=stateptr@entry=0x7f032117f340) at vm_eval.c:1858#98 0x00007f032a0b4bbc in rb_catch_obj (t=&lt;optimized out&gt;, func=func@entry=0x7f032a0c26c0 &lt;catch_i&gt;, data=data@entry=0) at vm_eval.c:1837#99 0x00007f032a0b4c6e in rb_f_catch (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;) at vm_eval.c:1823#100 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f03212802f0, th=0x7f030c0455c0) at vm_insnhelper.c:1470#101 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f03212802f0, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#102 0x00007f032a0b8385 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:999#103 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304#104 0x00007f032a0be89f in vm_invoke_proc (th=0x7f030c0455c0, proc=proc@entry=0x96c8680, self=133126360, defined_class=118126960, argc=argc@entry=2, argv=argv@entry=0x7f0321181070, blockptr=0x0) at vm.c:788#105 0x00007f032a0be8da in rb_vm_invoke_proc (th=&lt;optimized out&gt;, proc=proc@entry=0x96c8680, argc=argc@entry=2, argv=argv@entry=0x7f0321181070, blockptr=&lt;optimized out&gt;) at vm.c:807#106 0x00007f0329f6e550 in proc_call (argc=2, argv=0x7f0321181070, procval=133281320) at proc.c:734#107 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f0321280f70, th=0x7f030c0455c0) at vm_insnhelper.c:1470---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---#108 vm_call_cfunc (th=0x7f030c0455c0, reg_cfp=0x7f0321280f70, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#109 0x00007f032a0b8385 in vm_exec_core (th=th@entry=0x7f030c0455c0, initial=initial@entry=0) at insns.def:999#110 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0455c0) at vm.c:1304#111 0x00007f032a0be89f in vm_invoke_proc (th=th@entry=0x7f030c0455c0, proc=0x7f030c044ca0, self=133281640, defined_class=117256680, argc=0, argv=0x7f02f4c943a0, blockptr=blockptr@entry=0x0) at vm.c:788#112 0x00007f032a0be8da in rb_vm_invoke_proc (th=th@entry=0x7f030c0455c0, proc=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, blockptr=blockptr@entry=0x0) at vm.c:807#113 0x00007f032a0d475d in thread_start_func_2 (th=th@entry=0x7f030c0455c0, stack_start=&lt;optimized out&gt;) at thread.c:535#114 0x00007f032a0d4a9b in thread_start_func_1 (th_ptr=0x7f030c0455c0) at thread_pthread.c:803#115 0x00007f032990c182 in start_thread (arg=0x7f0321180700) at pthread_create.c:312#116 0x00007f0329c1d30d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111Thread 2 (Thread 0x7f0320f7e700 (LWP 9563)):#0 pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185#1 0x00007f032a0d6ce3 in native_cond_wait (mutex=0x8ef42b0, cond=0x8ef42d8) at thread_pthread.c:334#2 lock_func (timeout_ms=0, mutex=0x8ef42b0, th=0x7f030c0485d0) at thread.c:4324#3 rb_mutex_lock (self=120765160) at thread.c:4398#4 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032107ebb0, th=0x7f030c0485d0) at vm_insnhelper.c:1470#5 vm_call_cfunc (th=0x7f030c0485d0, reg_cfp=0x7f032107ebb0, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#6 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x7f030c0485d0, initial=initial@entry=0) at insns.def:1028#7 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0485d0) at vm.c:1304#8 0x00007f032a0be89f in vm_invoke_proc (th=0x7f030c0485d0, proc=proc@entry=0x96c8680, self=133126360, defined_class=118126960, argc=argc@entry=2, argv=argv@entry=0x7f0320f7f070, blockptr=0x0) at vm.c:788#9 0x00007f032a0be8da in rb_vm_invoke_proc (th=&lt;optimized out&gt;, proc=proc@entry=0x96c8680, argc=argc@entry=2, argv=argv@entry=0x7f0320f7f070, blockptr=&lt;optimized out&gt;) at vm.c:807#10 0x00007f0329f6e550 in proc_call (argc=2, argv=0x7f0320f7f070, procval=133281320) at proc.c:734#11 0x00007f032a0b3024 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032107ef70, th=0x7f030c0485d0) at vm_insnhelper.c:1470#12 vm_call_cfunc (th=0x7f030c0485d0, reg_cfp=0x7f032107ef70, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1560#13 0x00007f032a0b8385 in vm_exec_core (th=th@entry=0x7f030c0485d0, initial=initial@entry=0) at insns.def:999#14 0x00007f032a0bb5ec in vm_exec (th=0x7f030c0485d0) at vm.c:1304#15 0x00007f032a0be89f in vm_invoke_proc (th=th@entry=0x7f030c0485d0, proc=0x7f030c047b40, self=133281640, defined_class=117256680, argc=0, argv=0x7f02f4cd2290, blockptr=blockptr@entry=0x0) at vm.c:788#16 0x00007f032a0be8da in rb_vm_invoke_proc (th=th@entry=0x7f030c0485d0, proc=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, blockptr=blockptr@entry=0x0) at vm.c:807#17 0x00007f032a0d475d in thread_start_func_2 (th=th@entry=0x7f030c0485d0, stack_start=&lt;optimized out&gt;) at thread.c:535#18 0x00007f032a0d4a9b in thread_start_func_1 (th_ptr=0x7f030c0485d0) at thread_pthread.c:803#19 0x00007f032990c182 in start_thread (arg=0x7f0320f7e700) at pthread_create.c:312#20 0x00007f0329c1d30d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111Thread 1 (Thread 0x7f032a5c2740 (LWP 8999)):#0 pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185#1 0x00007f032a0d2390 in native_cond_wait (mutex=0x1ed96d0, cond=&lt;optimized out&gt;) at thread_pthread.c:334#2 native_sleep (th=th@entry=0x1ed95b0, timeout_tv=0x0) at thread_pthread.c:1059#3 0x00007f032a0d618a in sleep_forever (deadlockable=1, spurious_check=0, th=0x1ed95b0) at thread.c:996#4 thread_join_sleep (arg=arg@entry=140734195440096) at thread.c:787#5 0x00007f0329f67d97 in rb_ensure (b_proc=b_proc@entry=0x7f032a0d6090 &lt;thread_join_sleep&gt;, data1=data1@entry=140734195440096, e_proc=e_proc@entry=0x7f032a0cfa10 &lt;remove_from_join_list&gt;, data2=data2@entry=140734195440096) at eval.c:850#6 0x00007f032a0d0cd0 in thread_join (delay=&lt;optimized out&gt;, target_th=0x96c9f10) at thread.c:829#7 thread_join_m (argc=&lt;optimized out&gt;, argv=0x7f032a4c1168, self=&lt;optimized out&gt;) at thread.c:909#8 0x00007f032a0bfae1 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032a5c0ca0, th=0x1ed95b0) at vm_insnhelper.c:1470#9 vm_call_cfunc (ci=&lt;optimized out&gt;, reg_cfp=0x7f032a5c0ca0, th=0x1ed95b0) at vm_insnhelper.c:1560#10 vm_call_method (th=0x1ed95b0, cfp=0x7f032a5c0ca0, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1754#11 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x1ed95b0, initial=initial@entry=0) at insns.def:1028#12 0x00007f032a0bb5ec in vm_exec (th=th@entry=0x1ed95b0) at vm.c:1304#13 0x00007f032a0c2cff in invoke_block_from_c (defined_class=51649880, cref=0x0, blockptr=0x0, argv=0x7fff3bba1e08, argc=1, self=51647880, block=&lt;optimized out&gt;, th=0x1ed95b0) at vm.c:732#14 vm_yield (argv=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, th=&lt;optimized out&gt;) at vm.c:763#15 rb_yield_0 (argv=0x7fff3bba1e08, argc=1) at vm_eval.c:938#16 rb_yield (val=val@entry=52428600) at vm_eval.c:948#17 0x00007f0329fcb079 in rb_obj_tap (obj=52428600) at object.c:675#18 0x00007f032a0bfae1 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032a5c0e30, th=0x1ed95b0) at vm_insnhelper.c:1470#19 vm_call_cfunc (ci=&lt;optimized out&gt;, reg_cfp=0x7f032a5c0e30, th=0x1ed95b0) at vm_insnhelper.c:1560#20 vm_call_method (th=0x1ed95b0, cfp=0x7f032a5c0e30, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1754#21 0x00007f032a0b8385 in vm_exec_core (th=th@entry=0x1ed95b0, initial=initial@entry=0) at insns.def:999#22 0x00007f032a0bb5ec in vm_exec (th=0x1ed95b0) at vm.c:1304#23 0x00007f032a0c43c9 in rb_iseq_eval (iseqval=&lt;optimized out&gt;) at vm.c:1549#24 0x00007f0329f6a630 in rb_load_internal0 (th=0x1ed95b0, fname=52100160, wrap=wrap@entry=0) at load.c:615#25 0x00007f0329f6bdde in rb_load_internal (wrap=0, fname=&lt;optimized out&gt;) at load.c:644#26 rb_require_safe (fname=52099680, safe=0) at load.c:996#27 0x00007f032a0bfae1 in vm_call_cfunc_with_frame (ci=&lt;optimized out&gt;, reg_cfp=0x7f032a5c0f70, th=0x1ed95b0) at vm_insnhelper.c:1470#28 vm_call_cfunc (ci=&lt;optimized out&gt;, reg_cfp=0x7f032a5c0f70, th=0x1ed95b0) at vm_insnhelper.c:1560#29 vm_call_method (th=0x1ed95b0, cfp=0x7f032a5c0f70, ci=&lt;optimized out&gt;) at vm_insnhelper.c:1754#30 0x00007f032a0b7a44 in vm_exec_core (th=th@entry=0x1ed95b0, initial=initial@entry=0) at insns.def:1028#31 0x00007f032a0bb5ec in vm_exec (th=th@entry=0x1ed95b0) at vm.c:1304#32 0x00007f032a0c4636 in rb_iseq_eval_main (iseqval=iseqval@entry=52095080) at vm.c:1562#33 0x00007f0329f6510a in ruby_exec_internal (n=0x31ae868) at eval.c:253#34 0x00007f0329f670ad in ruby_exec_node (n=n@entry=0x31ae868) at eval.c:318#35 0x00007f0329f694dc in ruby_run_node (n=0x31ae868) at eval.c:310#36 0x000000000040088b in main (argc=3, argv=0x7fff3bba2b88) at main.c:36I then thought I might get a bit more info if I dumped the current Ruby backtrace:(gdb) call (void) close(1)(gdb) call (void) close(2)(gdb) shell tty/dev/pts/15 &lt;--- this will likely be different on your machine(gdb) call (int) open(\"/dev/pts/15\", 2, 0)$1 = 1(gdb) call (int) open(\"/dev/pts/15\", 2, 0)$2 = 2(gdb) call (void)rb_backtrace()This now shows me what’s going on inside my Ruby process: gems/ruby-2.1.1/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in `block in spawn_thread' gems/ruby-2.1.1/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in `call' gems/ruby-2.1.1/gems/puma-2.8.2/lib/puma/server.rb:254:in `block in run' gems/ruby-2.1.1/gems/puma-2.8.2/lib/puma/server.rb:361:in `process_client' gems/ruby-2.1.1/gems/puma-2.8.2/lib/puma/server.rb:490:in `handle_request' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/content_length.rb:14:in `call' gems/ruby-2.1.1/gems/railties-4.1.1/lib/rails/application.rb:144:in `call' gems/ruby-2.1.1/gems/railties-4.1.1/lib/rails/engine.rb:514:in `call' gems/ruby-2.1.1/gems/airbrake-4.0.0/lib/airbrake/user_informer.rb:12:in `call' gems/ruby-2.1.1/gems/airbrake-4.0.0/lib/airbrake/user_informer.rb:16:in `_call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/sendfile.rb:112:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/static.rb:64:in `call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/lock.rb:17:in `call' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/cache/strategy/local_cache_middleware.rb:26:in `call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/runtime.rb:17:in `call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/methodoverride.rb:21:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/request_id.rb:21:in `call' gems/ruby-2.1.1/gems/quiet_assets-1.0.2/lib/quiet_assets.rb:18:in `call_with_quiet_assets' gems/ruby-2.1.1/gems/railties-4.1.1/lib/rails/rack/logger.rb:20:in `call' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/tagged_logging.rb:68:in `tagged' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/tagged_logging.rb:26:in `tagged' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/tagged_logging.rb:68:in `block in tagged' gems/ruby-2.1.1/gems/railties-4.1.1/lib/rails/rack/logger.rb:20:in `block in call' gems/ruby-2.1.1/gems/railties-4.1.1/lib/rails/rack/logger.rb:38:in `call_app' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/show_exceptions.rb:30:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/debug_exceptions.rb:17:in `call' gems/ruby-2.1.1/gems/airbrake-4.0.0/lib/airbrake/rails/middleware.rb:13:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/remote_ip.rb:76:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/reloader.rb:73:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/callbacks.rb:27:in `call' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:82:in `run_callbacks' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/callbacks.rb:29:in `block in call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/cookies.rb:560:in `call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/session/abstract/id.rb:220:in `call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/session/abstract/id.rb:225:in `context' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/flash.rb:254:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/middleware/params_parser.rb:27:in `call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/head.rb:11:in `call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/conditionalget.rb:25:in `call' gems/ruby-2.1.1/gems/rack-1.5.2/lib/rack/etag.rb:23:in `call' gems/ruby-2.1.1/gems/warden-1.2.3/lib/warden/manager.rb:34:in `call' gems/ruby-2.1.1/gems/warden-1.2.3/lib/warden/manager.rb:34:in `catch' gems/ruby-2.1.1/gems/warden-1.2.3/lib/warden/manager.rb:35:in `block in call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/routing/route_set.rb:676:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/journey/router.rb:59:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/journey/router.rb:59:in `each' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/journey/router.rb:71:in `block in call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/routing/route_set.rb:48:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/routing/route_set.rb:80:in `dispatch' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_dispatch/routing/route_set.rb:80:in `call' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_controller/metal.rb:231:in `block in action' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_controller/metal/rack_delegation.rb:13:in `dispatch' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_controller/metal.rb:195:in `dispatch' gems/ruby-2.1.1/gems/actionview-4.1.1/lib/action_view/rendering.rb:30:in `process' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/abstract_controller/base.rb:136:in `process' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_controller/metal/params_wrapper.rb:250:in `process_action' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_controller/metal/instrumentation.rb:30:in `process_action' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/notifications.rb:159:in `instrument' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/notifications/instrumenter.rb:20:in `instrument' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/notifications.rb:159:in `block in instrument' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_controller/metal/instrumentation.rb:31:in `block in process_action' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/action_controller/metal/rescue.rb:29:in `process_action' gems/ruby-2.1.1/gems/actionpack-4.1.1/lib/abstract_controller/callbacks.rb:19:in `process_action' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:86:in `run_callbacks' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:86:in `call' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:166:in `block in halting' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:166:in `call' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:160:in `block in halting' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:160:in `call' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:440:in `block in make_lambda' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/callbacks.rb:440:in `instance_exec' gems/ruby-2.1.1/gems/mongoid_userstamp-0.3.2/lib/mongoid/userstamp/railtie.rb:15:in `block (2 levels) in &lt;class:Railtie&gt;' gems/ruby-2.1.1/gems/devise-3.2.4/lib/devise/controllers/helpers.rb:58:in `current_user' gems/ruby-2.1.1/gems/warden-1.2.3/lib/warden/proxy.rb:104:in `authenticate' gems/ruby-2.1.1/gems/warden-1.2.3/lib/warden/proxy.rb:318:in `_perform_authentication' gems/ruby-2.1.1/gems/warden-1.2.3/lib/warden/proxy.rb:212:in `user' gems/ruby-2.1.1/gems/warden-1.2.3/lib/warden/session_serializer.rb:34:in `fetch' gems/ruby-2.1.1/gems/devise-3.2.4/lib/devise.rb:462:in `block (2 levels) in configure_warden!' gems/ruby-2.1.1/gems/devise-3.2.4/lib/devise/models/authenticatable.rb:208:in `serialize_from_session' gems/ruby-2.1.1/gems/orm_adapter-0.5.0/lib/orm_adapter/adapters/mongoid.rb:22:in `get' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/contextual.rb:20:in `first' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/contextual/mongo.rb:197:in `first' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/contextual/mongo.rb:447:in `try_cache' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/contextual/mongo.rb:198:in `block in first' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/contextual/mongo.rb:535:in `with_sorting' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/contextual/mongo.rb:199:in `block (2 levels) in first' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/query_cache.rb:186:in `first_with_cache' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/query_cache.rb:135:in `with_cache' gems/ruby-2.1.1/bundler/gems/mongoid-49bc68fd3011/lib/mongoid/query_cache.rb:187:in `block in first_with_cache' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/query.rb:127:in `first' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/read_preference/primary.rb:54:in `with_node' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/read_preference/selectable.rb:65:in `with_retry' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/read_preference/selectable.rb:65:in `call' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/read_preference/primary.rb:55:in `block in with_node' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/cluster.rb:240:in `with_primary' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/cluster.rb:151:in `nodes' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/cluster.rb:194:in `refresh' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/cluster.rb:194:in `each' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/cluster.rb:182:in `block in refresh' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:432:in `refresh' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:90:in `command' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:648:in `read' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/operation/read.rb:48:in `execute' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:391:in `process' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:587:in `flush' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:616:in `logging' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/instrumentable.rb:31:in `instrument' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/notifications.rb:159:in `instrument' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/notifications/instrumenter.rb:20:in `instrument' gems/ruby-2.1.1/gems/activesupport-4.1.1/lib/active_support/notifications.rb:159:in `block in instrument' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:617:in `block in logging' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:589:in `block in flush' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:187:in `ensure_connected' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:184:in `rescue in ensure_connected' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/failover/retry.rb:29:in `execute' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:114:in `connection' gems/ruby-2.1.1/gems/connection_pool-2.0.0/lib/connection_pool.rb:58:in `with' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:115:in `block in connection' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/failover/retry.rb:30:in `block in execute' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/node.rb:590:in `block (2 levels) in flush' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection.rb:172:in `write' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection.rb:220:in `with_connection' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection.rb:52:in `connect' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection/socket/connectable.rb:144:in `connect' rubies/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:106:in `timeout' rubies/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:35:in `catch' rubies/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:35:in `catch' rubies/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:35:in `block in catch' rubies/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:91:in `block in timeout' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection/socket/connectable.rb:145:in `block in connect' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection/socket/connectable.rb:145:in `new' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection/socket/tcp.rb:20:in `initialize' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection/socket/connectable.rb:119:in `handle_socket_errors' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection/socket/tcp.rb:20:in `block in initialize' gems/ruby-2.1.1/bundler/gems/moped-074ba070aa98/lib/moped/connection/socket/tcp.rb:20:in `initialize'Looks like the whole think started with a moped socket connection (which makes sense).Investigating further …" }, { "title": "Accessing a running process' STDOUT", "url": "/blog/2014/06/02/accessing-stdout-of-a-running-process/", "categories": "Linux", "tags": "linux, ruby, rake", "date": "2014-06-02 09:16:54 -0400", "snippet": "I’m currently doing a lot of scripting at work. Although the primary environment is Windows, I have to manage some Linux processes as well.One example is a series of PowerShell scripts I’ve written...", "content": "I’m currently doing a lot of scripting at work. Although the primary environment is Windows, I have to manage some Linux processes as well.One example is a series of PowerShell scripts I’ve written which abstract away various ETL tasks that we need in order to get legacy data extracted, updated and inserted into other databases.One of the longer running processes I have is a Ruby Rake task that processes an uploaded CSV file. This can potentially take a long time, and although I could modify the script to feed progress data back, I’d like to demonstrate how to follow-up from another terminal session.Any Rake task I write that can potentially take a while, I tend to jazz up a bit with progress details. For this current example, I am using the progress_bar gem, which writes an ASCII progress bar to the terminal along with some other useful progress information.If I were to execute the task directly, I could watch the progress directly. Since we’re running this script remotely though, this information isn’t directly accessible.In order to gain access to the progress info for this task, we’ll need to access the process’ STDOUT from an alternate session.The first step is to find the PID of the process we would like:ps aux | grep ruby&gt;&gt; 1001 16544 67.7 29.4 1319420 506456 ? Rl 13:00 3:02 ruby rake phoenix:sync_locations_from_csvNow that we have the PID, we can pass this to strace in order to gain a bit more insight into the current process.sudo strace -p 16544 -s 80 -e write 2&gt;&amp;1 | grep \"write(2, \\\"\\[\"We’re redirecting the output of strace in order to further filter the results using grep. Depending on what process you’re looking to monitor you may need to adjust your grep conditions.You can also exclude the redirect and grep entirely, but this may result in too much information to be useful ;)" }, { "title": "Redmine Plugin Extension and Development: Published!!!", "url": "/blog/2014/03/19/redmine-plugin-extension-and-development-published/", "categories": "Writing", "tags": "redmine, book", "date": "2014-03-19 06:08:02 -0400", "snippet": "Well, it’s official. The journey I began over six months ago has finally come to an end, and Redmine Plugin Extension and Development has officially been published :)I plan on doing a “post-mortem”...", "content": "Well, it’s official. The journey I began over six months ago has finally come to an end, and Redmine Plugin Extension and Development has officially been published :)I plan on doing a “post-mortem” on the process at some point, but for now I’m planning on taking a little time off from writing." }, { "title": "Production Timeline", "url": "/blog/2014/03/03/8748os-production-timeline/", "categories": "Writing", "tags": "redmine, book", "date": "2014-03-03 08:22:30 -0500", "snippet": "Well, it’s been a long (longer than anticipated) journey, but Redmine Plugin Extension and Development (code number 8748OS) is in the final stages of production.Since I wanted to share a bit more d...", "content": "Well, it’s been a long (longer than anticipated) journey, but Redmine Plugin Extension and Development (code number 8748OS) is in the final stages of production.Since I wanted to share a bit more detail about what went into the production of this book, I’m attaching a (slightly edited) copy of the production timeline I received from my Technical Editor.Technical Editing / Copy EditingWe would be editing chapters for language and content-related clarity. There might be instances when we’ll require your assistance while we make any content-related changes. Even if there are no queries, we would be sending our first edited chapter (Per TE) to you to ensure that you are happy with the quality of editing. When chapter are sent across to you, the expected turnaround time will be 24 hours per chapter. Start of Editing: 21st February 2014 End of Editing: 6th March 2014After the chapters are edited, they are indexed and then laid out. The laid out chapters are then sent across to the Proof Reader, who are experts in English language. The Proof read chapters are then finalized and made into what we term as “Pre-Finals”. This is the stage where-in you will get to see the final chapters as you would see them in the book.Schedule for Pre-finals2 to 4 Pre-Finals will be sent across to you in a batch. The expected turnaround time will be 24 hours for a batch of two chapters.At this stage, you’ll be expected to just go through the chapters and provide feedback, if any. Any major content changes should be avoided. We plan to start sending these within 7-10 days from today and would continue to do so, until the day the book is clubbed and finalized.Upload DateThis is the day when the clubbed book will be finalized and then sent to our printers. We call it the upload day, since the files are transferred to our printers and we cannot make any changes here after. Current Upload Date: 17th March 2014Publishing DateThe book gets published and will be available for purchase in five to eight working days from the day of upload.So all going well, it looks like the book will officially be available by the end of March 2014. For those who were anticipating the original January 2014 release date, I sincerely apologize for the delay. I felt the delay resulted in a more cohesive final product, and hope that it meets (and even exceeds) your expectations!" }, { "title": "Final Rewrites Over 50% Done", "url": "/blog/2014/02/18/final-rewrites-over-50-percent-done/", "categories": "Writing", "tags": "redmine, book", "date": "2014-02-18 22:08:21 -0500", "snippet": "This is just a quick post to update everyone that chapters 1, 2 and 3 of Redmine Plugin Extension and Development have been submitted to Packt with final revisions. Chapter 8 as well as the Appendi...", "content": "This is just a quick post to update everyone that chapters 1, 2 and 3 of Redmine Plugin Extension and Development have been submitted to Packt with final revisions. Chapter 8 as well as the Appendix were accepted after the initial round of rewrites, so that means 4 MORE CHAPTERS TO GO!!!Clearly I didn’t meet the target I hinted at in my last post, but it was Family Day here so I took the family away for the long weekend to go skating on the Rideau Canal in Ottawa :)" }, { "title": "First Round of Rewrites Completed!", "url": "/blog/2014/02/09/first-round-of-rewrites-completed/", "categories": "Writing", "tags": "redmine, book", "date": "2014-02-09 21:48:29 -0500", "snippet": "This is just a quick update that I’ve finally finished the first round of rewrites for the upcoming Redmine Plugin Extension and Development book.The issue I was struggling with was little more tha...", "content": "This is just a quick update that I’ve finally finished the first round of rewrites for the upcoming Redmine Plugin Extension and Development book.The issue I was struggling with was little more than a complete lack of motivation. Since I work full time and do contract development on the side (plus have a wife and kids), yet another distraction can be difficult to find time for; especially if it’s not a primary source of income.Packt Publishing has been very patient with me though so I’m going to try to find the time in the coming week to get through the second round of revisions and rewrites and hopefully get this puppy to market ;)" }, { "title": "My First Book: Almost Ready for Production", "url": "/blog/2014/01/27/my-first-book-is-almost-ready-for-production/", "categories": "Writing", "tags": "redmine, book", "date": "2014-01-27 15:37:10 -0500", "snippet": "For the last 4 months I’ve been working with Packt Publishing on a book about Redmine plugin extension and development.Although it likely won’t be ready until February 2014, the book is now availab...", "content": "For the last 4 months I’ve been working with Packt Publishing on a book about Redmine plugin extension and development.Although it likely won’t be ready until February 2014, the book is now available for pre-order on Packt’s site.Since the book is almost through the first stage of rewrites, I though it might be a good idea to start documenting the process.What I’m going to write about initially are my first impressions on writing a book, as well as a few notes (which may sound like complaints) about what I did “wrong” and could improve upon in the future.Originally, I wasn’t sure that I would be a good choice as an author as I didn’t have the community recognition that some other plugin developers had, and I hadn’t been “on the scene” for more than a few years.After much consideration, I decided to dive in and start writing.The first step was to provide the publisher with a rough outline that also included the page count. Initially, I just threw together a handful of topics that I wanted to cover, then (very generically) estimated how many pages I would need to completely cover those topics.Once approved, and I actually started writing, I very quickly realized that meeting my page estimates was almost impossible; some chapters would be way too short, and others WAY too long.I also had to provide a rough outline as to how long I thought it would take me to write each chapter. This step, in retrospect, I wish I had taken more time to properly prepare.Writing a technically-oriented book involves a lot more research than you may think (no matter how well you think you know the source material). As such, the further I got into the book, the more time I found myself researching, writing code, testing code, then writing about the code I’d built.This process took a good 3x longer than I anticipated, and as such, my deadlines tended to slip by. As a result, the tentative release date for the final book of January 2014 is not very likely.I’m hoping that this blog post will also serve as a motivator to get my ass in gear and finish the final revisions." }, { "title": "Developer Braindump: Warcraft", "url": "/blog/2013/03/26/developer-braindump-warcraft/", "categories": "Game Development", "tags": "rts", "date": "2013-03-26 12:50:00 -0400", "snippet": "In an effort to centralize some articles about how some classic games were devloped, I’ve collected the various parts of Patrick Wyatt’s blog series about the initial development of Warcraft.Please...", "content": "In an effort to centralize some articles about how some classic games were devloped, I’ve collected the various parts of Patrick Wyatt’s blog series about the initial development of Warcraft.Please note that this version of the series is just meant to capture the content, and not all formatting changes have been captured.Please see the original posts for the full experience ;)Back before the dawn of time, which is to say when PC games were written for the DOS operating system, I got to work on a game called Warcraft.I get to lead a project!While I had developed several PC games, a couple of Mac games, and seven console titles for the Super Nintendo and Sega Genesis, I was either in a junior role on those projects, or the projects were game “ports” rather than original development work. A game “port” is the process of moving a game from one platform, like the Amiga, and converting the code, design, artwork and other game assets to make them work on another, like the Nintendo.My role encompassed two jobs: leading the development team as Producer — a game industry term for project manager, designer, evangelist, and cat herder — and writing the majority of the game code as Lead Programmer. This was perhaps less daunting then, when a game project might employ ten or twenty developers, than it is now, with development teams tipping the scales at two-hundred or more developers.The source of WarcraftThe developers at the startup company I worked for — then named Silicon &amp; Synapse but later renamed Blizzard in a nod towards our tempestuous development methodology — played a great many games during our free time. And from that game-playing came the spark to create Warcraft.We were inspired to create Warcraft after playing (and replaying and replaying) a game called Dune 2, by Westwood Studios. Dune 2 was arguably the first modern real-time strategy (RTS) game; with a scrolling world map, real-time unit construction and movement, and individual unit combat. It isn’t that much different in design than a modern RTS like Starcraft 2, excepting perhaps a certain scale and graphics quality.Its predecessor, Dune 1 — a very worthy game itself — shared some of the same elements, but its semi-real-time unit combat was wrapped inside an adventure game. Dune 2 stripped its predecessors’ idea of the player representing a character inside the game-world and focused exclusively on the modern RTS mechanics: harvesting resources, building a base, harvesting more resources, building an army, and finally, finding and conquering the enemy.Along with the other folks at Blizzard I exhaustively played Dune 2 during lunch breaks and after work, playing each of the three competing races to determine their strengths and weaknesses; and afterward comparing play-styles, strategies and tactics with others in the office.While the game was great fun, it suffered from several obvious defects that called out (nay, screamed) to be fixed. Most notably, the only way that my friends and I could play the game was against the computer. It was obvious that this gaming style would be ideal as a multiplayer game. Unlike turn-based games, where each player must wait for all opponents to issue unit movement orders, a real-time game would enable all players to give orders simultaneously, placing a premium on rapid, decisive tactical movements over long, drawn-out strategic planning.And with that singular goal in mind, development of the game began without any serious effort to plan the game design, evaluate the technical requirements, build the schedule, or budget for the required staff. Not even on a napkin. Back at Blizzard we called this the “business plan du jour”, which was or standard operating methodology.Initial developmentAs the sole developer on the project, and lacking an art team during the initial phase, I screen-captured the artwork of Dune 2 to use until such time as my forward progress warranted an artist or two. The artists were tied up working on any number of other pressing deadlines and didn’t need distractions at this point — we were always pressed for time.My early programming efforts developing the game engine included creating a tile-based scrolling map renderer, a sprite renderer to draw game units and other bitmaps, a sprite-sequencing engine to animate game units, an event-dispatcher to post mouse and keyboard events, a game-dispatcher to control unit-behavior, and a great deal of user-interface code to control application behavior. With this subset of the project completed in the first few weeks it became possible to “play” a solo game, though I didn’t implement unit-construction until sometime later; early play required using typed commands to spawn units on screen.Each day I’d build upon the previous efforts in organic fashion. Without schedule milestones or an external driver for the project, I was in the enviable position of choosing which features to build next, which made me incredibly motivated. I already enjoyed game development, and getting to do this green-field programming was like a drug. Even now, some 22 years after getting into the game industry, I still love the creative aspects of programming.The first unique feature: multi-unit selectionOne feature of which I was particularly proud was unit-selection. Unlike Dune 2, which only allowed the user to select a single unit at a time, and which necessitated frenzied mouse-clicking to initiate joint-unit tactical combat, it was obvious that enabling players to select more than one unit would speed task-force deployment and dramatically improve game combat.Before I started in the game industry I had worked extensively with several low-end “Computer Assisted Design” (CAD) programs like MacDraw and MacDraft to design wine-cellars for my dad’s wine cellar business, so it seemed natural to use the “click &amp; drag” rectangle-selection metaphor to round up a group of units to command.I believe that Warcraft was the first game to use this user-interface metaphor. When I first implemented the feature it was possible to select and control large numbers of units at a time; there was no upper limit on the number of units that could be selected.While selecting and controlling one hundred units at a time demonstrated terrible weaknesses in the simple path-finding algorithm I had implemented, after I got the basic algorithms working I nevertheless spent hours selecting units and dispatching game units to destinations around the map instead of writing more code; it was the coolest feature I had ever created in my programming career up to that time!Later in the development process, and after many design arguments between team-members, we decided to allow players to select only four units at a time based on the idea that users would be required to pay attention to their tactical deployments rather than simply gathering a mob and sending them into the fray all at once. We later increased this number to nine in Warcraft II. Command and Conquer, the spiritual successor to Dune 2, didn’t have any upper bound on the number of units that could be selected. It’s worth another article to talk about the design ramifications, for sure.Apart from the ability to control multiple units at one time, at this phase Warcraft resembled nothing so much as a stripped-down version of Dune 2, so much so that I defensively joked that, while Warcraft was certainly inspired by Dune 2, the game was radically different — our radar minimap was in the upper-left corner of the screen, whereas theirs was in the lower-right corner.The formation of the fellowshipBy early 1994, I had made enough progress to warrant additional help on the project. I was joined by Ron Millar, Sam Didier, Stu Rose, Bob Fitch, Jesse McReynolds, Mike Morhaime, Mickey Nielsen, and others. Many of these folks started work on the game after our company was acquired by Davidson &amp; Associates in February 1994.Ron Millar, who, with his long blond hair and strong build, was obviously the progeny of Viking warriors. He was originally hired on as an artist based on his skill in creating artwork for Gameboy titles at Virgin Games, but his amazing creativity and design sensibilities led to his taking on a design role in many Blizzard projects, and he stepped into a similar role for Warcraft.Sam Didier, a strong, stocky and stalwart character who resembled nothing so much as a bear scaled down to human proportions, and whose heroic characters and epic drawings are now the definitive art style for Blizzard games, had honed his computer drawing skills on sixteen-bit console titles, but his penchant for drawing fantasy artwork during meetings and at any other spare moment demonstrated his capability to lead the art direction for this new title.Stu Rose — whose background as an illustrator led to his design of the Blizzard logo still used today — initially contributed to the background tile-map artwork, but he would later take on a critical role in the ultimate design of Warcraft. Stu is quite memorable as a voice actor in the role of Human Peon Peasant, where his rendition of a downtrodden brute-laborer was comedic genius.Bob Fitch had started work as a programmer and project lead on another title at the same time I started development of Warcraft. Allen Adham, the president of Blizzard, had assigned Bob the task of building a word game called “Games People Play” that would include crossword, scramble, boggle, and other similar diversions. Bob’s notable lack of enthusiasm for the project resulted in his making little progress on the title for many months; with Warcraft showing well Bob was released to assist me, and his enthusiasm for the game helped move the project forward more rapidly.Jesse, a Caltech graduate, started work on building a network driver for the IPX network protocol so the game could be played on a Local Area Network (LAN). Mike Morhaime, one of the two co-founders of Blizzard, later took on the significantly more difficult task of writing a “mixed-mode” modem driver. While Warcraft was a DOS “Protected Mode” game, the modem driver could be called from both Protected Mode and Real Mode due to quirks in the DOS operating system and the 80386 chip-architecture it ran on, so he could regularly be found in his office staring at screens full of diagnostic numbers as he worked through the complicated timing issues related to re-entrant code. At the end of the day, the modem code was rock-solid, quite an achievement given the primitive toolset we had at the time.Warcraft artAllen Adham hoped to obtain a license to the Warhammer universe to try to increase sales by brand recognition. Warhammer was a huge inspiration for the art-style of Warcraft, but a combination of factors, including a lack of traction on business terms and a fervent desire on the part of virtually everyone else on the development team (myself included) to control our own universe nixed any potential for a deal. We had already had terrible experiences working with DC Comics on “Death and Return of Superman” and “Justice League Task Force”, and wanted no similar issues for our new game.It’s surprising now to think what might have happened had Blizzard not controlled the intellectual property rights for the Warcraft universe — it’s highly unlikely Blizzard would be such a dominant player in the game industry today.Years after the launch of Warcraft my dad, upon returning from a trip to Asia, gave me a present of a set of Warhammer miniatures in the form of a skeleton charioteer and horses with the comment: “I found these cool toys on my trip and they reminded me a lot of your game; you might want to have your legal department contact them because I think they’re ripping you off.” Hmmm!Blockers to game developmentOne interesting facet of the early development process was that, while I was building a game that would be playable using modems or a local area network, the company had no office LAN. Because we developed console titles, which would easily fit on a floppy disk, it wasn’t something that was necessary, though it would certainly have simplified making backups.So when I started collaborating with other artists and programmers, we used the “sneaker network”, carrying floppy disks back and forth between offices to integrate source code revisions and artwork.Bob Fitch was the second programmer on the project, and he and I would regularly copy files and code-changes back and forth. Periodically we’d make integration mistakes and a bug we fixed would re-appear. We’d track it down and discover that — during file-copying while integrating changes — we had accidentally overwritten the bug fix, and we’d have to remember how we had fixed it previously.This happened more than a few times because of the rapidity with which we developed code and our lack of any processes to handle code-integration other than “remembering” which files we had worked on. I was somewhat luckier in this regard in that my computer was the “master” system upon which we performed all the integrations, so my changes were less likely to get lost. These days we use source-control to avoid such stupidities, but back then we didn’t even know what it was!With more programmers, designers and artists working on the title progress increased substantially, but we also discovered a big blocker to our progress. The game was initially developed in DOS “Real Mode”, which meant that only 640K of memory was available, less about 120K for the operating system. Can you believe how crap computers were back then!?!As the art team started creating game units, backgrounds and user-interface artwork, we rapidly burned through all of the memory and started looking for alternatives. A first attempt at a solution was to use EMS “paged memory” mapping and store art resources “above” the 640K memory barrier.Stories programmers tell about EMS memory are like those that old folks tell about walking uphill to school, barefoot, in the snow, both ways, except that EMS stories are even more horrible, and actually true.In any event the EMS solution quite fortunately didn’t work; it turned out there was a better solution. A company called Watcom released a C compiler which included a DOS-mode “extender” that allowed programs to be written in “Protected Mode” with access to linear 32-bit memory, something every programmer takes for granted today when they write 32-bit (or even 64-bit applications). While it required a couple of days to update the source code, the DOS-mode extender worked perfectly, and we were back in business, now with access to substantially more memory.Initial proposalBlizzard was working on at least four other games when I started on the Warcraft project, and as the company numbered only 20 everyone was mega-busy keeping those projects on track. It wasn’t uncommon for artists, programmers and designers to be working on two or sometimes three projects at a time, and of course our sole musician/sound-engineer, Glenn Stafford, worked on everything.But we regularly found time to meet in large groups to brainstorm and discuss company strategy, so much that we called our efforts the “business plan du jour“.I already discussed our motivation to create a Real Time Strategy (RTS) game modeled after Dune II in a previous article, but one other key idea propelled us forward.The other impetus for the game started with a proposal that Allen Adham — president and company co-founder — made during one of our brainstorming sessions. He wanted to build a series of war games that would be released in near identical white boxes under the banner “Warcraft”, with subheadings announcing the historical context for each game: The Roman Empire, The Vietnam War, and so forth.The goal with the identical boxes was to control a large section of shelf-space that would be easy for players to spot in a crowded retail environment, similar to the Gold Box series of Dungeons &amp; Dragons games from SSI, which enjoyed great success during the late 1980′s. New players would be drawn to the section of games by its dominating shelf-presence, and veteran players who enjoyed one game they would know where to find the next. I know; retail: so archaic compared to app stores and Amazon, right?!?Ron Millar and Sam Didier, two of the early artists to work at the company, weren’t excited about the idea of working on historical simulations, they enjoyed fantasy games like Warhammer and Dungeons &amp; Dragons. One glance at Sam’s artwork is enough to demonstrate his passion for the fantasy milieu. So at a later meeting they proposed the idea that the first game should be set in a high-fantasy world of Orcs and humans, where they’d have more opportunity to create innovative game artwork instead of being required to conform to the tenets of historical accuracy. The idea took hold, with the first game in the series becoming Warcraft: Orcs and Humans.Initial game designMany people believe that a game designer is solely responsible for all idea conception and actually “creates the game design”, and this may be true for some development teams. Designers do need to be highly creative and bring to life many of the elements of the game personally.But equally important is for designers to be receptive to the ideas of others: without some involvement in the game’s design the rest of the team has less motivation to do their best work. And beyond that, it’s never possible to know where the next great design idea is going to come from. It’s critical for designers to listen so that the best ideas of others aren’t stifled.Our informal design process during the early period of Warcraft’s development worked effectively in that regard. Many brainstorming sessions occurred during hallway meetings, lunches, smoke-breaks, and after late evenings of game playing. Everyone in the company contributed their thoughts. With little formal process and no single design document, the game design evolved with each passing month.Ron, who had started his career in the game industry as an artist, was at that time our go-to guy for design on Blizzard games. Though he was finishing up the development of Blackthorne, a side-scrolling shooter for Super Nintendo, he devoted time to generating ideas for the game.Stu Rose was another artist who became one of Blizzard’s early staffers. From a personality standpoint he was the polar opposite of Ron in most respects, and his efforts as part of the design group occasioned conflicts of opinion with Ron, though during the times they did agree they were an unassailable force.These two ended up as the book-ends for the entire design process, each working independently to develop the world’s culture and plot overview, define the game’s units, specify the play mechanics, envision how magic spells worked, develop the game’s missions, choose place-names, and finalize other minutiae that are nevertheless important to make games comes to life.At this late date it’s not possible to document who developed exactly which idea without canvassing the entire team and sorting out arguments over events that happened so long ago. Even back then we had difficulties determining how game-design credit should be shared, and ultimately decided the fairest, most egalitarian solution was to credit everyone, and thus the Warcraft: Orcs vs. Humans box credits include “Game design by Blizzard Entertainment”. Incidentally the Moby Games credits for Warcraft 1 are completely borked because they mix the much later Macintosh and 1998 releases of the game with the original 1994 DOS release, so many folks are mis-credited.While my recollection of the exact timing of events is dim, I’ve recently seen an early design document dated 1994 and labeled “Chaos Studios”, which means it was generated in early 1994 before the company had been renamed Blizzard. By February 1994 we had a set of (still very rough) design documents that had been through several iterations and contained the key concepts for the game.Admittedly, it would probably have been better to have a design in place before I started programming in September 1993, but with the amount of “substrate” that I needed to build before the actual fun-n-game parts could be developed, the lack of a design wasn’t a show-stopper at that stage, particularly since we already pulled many of the game’s elements from Dune 2.What got choppedWhile it’s still (barely) possible to play Warcraft 1 today, it’s not much fun compared to later RTS games. The difficulty of getting the game running on modern computers leads one to high expectations that are then crushed when viewing a game with a screen resolution of only 320×200 pixels — one twentieth of the resolution of a modern high res monitor — and with user interface and play balance that are markedly inferior to our later efforts.But by playing Warcraft 1 it is possible to see the ideas that survived through the design winnowing process into the game’s final release. In many ways Warcraft 1 isn’t so much different from later games in the series.Today gamers are familiar with classic Warcraft units like Barracks, Town Halls, Lumber Mills and Gold Mines, all of which survived into future releases of Warcraft games. Those iconic units persist because their names and functions are easily comprehensible to those of us who live in the real world instead of Azeroth.But many of the ideas that our early design documents contained didn’t come to fruition. Some of this was related to the brutal timeline — the game had to launch for Christmas, 1994 and we barely made it. Ideas died because better alternatives existed, or didn’t have strong advocates, or were too time-consuming to implement, or would have required too much memory, or weren’t fun.I thought folks might like to know about ideas that ended up on the cutting room floor, like the Mason’s Hall [required for stone buildings], Dwarven Inn [greater production of stone], Elven Fletcher [upgrades for archers], Tax House and Ale House.These buildings all served secondary functions, some of which could be combined elsewhere. We instead added their functionality to existing buildings instead of creating buildings solely dedicated to one function, as for example the Dwarven Inn and Elven Fletcher buildings.The Mason’s Hall was dropped because we considered using stone as a third resource (in addition to gold and lumber) an unnecessary complexity. We revisited the idea again for Warcraft 2, and dropped it again after actually implementing (programming) the idea.The Ale Stand was designed to increase the rate at which soldiers and gold would be produced. I’m not sure how we can rectify that design idea with the amount of work that actually gets done after a night of heavy drinking in our world, but I imagine there are special rules of magic at work in Azeroth. Or maybe that’s why we cut the Ale Stand.And NPC races like lizard men, hobgoblins and Halflings were also on the drawing board but were ultimately rejected, almost certainly due to the effort of drawing and animating the figures in DPaint.Game development is about trade-offs — great games don’t have to do everything, they have to do a limited number of things well.FormationsA design idea much discussed but never implemented was “formations”, where units would stick together on the battlefield. Formations are difficult to implement so the feature was chopped from the spec.Some of the complexities that prevented implementation: units in formation all move at the same speed so slow units don’t get left behind — this created programming complexity. Formations need the ability to rotate — or “wheel” in military parlance — so that a formation heading north comprised of infantry carrying pikes with archers following behind can turn as a group to face an enemy detachment approaching from the east, with the archers still lined up behind the protective wall of infantry — this created user interface complexity. Given enough time we could have completed the feature, but we needed the development time for more basic features.As a stand-in, I did implement “numbered group selection”. A user would select a group of units and press the Ctrl (control) key plus a number key (1-4). Those unit-groupings would be remembered so it would be possible to later re-select those units by pressing the number key (1-4) by itself. But those units would move independently even though selected as a group.A player-character on the battlefieldAnother idea much discussed but never implemented was that of having a unit that represented the player on the game map: an avatar that would progress from mission to mission during the game.For a game-avatar to represent the player, it should morph from a weak unit into a mighty hero over the course of several missions to create a sense of progression. To do this properly would require that the character would only become more powerful if utilized. An underutilized avatar would remain weak, while an avatar constantly at work on the front lines would become stronger.Carrying a unit over from one mission to the next adds to the difficulty of play-balancing missions. A great player will graduate a strong avatar from each mission, and that avatar’s strength will make succeeding missions seem easy, while a less-skilled player’s avatar could be so weak as to make winning later missions impossible. These two problems would lead players to drop out of the game — in the first instance for lack of challenge, and in the second due to frustration, as few players want to go backwards and redo previous missions in order to survive a mission later in the game.A competitor’s product named War Wind released several years after Warcraft allowed units to be carried over from mission to mission; the game’s designers allowed up to four units to be transferred, but finessed the play-balance problem by ensuring that these units weren’t powerful enough to affect gameplay, somewhat the antithesis of what a heroic player-avatar is supposed to represent.Heroes — in Warcraft 1?!?We also considered including hero-units in Warcraft 1; they had names like the Illusion Thief, Barbarian, Huntress, and Juggernaut, each with specialized skills. Ultimately we trimmed the list of game units substantially; probably due to design and art-animation time constraints.As someone with limited involvement in Warcraft III, it was interesting to see the idea of heroes finally implemented in the series, though the design genesis of heroes in Warcraft III comes from a different source — that is, not from ideas re-hashed from Warcraft 1 design documents.Briefly, Warcraft III started out as a game called Heroes of Warcraft, which departed from the type of traditional RTS we had already launched five times before (W1, W2:ToD, W2:BtDP, SC, SC:BW) and was instead a squad-based tactical combat game set in the Warcraft universe. This game morphed into a more traditional RTS — but retained the element of heroes — after a change of team-leads halfway through the development.Warcraft’s bright color paletteIf you consider the artwork of the Warcraft series, you’ll see that the colors are shockingly loud in comparison to, say, Diablo, where only in a dim room is it truly possible to see the beauty of the art. The bright, cartoony art-style was different from the style of many other PC war games of the era, which hewed to more realistic color palette.Part of that difference can be explained by the past experiences of our artists, who had worked on several Super Nintendo and Sega Genesis console titles, where games required more dynamic colors since televisions of that era were so much worse at displaying colors than PC monitors. Console games on TVs, which had lower pixel resolution and color bleed, needed high-contrast artwork to show well.Another reason was at the behest of Allen, who charged all the artists with drawing artwork in bright conditions. He’d regularly stalk the halls of Blizzard turning on lights and opening window-blinds.His view was that most folks play games in bright rooms, so our artists should be authoring our games to play well in that environment. He argued that it’s easy to draw artwork that reads well when viewed in a dark room with no outside light can distract from the monitor. But when computer art is competing with bright lights it’s much more difficult to see. And fluorescent bulbs are the worst form of light available — the cold, flickering glow of their tubes tires the eyes and washes out colors.So the lights were always on in the art rooms to force artists to compensate for terrible lighting by creating art that accounted for those conditions. These working conditions chafed on some (all?) of the art team, but ultimately led to artwork that stood out compared to products of the day.Now you know why Warcraft artwork looks like it has been candy-coated!After six months of development that started in September 1993, Warcraft: Orcs vs. Humans, the first product in what would eventually become the Warcraft series, was finally turning into a game instead of an extended tech-demo.For several months I was the only full-time employee on the project, which limited the rate of development. I was fortunate to be assisted by other staff members, including Ron Millar, Stu Rose, and others, who did design work on the project. And several artists contributed prototype artwork when they found time in between milestones on other projects.The team was thinly staffed because the development of Warcraft was self-funded by the company from revenues received for developing titles for game publishers like Interplay and SunSoft, and the company coffers were not deep.At that time we were developing four 16-bit console titles: The Lost Vikings 2 (the sequel to our critically-acclaimed but low-selling, side-scrolling “run-and-jump” puzzle game), Blackthorne (a side-scrolling “run-and-jump” game where the lead character gets busy with a shotgun), Justice League Task Force (a Street Fighter clone set in the DC Comics universe), and Death and Return of Superman (a side-scrolling beat-em-up based on the DC universe comic series of the same name).With the money received for developing these games and other odd jobs the company was able to pay initial development costs.Game development economicsFor most of the history of the game industry, independent game development studios — which is to say studios that weren’t owned by a retail game publishing company — usually funded their projects by signing contracts with those publishing companies. Publishers would “advance” money for the development of the project. In addition to advances for development, publishers were responsible for publicity, marketing, manufacturing, retail distribution, customer support and so forth.Back in the early 90′s there were many more retail game-publishers than exist today, but the increasing cost of game development and especially of game publishing led to massive industry consolidation due to bankruptcies and acquisitions, so when you think of a retail game publisher today you’ll probably think of Activision-Blizzard, EA or Ubisoft instead of the myriad mid-sized companies that existed twenty years ago.As in all industries, the terms of contracts are drawn up to be heavily in the favor of the people with the money. This is the other golden rule: “he who has the money makes the rules”. While in theory these agreements are structured so that the game developer is rewarded when a game sells well, as in the record and movie industries publishers capture the vast majority of profits, with developers receiving enough money to survive to sign another agreement — if they’re lucky.When I mentioned “advances” paid by the publisher, the more correct term is “advances against royalties”, where the developer if effectively receiving a forgivable loan to be repaid from royalties for game sales. It sounds great: develop a game, get paid for each copy sold. But the mechanics work out such that the vast majority of game titles never earn enough money to recoup (pay for) the advances. Since development studios often had to give up the rights to their title and sequels, these agreements are often thinly disguised work-for-hire agreements.To aim for better deal terms, a common strategy employed by development studios was to self-fund an initial game prototype, then use the prototype to “pitch” a development deal to publishers. The longer a developer was able to self-fund game creation the better the eventual contract terms.Perhaps the best example of this strategy is Valve Software, where Gabe Newell used the wealth he earned at Microsoft to fund the development of Half Life and thereby gain a measure of control over the launch schedule for the game — releasing the game only when it was a high-quality product instead of rushing it out the door to meet quarterly revenue goals as Sierra Entertainment (the game’s publisher) desired. More importantly, Gabe’s financial wherewithal enabled Valve to obtain ownership of the online distribution rights for Half Life just as digital downloads were becoming a viable strategy for selling games, and led to that studio’s later — vast — successes.The downside to self-funding a prototype is the risk that the developer takes in the event that the game project is not signed by a publisher — oftentimes resulting in the death of the studio.The company I worked for — at that time named Silicon and Synapse — was self-funding Warcraft, along with another project called Games People Play, which would include crossword puzzles, boggle and similar games found on the shelves at airport bookstores to entertain stranded travelers.By developing two games that targeted radically different audiences the company owners hoped to create multiple sources of revenue that would be more economically stable compared to betting all the company’s prospects on the core entertainment market (that is, “hard core” gamers like you ‘n me).Of course spreading bets across diverse game genres also has risks, inasmuch as a company brand can be diluted by creating products that don’t meet the desires its audiences. One of the great strengths of the Blizzard brand today is that users will buy its games sight-unseen because they believe in the company’s vision and reputation. That reputation would have been more difficult to establish had the company released both lower-budget casual titles and high-budget AAA+ games, as did Sierra Entertainment, which is now out of business after repeated struggles to find an audience.In any event, creating Games People Play turned out to be a misstep because developing a casual entertainment product was so demoralizing for the lead programmer that the project never matured and was later canceled. Or perhaps it wasn’t a mistake, because the combination of Warcraft and Games People Play convinced Davidson &amp; Associates, at that time the second largest educational software company in the world, to purchase Silicon &amp; Synapse.Our new overlordsDavidson &amp; Associates, started by Jan Davidson and later joined by her husband Bob, was a diversified educational software company whose growth was predicated on the success of a title named Math Blaster, in which a player answers math problems to blow up incoming asteroids before they destroy the player’s ship. It was a clever conjunction of education and entertainment, and the company reaped massive rewards from its release.Aside: As an educational title Math Blaster may have had some value when used properly, but I had occasion to see it used in folly. My high school journalism class would write articles for our school newspaper in a computer room shared with the remedial education class; my fellow journalism students and I watched in horror as remedial twelfth graders played Math Blaster using calculators. As asteroids containing expressions like “3 + 5″ and “2 * 3″ approached those students would rapidly punch the equations into calculators then enter the results to destroy those asteroids. Arguably they were learning something, considering they outsmarted their teachers, but I’m not sure it was the best use of their time given their rapidly approaching entry into the work-force.With good stewardship and aggressive leadership Davidson &amp; Associates expanded into game manufacturing (creating &amp; packing the retail box), game distribution (shipping boxes to retailers and intermediate distributors), and direct-to-school learning-materials distribution. They saw an opportunity to expand into the entertainment business, but their early efforts at creating entertainment titles internally convinced them that it would make better sense to purchase an experienced game development studio rather than continuing to develop their own games with a staff more knowledgeable about early learning than swords &amp; sorcery.And so at a stroke the cash-flow problems that prevented the growth of the Warcraft development team were solved by the company’s acquisition; with the deep pockets of Davidson backing the effort it was now possible for Silicon &amp; Synapse (renamed Blizzard in the aftermath of the sale) to focus on its own titles instead of pursuing marginally-profitable deals with other game publishers. And they were very marginal — even creating two top-rated games in 1993, which led to the company being named “Nintendo Developer of the Year”, the company didn’t receive any royalties.With a stack of cash from the acquisition to hire new employees and enable existing staff to jump on board the project, the development of Warcraft accelerated dramatically.The design “process”The approach to designing and building games at Blizzard during its early years could best be described as “organic”. It was a chaotic process that occurred during formal design meetings but more frequently during impromptu hallway gatherings or over meals.Some features came from design documents, whereas others were added by individual programmers at whim. Some game art was planned, scheduled and executed methodically, whereas other work was created late at night because an artist had a great idea or simply wanted to try something different. Other elements were similarly ad-libbed; the story and lore for Warcraft came together only in the last several months prior to launch.While the process was unpredictable, the results were spectacular. Because the team was comprised of computer game fanatics, our games evolved over the course of their development to become something that gamers would want to play and play and play. And Warcraft, our first original game for the IBM PC, exemplified the best (and sometimes the worst) of that process, ultimately resulting in a game that — at least for its day — was exemplary.How the Warcraft unit-creation system came aboutAs biologists know the process of evolution has false starts where entire branches of the evolutionary tree are wiped out, and so it was with our development efforts. Because we didn’t have spec to measure against, we instead experimented and culled the things that didn’t work. I’d like to say that this was a measured, conscious process in each case, but many times it arose from accidents, arguments, and personality conflicts.One event I remember in particular was related to the creation of game units. During the early phase of development, units were conjured into existence using “cheat” commands typed into the console because there was no other user-interface mechanism to build them. As we considered how best to create units, various ideas were proposed.Ron Millar, an artist who did much of the ideation and design for early Blizzard games proposed that players would build farm-houses, and — as in the game Populous — those farms would periodically spawn basic worker units, known as (Human) peasants and (Orc) peons. The player would be able to use those units directly for gold-mining, lumber-harvesting and building-construction, but they wouldn’t be much good as fighters.Those “peons” not otherwise occupied could be directed by the player to attend military training in barracks, where they’d disappear from the map for a while and eventually emerge as skilled combatants. Other training areas would be used for the creation of more advanced military units like catapult teams and wizards.The idea was not fully “fleshed out”, which was one of the common flaws of our design process: the end result of the design process lacked the formality to document how an idea should be implemented. So the idea was kicked around and argued back and forth through the informal design team (that is, most of the company) before we started coding (programming) the implementation.Before we started working on the code Ron left to attend a trade show (probably Winter CES — the Consumer Electronics Show), along with Allen Adham, the company’s president. And during their absence an event occurred which set the direction for the entire Warcraft series, an event that I call the “Warcraft design coup”.Stu Rose, another early artist/designer to join the company (employee #6, I believe), came late one afternoon to my office to make a case for a different approach. Stu felt that the unit creation mechanism Ron proposed had too many as-yet-unsolved implementation complexities, and moreover that it was antithetical to the type of control we should be giving players in a Real-Time Strategy (RTS) game.In this new RTS genre the demands on players were much greater than in other genres and players’ attention could not be focused in one place for long because of the many competing demands: plan the build/upgrade tree, drive economic activity, create units, place buildings, scout the map, oversee combat and micromanage individual unit skills. In an RTS the most limited resource is player attention so adding to the cognitive burden with an indirect unit creation mechanism would add to the attention deficit and increase the game’s difficulty.To build “grunts”, the basic fighting unit, it would be necessary to corral idle peasants or those working on lower priority tasks to give them training, unnecessarily (in Stu’s view) adding to the game’s difficulty.I was a ready audience for his proposal as I had similar (though less well thought out) concerns and didn’t feel that unit creation was an area where we needed to make bold changes. Dune II, the game from which the design of Warcraft was derived, had a far simpler mechanism for unit creation: just click a button on the user-interface panel of a factory building and the unit would pop out a short time later. It wasn’t novel — the idea was copied from even earlier games — but it just worked.Stu argued that we should take this approach, and in lieu of more debate just get it done now, so over the next couple of days and late nights I banged out the game and user-interface code necessary to implement unit creation, and the design decision became fait accompli. By the time Ron and Allen returned the game was marginally playable in single-player mode, excepting that the enemy-computer AI was still months away from being developed.Warcraft was now an actual game that was simple to play and — more importantly — fun. We never looked back.The first multiplayer game of WarcraftIn June 1994, after ten months of development, the game engine was nearly ready for multiplayer. It was with a growing sense of excitement that I integrated the code changes that would make it possible to play the first-ever multiplayer game of Warcraft. While I had been busy building the core game logic (event loop, unit-dispatcher, path-finding, tactical unit-AI, status bar, in-game user-interface, high-level network code) to play, other programmers had been working on related components required to create a multiplayer game.Jesse McReynolds, a graduate of Caltech, had finished coding a low-level network library to send IPX packets over a local-area network. The code was written based on knowledge gleaned from the source code of Quake Doom, which had been recently was later open-sourced by John Carmack at id software. While the IPX interface layer was only several hundred lines of C code, it was the portion of the code that interfaced with the network-card driver to ensure that messages created on one game client would be sent to the other player.And Bob Fitch, who was earning his master’s degree from Cal State Fullerton, developed the initial “glue screens” that enabled players to create and join multiplayer games. My office was next to Bob’s, which was mighty convenient since it was necessary for us to collaborate closely to integrate his game join-or-create logic to my game-event loop.After incorporating the changes I compiled the game client and copied it to a network drive while Bob raced back to his office to join the game. In what was a minor miracle, the code we’d written actually worked and we were able to start playing the very first multiplayer game of Warcraft.As we started the game I felt a greater sense of excitement than I’d ever known playing any other game. Part of the thrill was in knowing that I had helped to write the code, but even more so were two factors that created a sense of terror: playing against a human opponent instead of a mere computer AI, and more especially, not knowing what he was up to because of the fog of war.The fog of warOne of the ideas drawn from earlier games was that of hiding enemy units from sight of the opposing player. A black graphic overlay hid areas of the game map unless a friendly unit explored the area, which is designed to mimic the imperfect information known by a general about enemy operations and troop movements during real battles.Empire, a multiplayer turn-based strategy game written almost seventeen years before by the brilliant Walter Bright (creator the “D” programming language), used fog of war for that same purpose. Once an area of the map was “discovered” (uncovered) it would remain visible forever afterwards, so an important consideration when playing was to explore enough of the map early in the game so as to receive advance warning of enemy troop movements before their incursions could cause damage to critical infrastructure or economic capability.The psychological terror created by not knowing what the enemy is doing has been the demise of many generals throughout history, and adding this element to the RTS genre is a great way to add to the excitement (and fear) level. Thank Walter and the folks at Westwood who created Dune II for their savvy!Computer AIAs many gamers know, computer-controlled “Artificial Intelligence” (AI) players in strategy games are often weak. It’s common for human players to discover exploits that the computer AI is not programmed to defend against that can be used destroy the AI with little difficulty, so computer AI players usually rely upon a numeric troop advantage, positional advantage, or “asymmetric rules” in order to give players a good challenge.In most Warcraft missions the enemy computer players are given entire cities and armies to start with when battling human players. Moreover, Warcraft contains several asymmetric rules which make it easier for the AI player to compete, though these rules would perhaps be called outright cheating by most players.One rule we created to help the computer AI was to reduce the amount of gold removed from gold mines to prevent them from being mined-out. When a human player’s workers emerge from a gold mine those workers remove 100 units of ore from the mine and deliver it back to the player’s town hall on each trip, and eventually the gold mine is exhausted by these mining efforts. However, when an AI-controlled worker makes the same trip, the worker only remove 8 units of ore from the mine, while still delivering 100 units into the AI treasury.This asymmetric rule actually makes the game more fun in two respects: it prevents humans from “turtling”, which is to say building an unassailable defense and using their superior strategic skills to overcome the computer AI. Turtling is a doomed strategy against computer AIs because the human player’s gold-mines will run dry long before those of the computer.Secondarily, when the human player eventually does destroy the computer encampment there will still be gold left for the player to harvest, which makes the game run faster and is more fun than grinding out a victory with limited resources.Most players are aware of a more serious violation of the spirit of fair competition: the computer AI cheats because it can see through the fog of war; the AI knows exactly what the player is doing from moment to moment. In practice this wasn’t a huge advantage for the computer and merely served to prevent it from appearing completely stupid.Interestingly, with the long popularity of StarCraft (over 14 years since launch and still played), a group of AI programmers has risen to the challenge of building non-cheating AIs. Aided by a library called BWAPI, these programmers write code that can inject commands directly into the StarCraft engine to play the game. Programmers enter their AIs in competitions with each other to determine the victor. While these BWAPI AI players are good, the best of them are handily beaten by skilled human opponents.Playing against a humanAs a person who had played many (many many) strategy games before developing Warcraft, I was well aware of the limitations of computer AIs of that era. While I had battled against many computer AIs, sometimes losing, many times winning, I was never scared by AI intelligence, even when battling the terrible Russian offensive in the game Eastern Front by Chris Crawford, which I played on a friend’s Atari 800 until eventually the audio cassette tape (!) that contained the game was so old it could no longer be read.These games were fun, exciting, and most certainly challenging, but not scary. But something changed when I played the first multiplayer game of Warcraft.The knowledge that I was competing against an able human player — not just in terms of skill and strategy, but also in terms of speed of command — but was prevented from seeing his actions by the fog of war was both electrifying and terrifying. In my entire career I have never felt as excited about a single game as I was during that first experience playing Warcraft, where it was impossible to know whether I was winning or losing.As a massive adrenaline rush spiked in my bloodstream, I did my best to efficiently harvest gold and lumber, build farms and barracks, develop an offensive capability, explore the map, and — most importantly — crush Bob’s armies before he could do the same to mine.This was no test-game to verify the functionality of the engine; I know he felt the same desire to claim bragging rights over who won the first-ever multiplayer game of Warcraft. Moreover, when we had played Doom together at Blizzard, I had won some renown because, after a particularly fierce game Bob had become so angry at me for killing him so frequently with a rocket launcher that he had vowed never to play me again. I knew he’d be looking for payback.As our armies met in battle, we redoubled our efforts to build more units and threw them into the fray. Once I discovered his encampment and attacked, I felt more hopeful. Bob’s strategy seemed disorganized and it appeared I would be able to crush his forces, but I wanted to leave nothing to chance so I continued at a frenzied pace, attacking his units and buildings wherever I could find them.And then … crash:Bad news bears – DOS4GW lets us know Warcraft crashedAs any programmer knows, the likelihood of new code working properly the first time is close to zero, and so it should be no surprise that the game crashed. The game’s graphics scrolled off the top of the monitor and were replaced with the blocky text of the DOS4GW “crash screen” so familiar to gamers in the era before Windows gaming. Now we have the far more sophisticated Windows Error Reporting dialog which enables the player to submit the crash report, though occasionally players see the dreaded “blue screen of death” which is remarkably similar to those of old.After the crash I leaped up from my chair and ran into Bob’s office, yelling “That was awesooooommmme!” immediately followed by “… and I was kicking your ass!” So I was surprised to hear Bob’s immediate rebuttal: to the contrary, he had been destroying me.It took a few minutes for our jangled nerves to return to normal but in short order we determined that not only did we have a crash bug but also a game-state synchronization problem, which I termed a “sync bug”: the two computers were showing entirely different battles that, while they started identically, diverged into two entirely different universes.Someone who hasn’t worked on programming network code might assume that the two Warcraft game clients would send the entire game state back and forth each turn as the game is played. That is, each turn the computers would send the positions and actions for every game unit. In a slow-paced game with only a few board positions, like Chess or Checkers, this would certainly be possible but in a game like Warcraft, with up to 600 units in action at once, it was impossible to send that volume of information over the network.We anticipated that many gamers would play Warcraft with 2400 baud modems, which could only transmit a few hundred characters per second. Younger gamers who never used a modem should take the time to read up on the technology, which was little removed from smoke signals, and only slightly more advanced than banging rocks together. Remember, this was before Amazon, Google and Netflix — we’re talking the dark ages, man.Having previously “ported” Battle Chess from DOS to Windows, I was familiar with how multiplayer games could communicate using modems. I knew that because of the limited bandwidth available via a modem it would have been impossible to send the entire game state over the network, so my solution was to send only each player’s commands and have both players execute those commands simultaneously.I knew that this solution would work because computers are great at doing exactly what they’re told. Unfortunately it turned out that many times we humans who program them are not so good at telling computers exactly the right thing to do, and that is a major source of bugs. When two computers are supposed to be doing the same thing, but disagree because of a bug, well, that’s a problem.A sync bug arises when the two computers simulating the game each choose different answers to the same question, and from there diverge further and further over time. As in time-travel movies like Back to the Future, small changes made by the time-traveler while in the past lead to entirely different futures; so it was that games of Warcraft would similarly diverge. On my computer my Elvish archer would see your Orcish peon and attack, whereas on your computer the peon would fail to notice the attack and wander off to harvest lumber. With no mechanism to discover or rectify these types of disagreements, our two games would soon be entirely different.So it was that the first game of Warcraft was a draw, but at the same time it was a giant win for the game team — it was hella fun! Other team members in the office played multiplayer soon afterwards and discovered it was like Blue Sky, the pure crystal meth manufactured by Walter White in Breaking Bad. Once people got a taste for multiplayer Warcraft, nothing else was as good. Even with regular game crashes, we knew we were on to something big.All we needed to do was get the game done.Tragically, we soon made an even worse discovery: not only did we have numerous sync bugs, but there were also many different causes for those sync bugs. If all the sync bugs were for similar reasons we could have endeavored to fix the singular root cause. Instead it turned out there were numerous different types of problems, each of which caused a different type of sync bug, and each which therefore necessitated its own fix.Why do sync bugs happenWhen developing Warcraft I had designed a solution to minimize the amount of data that needed to be transmitted over the network by only sending the commands that each player initiated, like “select unit 5″, “move to 650, 1224″, and “attack unit 53″. Many programmers have independently designed this same system; it’s an obvious solution to the problem of trying to synchronize two computers without sending the entire game state between them every single game turn.Aside: These days there are probably several patents retroactively trying to claim credit for this approach. Over time I’ve come to believe that software should not be patentable; most any idea in software is something that a moderately experienced programmer could invent, and the definition of patents requires that patents be non-obvious. Nuff said.I hadn’t yet implemented a mechanism to verify synchronization between the two computers, so any bug in the game code that caused those computers to make different choices would cause the game to “bifurcate” – that is, split it into two loosely-coupled universes that would continue to communicate but diverge with increasing rapidity over time.Creating systems designed to detect de-synchronization issues was clearly the next task on my long list of things to do to ship the game!In for the long haulYou know the ending to this story: Warcraft eventually shipped only five months later. It seemed an eternity because we worked so many hours each day, encountered so many obstacles, overcame so many challenges, and created something we cared for so passionately. I’ll continue to explore those remaining months in future blog articles, but so much was packed into that time that it’s impossible to squeeze those recollections into this already too long post!" }, { "title": "Fetching Changesets in Redmine from Heroku using Subversion", "url": "/blog/2012/11/21/fetching-changesets-in-redmine-from-heroku-using-svn/", "categories": "Redmine", "tags": "redmine, heroku, subversion, vagrant", "date": "2012-11-21 08:44:00 -0500", "snippet": "[NOTE: The method described below should still work, but it’s much easier to just use a Heroku buildpack (see this post for details)]I manage my open source and contract development projects using ...", "content": "[NOTE: The method described below should still work, but it’s much easier to just use a Heroku buildpack (see this post for details)]I manage my open source and contract development projects using Redmine.Since I’m “frugal”, I tend to try to push the free hosting envelope as far as possible. As a result, I have my Redmine deployment on Heroku, my files and attachments on Dropbox and my source on GitHub.I also like to link to changesets in my projects, which is easy enough to do when you host the source and the Redmine server on the same machine.Not so easy with Heroku+GitHub …. until now!By the end of this tutorial, we will have: Setup a build system using Vagrant that matches the Heroku hosting environment Compiled a statically linked Subversion client Added the svn client to our Redmine repository and pushed it to Heroku Configured a project in Redmine to fetch changesets from GitHub using SubversionOverviewIn 2012, GitHub announced SVN support, which primarily opened the service up to developers who hadn’t given up the centralized development model.As an added bonus, it gave us an alternative view into the commit logs for our projects without the need for a local copy.In order to proceed, I’m making the following assumptions: You are hosting an instance of Redmine on Heroku You are comfortable using Git You’ve used a Debian-based Linux distribution in the past You’ve used the GNU Build System before You have Ruby and RubyGems configured (if not, RVM is a good place to start) You are not crazy enough to try doing this from Windows or OSX ;) You have a sense of humour and realize the winky above indicates this guide was written for Linux, but could easily be adapted for any OSConfiguring the Build EnvironmentHeroku (as of November, 2012 at least) deploys applications to an Ubuntu 10.04 x86_64 environment, and we’re going to be statically linking for that environment, so we need to setup a build system that reflects this requirement.The most efficient way of doing this is to use Vagrant to initialize a bare-bones Ubuntu Lucid system.Since Rubygems should already be installed, it can be used to quickly setup Vagrant:gem install vagrantNow, let’s fetch a pre-build lucid system (thanks Vagrant!) and initialize it in the current directory:vagrant box add lucid64 http://files.vagrantup.com/lucid64.boxvagrant initThis will create a Vagrantfile in the current director, which contains configuration information for our build system. In order to tell Vagrant to use the lucid64 instace we’ve downloaded, the Vagrantfile needs to be edited and the config.vm.box section updated.# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant::Config.run do |config| # ... # Every Vagrant virtual environment requires a box to build off of. config.vm.box = \"lucid64\" # ...Now that everything is ready to go, let’s start the instance.vagrant upOnce vagrant has started the virtual machine, we can access it via ssh using:vagrant sshBuilding SubversionFor the purposes of this tutorial, I’m going to be using the 1.6 branch of Subversion, which still had the subversion-deps packaged separately. NOTE With the 1.7 branch, there is a script that automates the process, and I’ll likely update this tutorial at some point to use this.First, we need to get the source:wget http://subversion.tigris.org/downloads/subversion-deps-1.6.19.tar.bz2wget http://subversion.tigris.org/downloads/subversion-1.6.19.tar.bz2tar xvf subversion-deps-1.6.19.tar.bz2tar xvf subversion-1.6.19.tar.bz2cd subversion-1.6.19Second, we’ll install any additional components required to compile successfully:sudo apt-get install build-essential libxml2-devFinally, we’ll configure the build to exclude as much as possible and to produce a static binary:./configure --with-ssl \\ --without-gssapi \\ --without-swig \\ --without-neon \\ --enable-all-staticmakeOnce the build completes, the only file we’re interested in is the svn client, so we’ll copy that to the /vagrant directory of our build machine, but first we’ll strip it.strip subversion/svn/svncp subversion/svn/svn /vagrant NOTE stripping the binary reduces the overall size from ~ 12 MB to ~ 4 MB, which is important since we have limited space on Heroku.\tvagrant@lucid64:~/subversion-1.6.19$ ls -l subversion/svn/svn\t-rwxr-xr-x 1 vagrant vagrant 12060462 2012-11-21 14:37 subversion/svn/svn* vagrant@lucid64:~/subversion-1.6.19$ strip subversion/svn/svnvagrant@lucid64:~/subversion-1.6.19$ ls -l subversion/svn/svn-rwxr-xr-x 1 vagrant vagrant 4257472 2012-11-21 14:56 subversion/svn/svn* The /vagrant directory is shared between the host and the guest machine by default, so this gives us access to the Subversion client if we open a new console and access the project folder we started Vagrant in.Since we’re now done, we can end the ssh session and stop the virtual machineexitvagrant haltAdding Subversion to RedmineGo to the root of your local Redmine repository, and create a bin folder. Now copy the svn binary to this folder, add the result to your repository and push to Heroku.mkdir bincp /path/to/static/svn bingit commit -a -m \"Adding a Statically Linked Subversion to Redmine\"git push heroku masterIf all went well, when you access the Repositories tab under Administration -&gt; Settings, Subversion should be listed and the version we just uploaded displayed.Accessing Changesets from GitHubThe final step is to configure an existing project to access a project on GitHub.First, add a new repository and point it to the GitHub url you would normally use to access the project on the web. NOTE Leave the Login and Password fields blank unless this is a private repositoryOnce configured, clicking on the Repository tab should fetch the changesets and show the source tree NOTE This can potentially time out on a larger project and require clicking on the Repository tab multiple times until all changesets have been fetched and parsed. NOTE Viewing changeset diffs doesn’t workI’ve used my personal installation as an example at http://alexbevi-pm.herokuapp.com/projects/redmine-dropbox-attachments.This method isn’t on-par with a self-hosted solution, but it’s good enough if you don’t have a VPS in your budget ;)REFERENCES http://rickvanderzwet.blogspot.ca/2007/11/building-static-subversion.html http://bindle.me/blog/index.php/405/running-binaries-on-heroku" }, { "title": "Developer Braindump: Tony Tough and the Night of the Roasted Moths", "url": "/blog/2012/11/02/developer-braindump-tony-tough-and-the-night-of-the-roasted-moths/", "categories": "Game Development", "tags": "scummvm, adventure", "date": "2012-11-02 08:28:00 -0400", "snippet": "The ScummVM project recently added support for Tony Tough and the Night of the Roasted Moths.The original developer, Giovanni Bajo appeared on the scummvm-devel mailing list and offered some insigh...", "content": "The ScummVM project recently added support for Tony Tough and the Night of the Roasted Moths.The original developer, Giovanni Bajo appeared on the scummvm-devel mailing list and offered some insight into the development of the original engine.The following is extracted from the original post.I went though the pull review code and of course lots of memories sprang to mind, so I'm happy to post a few things.The game originated in 1996, I was 17 back at the time. Luca Giusti and I wrote the first version of the game engine, and it was based on DOS and 8-bit (palette) graphics, with the venerable Mode X for achieving smooth scrolling and higher resolution (320x240), and compiled sprites (!!!). IIRC Luca wrote the game engine and I wrote the MPAL preprocessor (yuk!), compiler, linker and interpreter. We were a distributed team of 5-6 people working across Italy, and we were using Fidonet and BBS files areas to share files. I live in Florence and one of the artist (Sergio) was in Naples; I still remember that we used a direct modem connection on the phone line (so basically he dialed my home phone number with his modem, to setup a 14.4 kbit connection) and then send me the first 4 room background arts through it; they were an early 8-bit version, but I was in love with them :) I also remember our lead artist (Valerio) sent me the first version of the park map (hand drawn wireframe) by fax; the original thermal paper is now a picture hanging on my home wall :)At some point, in 1997, it became clear that we need to gather together to get to a decent point, so we organized a one-week crunch session in a alpine hut in northern italy; we worked 18-20hrs a day for a week, ate lots of pasta, and completed an early version of the first 4 rooms. I remember the game engine was using dirty-rectangles at that time to optimize drawing.In 1998 we got a deal with a local publisher (Prograph/Protonic), and part of the team got paid to finish the game. That included myself, but Luca declined (for personal reasons); that meant that I was able to work on the game full-time, while Luca only in his spare time. The publisher insisted on reworking the game with 16-bit graphics, and porting it from DOS to Windows; it wasn't trivial to port the existing code (Mode X stuff!), plus Luca couldn't handle it in his spare time, so I had to take over the game engine. I wasn't familiar with it at all (Luca had written it up until then), and thus decided to rewrite it from scratch. One of the few things that I brought over was the pathfinding code, and in fact I think it's quite obvious from its look that it's been written by a different hand.The game engine is quite easy, as you have seen. I think it's been the first real world C++ object-oriented program I have ever written, so it's obviously very stretched towards using inheritance everywhere (new toy!). I still remember the joy of finally *understanding* polymorphism, and that's when the RMTask base class was born, and the idea of having a list of RMTasks that could be anything by simply reimplementing the draw() call. I saw in the code that the list was called \"OTList\" and, while the name obviously rings a bell to me, I don't recall what it means. BTW, \"RM\" stands for Roasted Moths of course; some classes in the source code begins with \"FP\" instead; that's \"Falling Pumpkins\" which was the original title of the game.When I reimplemented the game, I went for full screen drawing, without dirty rectangles. I think i was positive that computers were fast enough for it, and in fact it did work quite well. I remember I quite optimized sprite drawing routines; for instance, most sprites are RLE-encoded as a way to quickly skip transparent pixels, and to avoid compiled sprites because there was simply too many sprites to compile them all without having memory issues. The game was running 60 fps on a Pentium 2, so that was good enough for us. The '30s (sepia) mode was a nice touch, but I remember it was noticeably slower at the time as it was an additional pass over the screen, so the framerate was dropping quite noticeably.To build the rooms, setup background animations, decide rectangles of interactions with objects, etc. we built a so-called \"location editor\" (we actually used the word \"location\" for what is usually called \"room\" in adventures), whose source code I think has been lost. It was able to load a .loc file, modify it, and save it back. So both the game and the editor was using the same file format. At some point, we realized that it was better to separate the two formats, because the editor needed more information in basic format, while the game engine might use, for instance, sprites that were already rle-compressed instead of rle-compressing on the fly at load time; so the \"lox\" format was born. I think the game engine still opens the original loc files, but the final game assets only contains lox files.One guy in the team (Marco) was one of those magic resources in game teams that is both an artist and a programmer; I think it's mostly impossible nowadays, and used to be just rare at the time. So he was in charge of both maintaining the editor *and* drawing the background animations *and* using the editor to insert them into the locations. So that's one hell of \"eating multiple levels of dogfood\" :) In fact, he was the one that prodded me to add support for \"slot positions\" to the game engine; with \"slot positions\", we meant having an optional x,y offset associated to a graphic frame within a animation; I remember we discussed whether this was worth it, but at the end of the day, he was in a better position to know, since he was using the editor after all. Later in the game, it proved *extremely* useful in one specific animation: to animate the beast walking out of its cage. In fact, the game engine has no concept of actors (the only \"actor\" is Tony), so every other actor in the game is just an animated background object; in fact, they just don't walk around. So, when the designer required the beast to walk out, Marco faked it through manually settings x,y offsets in the editor within the whole animation, making it scroll away. It took him a while of course, and won the title of \"slot position master\" for this achievement :)Stuffing all files in a single compressed \"resource\" file was very common at the time. I don't think there was a rationale for doing it, it was just the standard way of doing it. Lucas was doing it, so why shouldn't we? :) I think it just looked cool, in a way. We were using LZO as a compressor because it was very fast at decompression; you might have noticed also that LZO is GPL, which means that the game was in blatant violation. I don't think I have *knew* what free software was and what a free software license was. I don't recall the details, but I wouldn't be surprised if I simply altavista'd for \"fast compression\", found that source code, downloaded it at 64 kbit/s, and dumped it into the game. I'm now deep into free software at many levels, I even give talks and do trainings about free software licenses from time to time, so I hope Markus of LZO can forgive the young me for not even realizing what I was doing.Speeches gave me quite a few headaches. For a start, I realized that simply exporting all texts from MPAL to a txt/html file wasn't enough, because actors need context to meaningfully act. So I ended up writing a program that exported HTML file divided by rooms, with some contexts added, and even manually adding comments to explain them. This has been helpful and was important to get good speeches from them. I remember it was really fun when the actors director called us and played us over the phone the first recordings of character voice tests made by the actors. I think we actually voted between a few options to decide which voice sounded best for Tony. When I got the first speeches files back from them, I had some latency issues, especially when running off CDs. I think I ended up caching the main index in RAM and maybe precaching something (don't remember the details). I think I also changed the code so to delay display of subtitles to the moment I could effectively start playing the speech, because it was very unfriendly to first see the text on screen, and the hear the speech 1 second later.MPAL was very very basic. I actually didn't have a clue how to properly code a VM with bytecode and stuff, and I didn't have *any* proper compiler theory training, so in retrospect it's amazing I even managed to parse and compile files. So basically MPAL can only assign to variables (which are all globals, plus a weird namespace thingie for locality), call \"custom functions\" (aka C++ code stored in a pointer table) and do basic arithmetic. That's it. Not even a proper language, isn't it? :) No loop constructors, no ifs, no jumps, nothing. So MPAL was a collection of flat routines, each one associated to a specific verb (action), specific object (eg. door), under a specific condition (eg: when open, that is when global variable room1_door_open==1). Since you couldn't have conditions *within* a MPAL routine but only to select *which* routine to run, you can see that cut &amp; paste was a useful tool for scripters :)In fact, we didn't evolve MPAL much during the main game development. We basically went with what we had on the DOS version, and we were using the same compiler/linker binary for months/years like we had no source code for it. In fact, having written most of it in 1996 (2 yrs before), with zero experience on writing compilers, it was kind of \"magical\" in my eyes that it was somehow working, and I was almost scared to touch it, let alone refactoring. I remember that, near the end of development, the scripts hit some internal hard-coded limit of the compiler (say, too many total variables), and I was terrified to simply open the project in Visual Studio, bump that limit, and recompile everything. In retrospect, I think it was more of a psychological thing, since the compiler worked wonderfully for the whole development and did its job.There were no actors in MPAL simply because the game didn't really require them, so at some point we decided to simply avoid implementing it and tweak the storyboard accordingly. The game designer, Stefano, wasn't very happy about it but we eventually found a deal and I simply added a way to run background tasks in MPAL (actually, through a custom verb \"Idle\", so not to modify the compiler!), to let actors at least chat and animate in background, but without ever walking. In fact, the game engine had already been designed for multiple actors in mind (this is why you have the RMCharacter class separated from RMTony), but there was nothing in MPAL, and of course development time is limited.Given my ignorance of \"advanced\" programming techniques, I think I simply couldn't think of how one could execute a linear (blocking) piece of script code without recurring to multi-threading. So I think I didn't even explore alternatives to multithreading. Obviously multithreading programming isn't easy, so custom C++ functions, called from MPAL, were full of locking issues wrt to the main thread that redraws the screen. The game was developed under Windows 95 and in fact was released before Windows 2000 was out; on the NT kernel, the game didn't work too well and crashed often because of race conditions. I see that you debugged and fixed at least few those bugs, well done! Those simply weren't triggering on a 9x kernel.Another issue with MPAL is that there's no way to skip cutscenes; I don't remember whether I couldn't think of a good way of doing it, or whether all methods I devised would require too much workload on scripters that were already quite full. I think it's my #1 technical regret when I see the game, and in fact I would quite love if you found a way to implement it now.I think this ends my braindump of today. Feel free to ask questions of course.Thanks again for your work!" }, { "title": "Disable Hot Corner Hover in Linux Mint", "url": "/blog/2012/11/02/disable-hot-corner-hover-in-linux-mint-13/", "categories": "Linux, Configuration", "tags": "linux, mint, cinnamon", "date": "2012-11-02 08:01:00 -0400", "snippet": "UPDATE For Linux Mint 16, the hotCorner settings have been moved to /usr/share/cinnamon/js/ui/hotCorner.js.I use Synergy on all my computers to share a common mouse and keyboard, but I’ve found tha...", "content": "UPDATE For Linux Mint 16, the hotCorner settings have been moved to /usr/share/cinnamon/js/ui/hotCorner.js.I use Synergy on all my computers to share a common mouse and keyboard, but I’ve found that with Gnome 3 based distributions, the hot corner was causing me some grief.The beauty of using a Linux-based system though is that you can pretty much change anything you’re unhappy with, so that’s what I’m here to do.By default, Linux Mint uses the Cinnamon desktop, so the configuration file we’re looking for is at /usr/share/cinammon/js/ui/layout.js.For Linux Mint 12, which still used Gnome Shell, the file was at /usr/share/gnome-shell/js/ui/layout.js.Edit the appropriate file$ sudo nano /usr/share/cinammon/js/ui/layout.jsLocate the following section (I just searched for hot-corner)this._corner = new Clutter.Rectangle({ name: 'hot-corner', width: 1, height: 1, opacity: 0, reactive: true });And change the value of reactive from true to false:this._corner = new Clutter.Rectangle({ name: 'hot-corner', width: 1, height: 1, opacity: 0, reactive: false });Log off, then back on. Booya!" }, { "title": "Fixing Broken Sudo", "url": "/blog/2012/10/30/fixing-broken-sudo/", "categories": "Linux", "tags": "linux, ubuntu", "date": "2012-10-30 14:58:00 -0400", "snippet": "This is pretty much a transcription of http://www.psychocats.net/ubuntu/fixsudo, which is just such a good article I wanted to keep a copy for reference.How does sudo work?The way that Ubuntu has i...", "content": "This is pretty much a transcription of http://www.psychocats.net/ubuntu/fixsudo, which is just such a good article I wanted to keep a copy for reference.How does sudo work?The way that Ubuntu has implemented sudo, the /etc/sudoers file says that users in the admin group can (after a password authentication) temporarily escalate to system-wide privileges for particular tasks. And then the /etc/groups file says which users are in the admin group.You can read more on the community documentation about Ubuntu’s implementation of sudo.Cause and Symptomssudo breaks when one or more of the following occurs: the /etc/sudoers file has been altered to no longer allow users in the admin group to escalate privilege the permissions on the /etc/sudoers file are changed to something other than 0440 a user who should not have been has been taken out of the admin groupIf sudo is broken this way, you may notice an error saying you’re not in the _sudo_ers file and the incident is going to be reported. Or you may just see the next command prompt without any action being executed.Booting into recovery modeSince fixing sudo involves editing system files, and you would otherwise need sudo to do so, you’ll have to boot into recovery mode to gain root (system-wide) access in order to repair sudo.If you have a single-boot (Ubuntu is the only operating system on your computer), to get the boot menu to show, you have to hold down the Shift key during bootup.If you have a dual-boot (Ubuntu is installed next to Windows, another Linux operating system, or Mac OS X; and you choose at boot time which operating system to boot into), the boot menu should appear without the need to hold down the Shift key.From the boot menu, select recovery mode, which is usually the second boot option.After you select recovery mode and wait for all the boot-up processes to finish, you’ll be presented with a few options. In this case, you want the Drop to root shell prompt option so press the Down arrow to get to that option, and then press Enter to select it.The root account is the ultimate administrator and can do anything to the Ubuntu installation (including erase it), so please be careful with what commands you enter in the root terminal.In recent versions of Ubuntu, the filesystem is mounted as read-only, so you need to enter the follow command to get it to remount as read-write, which will allow you to make changes:mount -o rw,remount /Do the actual repairCase 1If you’d removed your last admin user from the admin group, then typeadduser username adminwhere username is your actual username.Case 2If you had previously edited the /etc/_sudo_ers file and screwed it up, then typesudo cp /etc/sudoers /etc/sudoers.backupsudo nano /etc/sudoers(the proper command is actually sudo visudo, which checks syntax before you save the /etc/sudoers file, but in some older versions of Ubuntu, that command uses the vi editor, which can be confusing to new users, as opposed to nano, which is more straightforward)and make it sure it looks like this:## This file MUST be edited with the 'visudo' command as root.## Please consider adding local content in /etc/sudoers.d/ instead of# directly modifying this file.## See the man page for details on how to write a sudoers file.#Defaults env_resetDefaults secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"# Host alias specification# User alias specification# Cmnd alias specification# User privilege specificationroot ALL=(ALL:ALL) ALL# Members of the admin group may gain root privileges%admin ALL=(ALL) ALL# Allow members of group sudo to execute any command%sudo ALL=(ALL:ALL) ALL# See sudoers(5) for more information on \"#include\" directives:#includedir /etc/sudoers.dWhen you’re done making changes, press Control-X, Y, Enter.Case 3If you are trying to fix the error where it says sudo is mode _____, should be 0440, then you’ll want to typechmod 0440 /etc/sudoersWhen you’re done with whatever commands you needed to enter, typeexitThis will bring you back to the recovery menu.Choose to resume a normal boot. Then you should be able to sudo again." }, { "title": "Install Latest MongoDB in Ubuntu", "url": "/blog/2012/08/29/install-latest-mongodb-in-ubuntu/", "categories": "Linux", "tags": "ubuntu, mongodb, bash", "date": "2012-08-29 13:46:00 -0400", "snippet": "A couple projects I work on use MongoDB as the database, and I’m generally not satisfied to use the (often outdated) version that ships with Ubuntu.As a result, I wrote this script to automate fetc...", "content": "A couple projects I work on use MongoDB as the database, and I’m generally not satisfied to use the (often outdated) version that ships with Ubuntu.As a result, I wrote this script to automate fetching, extracting and linking the latest version.To configure the script, just replace the PKG information with whatever value is most relevant for your configuration at http://www.mongodb.org/downloads.#!/bin/bashPKG=mongodb-linux-x86_64-2.2.0URL=http://fastdl.mongodb.org/linuxif [ -f /etc/init.d/mongod ]then echo &quot;mongod already installed ... stopping&quot; sudo service mongod stopelse echo &quot;installing mongod script&quot; wget https://raw.github.com/gist/3516078/713ad981715236473999f30636bca2ee3dcc5f24/mongod sudo mv mongod /etc/init.d/mongod sudo chmod +x /etc/init.d/mongod echo &quot;creating mongodb user&quot; sudo useradd mongodb echo &quot;creating directories&quot; sudo mkdir -p /var/lib/mongodb sudo mkdir -p /var/log/mongodb sudo chown mongodb.mongodb /var/lib/mongodb sudo chown mongodb.mongodb /var/log/mongodb sudo update-rc.d mongod defaultsfiwget $URL/$PKG.tgzsudo tar -xvzf $PKG.tgz -C /usr/local/if [ -d /usr/local/mongodb ]then echo &quot;removing existing mongodb symlink&quot; sudo rm -rf /usr/local/mongodb sudo ln -s /usr/local/$PKG /usr/local/mongodbelse echo &quot;creating symlinks&quot; sudo ln -s /usr/local/$PKG /usr/local/mongodb sudo ln -s /usr/local/mongodb/bin/bsondump /usr/local/bin/bsondump sudo ln -s /usr/local/mongodb/bin/mongo /usr/local/bin/mongo sudo ln -s /usr/local/mongodb/bin/mongod /usr/local/bin/mongod sudo ln -s /usr/local/mongodb/bin/mongodump /usr/local/bin/mongodump sudo ln -s /usr/local/mongodb/bin/mongoexport /usr/local/bin/mongoexport sudo ln -s /usr/local/mongodb/bin/mongofiles /usr/local/bin/mongofiles sudo ln -s /usr/local/mongodb/bin/mongoimport /usr/local/bin/mongoimport sudo ln -s /usr/local/mongodb/bin/mongorestore /usr/local/bin/mongorestore sudo ln -s /usr/local/mongodb/bin/mongos /usr/local/bin/mongos sudo ln -s /usr/local/mongodb/bin/mongosniff /usr/local/bin/mongosniff sudo ln -s /usr/local/mongodb/bin/mongostat /usr/local/bin/mongostatfiecho &quot;cleaning up&quot;rm $PKG.tgzecho &quot;starting mongod service&quot;sudo service mongod startNote that this script pulls mongod from a gist which I created. This script is originally from Ijonas Kisselbach’s dotfiles." }, { "title": "Keeping Pry Breakpoints out of Git", "url": "/blog/2012/08/23/keeping-pry-breakpoints-out-of-git/", "categories": "Ruby", "tags": "ruby, pry, git", "date": "2012-08-23 07:57:00 -0400", "snippet": "My Ruby workflow as of late has almost always contained Git for version control, and Pry for debugging.Although it’s extremely convenient during development to add a quick breakpoint using binding....", "content": "My Ruby workflow as of late has almost always contained Git for version control, and Pry for debugging.Although it’s extremely convenient during development to add a quick breakpoint using binding.pry, it can be a bit frustrating to clients if you accidentally deploy with these breakpoints still intact.After hunting around for a bit, I decided to write a pre-commit hook that would check the files I was about to check in to ensure that I didn’t accidentally still have breakpoints enabled.# Git pre-commit hook to check all staged Ruby (*.rb/haml/coffee) files # for Pry binding references## Installation## ln -s /path/to/pre-commit.sh /path/to/project/.git/hooks/pre-commit## Based on ## http://codeinthehole.com/writing/tips-for-using-a-git-pre-commit-hook/# http://mark-story.com/posts/view/using-git-commit-hooks-to-prevent-stupid-mistakes# https://gist.github.com/3266940#FILES_PATTERN=&#39;\\.(rb|haml|coffee)(\\..+)?$&#39;FORBIDDEN=&#39;binding.pry&#39;git diff --cached --name-only | \\ grep -E $FILES_PATTERN | \\ GREP_COLOR=&#39;4;5;37;41&#39; xargs grep --color --with-filename -n $FORBIDDEN &amp;&amp; \\ echo &#39;COMMIT REJECTED&#39; &amp;&amp; \\ exit 1exit 0This file just needs to be saved to /path/to/source/.git/hooks/pre-commit and made executable.HOOK_URL=\"https://gist.githubusercontent.com/alexbevi/3436040/raw/pre-commit.sh\"curl $HOOK_URL &gt; /path/to/source/.git/hooks/pre-commitchmod +x /path/to/source/.git/hooks/pre-commitIf you happen to leave a breakpoint intact, the next time you try to commit your changes, the commit will fail and indicate where these breakpoints are, and what files need to be updated to allow the commit to succeed." }, { "title": "Ubuntu 12.04 Desktop WITHOUT Unity", "url": "/blog/2012/08/10/ubuntu-12-dot-04-desktop-without-unity/", "categories": "Linux, Configuration", "tags": "linux, ubuntu", "date": "2012-08-10 08:03:00 -0400", "snippet": "As much as I enjoy Ubuntu, I haven’t warmed to their Unity desktop. I personally find it’s too similar to OS X, I don’t really like the launcher with larger icons, the global menu, the shifted wind...", "content": "As much as I enjoy Ubuntu, I haven’t warmed to their Unity desktop. I personally find it’s too similar to OS X, I don’t really like the launcher with larger icons, the global menu, the shifted window control buttons, notification area changes .. etc.Since it’s based on GTK3, I know you can modify this all, but I’d rather just use a desktop environment that doesn’t require that level of tweaking.Enter Cinnamon. Cinnamon is developed by the Linux Mint team, which has a distribution based on Ubuntu.Over at the LinuxBSDOS blog, there was an article on how to get Cinnamon installed in the latest Ubuntu.I found that after doing this, I ended up with no sound. To fix this, AskUbuntu has an article that I found useful." }, { "title": "Shoutcast Streaming from the Command Line", "url": "/blog/2012/07/18/shoutcast-streaming-from-the-command-line/", "categories": "Linux, Scripting", "tags": "linux, ubuntu, shoutcast", "date": "2012-07-18 08:20:00 -0400", "snippet": "I spend a lot of time working in a terminal, and I like to spend that time listening to music. Since I work primarily in Linux, I was hoping there would be an easy way to merge these two activities...", "content": "I spend a lot of time working in a terminal, and I like to spend that time listening to music. Since I work primarily in Linux, I was hoping there would be an easy way to merge these two activities … and it turns out there was :)After searching around, i found a post on the crunchbang forums that provided exactly the script I was looking for.I modified it a tiny bit to check for dependencies:#!/bin/bash# search shoutcast and send url to radiotray or another player# needs Bash 4, curl, [radiotray], [xsel to send url to X selection for pasting]# (comment out line 53 \"printf '%s'...\" if you don't use xsel)command -v curl &gt; /dev/null 2&gt;&amp;1 || { echo \"curl required.\" &gt;&amp;2; exit 1; }command -v xsel &gt; /dev/null 2&gt;&amp;1 || { echo \"xsel required.\" &gt;&amp;2; exit 1; }command -v radiotray &gt; /dev/null 2&gt;&amp;1 || { echo \"radiotray required.\" &gt;&amp;2; exit 1; }# choose player (&amp; options if necessary): radio url will be sent to it.radioplay() { radiotray \"$1\"# mplayer -playlist \"$1\" # replace 'mplayer -playlist' to taste, $1 will hold the url# exec mplayer -playlist \"$1\" # add 'exec' if you'd rather launch player and leave script}# start up radiotray in background if it's not already running# Comment out this line if you don't use radiotray.pgrep radiotray &gt;/dev/null || ( radiotray &gt;/dev/null 2&gt;&amp;1 &amp; )##########################################################################while truedoecho \"Please enter keyword(s)\"read keywordkeyword=\"${keyword// /%20}\" # escape spaces for urlresults=$( curl -s \"http://www.shoutcast.com/Internet-Radio/$keyword\" |awk 'BEGIN { RS=\"&lt;div class=\\\"dirlist\\\"&gt;\" FS=\"&gt;\"}NR &lt; 2 {next}{url = name = $2sub(/^.*title=\\\"/,\"\",name)sub(/\\\".*$/,\"\",name)sub(/^.*href=\\\"/,\"\",url)sub(/\\\".*$/,\"\",url)print url,name }' )[[ $results ]] || { echo \"Sorry, no results for $keyword\"; continue;}unset listdeclare -A list # make associative arraywhile read url name # read in awk's outputdo list[\"$name\"]=\"$url\"done &lt;&lt;&lt; \"$results\"PS3='Please enter the number of your choice &gt; 'while truedo select station in \"${!list[@]}\" 'Search Again' Quit do [[ $station = 'Search Again' ]] &amp;&amp; break 2 [[ $station = Quit ]] &amp;&amp; { echo 'Goodbye...'; exit; } [[ $station ]] &amp;&amp; { printf '%s' \"${list[$station]}\" | xsel --input #--clipboard # can paste url radioplay \"${list[$station]}\" break } doneecho \"Last station chosen was $station ( ${list[$station]} )\"donedone # closes loop started at line 18exit" }, { "title": "Here we go again ...", "url": "/blog/2012/07/18/here-we-go-again-dot-dot-dot/", "categories": "Writing", "tags": "blog", "date": "2012-07-18 07:53:00 -0400", "snippet": "So, another year, another blogging engine, another crack at blogging.This time around I’m trying out Octopress. There’s something very appealing about being able to interact with this blog via rake...", "content": "So, another year, another blogging engine, another crack at blogging.This time around I’m trying out Octopress. There’s something very appealing about being able to interact with this blog via rake tasks and Git.This page was generated using rake new_post[\"Here we go again ...\"]Octopress is essentially Jekyll, which means it can be deployed very easily to either Github or Heroku.Why all the links you may ask? Because I can write posts using Markdown, and I sometimes mix up the syntax between Markdown and Textile, so I’m just trying to hammer it into my brain ;)" } ]
