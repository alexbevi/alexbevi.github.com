<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: mongodb | ALEX BEVILACQUA]]></title>
    <link href="http://www.alexbevi.com/blog/categories/mongodb/atom.xml" rel="self"/>
    <link href="http://www.alexbevi.com/"/>
    <updated>2020-02-14T11:39:42-05:00</updated>
    <id>http://www.alexbevi.com/</id>
    <author>
        <name><![CDATA[Alex Bevilacqua]]></name>
        <email><![CDATA[alex@alexbevi.com]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[MongoDB Initial Sync Progress Monitoring]]></title>
        <link href="http://www.alexbevi.com/blog/2020/02/13/mongodb-initial-sync-progress-monitoring/"/>
        <updated>2020-02-13T12:34:49-05:00</updated>
        <id>http://www.alexbevi.com/blog/2020/02/13/mongodb-initial-sync-progress-monitoring</id>
        <content type="html"><![CDATA[Sometimes our replica sets fall off the [oplog](https://docs.mongodb.com/manual/core/replica-set-oplog/) and the node needs to be resynced. When this happens, an [Initial Sync](https://docs.mongodb.com/manual/core/replica-set-sync/#initial-sync) is required, which does the following:

1. Clones all databases except the local database. To clone, the `mongod` scans every collection in each source database and inserts all data into its own copies of these collections.
2. Applies all changes to the data set. Using the oplog from the source, the `mongod` updates its data set to reflect the current state of the replica set.

When the initial sync finishes, the member transitions from [`STARTUP2`](https://docs.mongodb.com/manual/reference/replica-states/#replstate.STARTUP2) to [`SECONDARY`](https://docs.mongodb.com/manual/reference/replica-states/#replstate.SECONDARY).

Some common questions when performing an initial sync of a [Replica Set Member](https://docs.mongodb.com/manual/core/replica-set-members/) are:

- How do I know if the sync is progressing?
- How long will this take to complete?

<!-- MORE -->

Determining if the sync is progressing can be done by either checking the size of the [`dbPath`]() of the syncing node or by running the [`db.adminCommand({ replSetGetStatus: 1, initialSync: 1 })`](https://docs.mongodb.com/manual/reference/command/replSetGetStatus/) command while connected to the SECONDARY via the mongo shell.

{% img /images/initsync-001.png %}

Checking the directory size of the SECONDARY that is being initial sync'ed will provide a good approximation as to how much data still remains to be copied. Note that as the WiredTiger storage engine doesn't "release" space when documents are deleted there is a high probability that the SECONDARY will have a _smaller total directory size_ than the sync source.

The `replSetGetStatus` command will produce a JSON document similar to the following. This document contains extensive details as to how the database/collection cloning is progressing, as well as any errors that have occurred during the process.

{% gist alexbevi/d52ffd2e27068dcdcc616a5aaf814907 %}

Depending on the number of databases and collections being sync'ed, the size of this document can be quite large and difficult to visually parse.

To try and improve this situation I've created the following script.

{% gist alexbevi/422890f191f4bcb82c06fbb621c69331 %}

By running this against the SECONDARY from the mongo shell, a more concise representation of the `initialSyncStatus` document is produced:

{% img /images/initsync-002.png %}

The script will also let you know if there have been any sync failures recorded, as well as what the last failure was.

{% img /images/initsync-003.png %}

Hopefully you'll find this useful when the time comes to resync one of your nodes.]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[What is MongoDB FTDC (aka. diagnostic.data)]]></title>
        <link href="http://www.alexbevi.com/blog/2020/01/26/what-is-mongodb-ftdc-aka-diagnostic-dot-data/"/>
        <updated>2020-01-26T18:14:50-05:00</updated>
        <id>http://www.alexbevi.com/blog/2020/01/26/what-is-mongodb-ftdc-aka-diagnostic-dot-data</id>
        <content type="html"><![CDATA[[Full Time Diagnostic Data Capture (FTDC)](https://docs.mongodb.com/manual/administration/analyzing-mongodb-performance/#full-time-diagnostic-data-capture) was introduced in MongoDB 3.2 (via [SERVER-19585](https://jira.mongodb.org/browse/SERVER-19585)), to incrementally collect the results of certain diagnostic commands to assist MongoDB support with troubleshooting issues.

On log rotation or startup, a `mongod` or `mongos` will collect and log:

- [`getCmdLineOpts`](https://docs.mongodb.com/manual/reference/command/getCmdLineOpts/): `db.adminCommand({getCmdLineOpts: true})`
- [`buildInfo`](https://docs.mongodb.com/manual/reference/command/buildInfo/): `db.adminCommand({buildInfo: true})`
- [`hostInfo`](https://docs.mongodb.com/manual/reference/command/hostInfo/): `db.adminCommand({hostInfo: true})`

As configured by [`diagnosticDataCollectionPeriodMillis`](https://docs.mongodb.com/manual/reference/parameters/index.html#param.diagnosticDataCollectionPeriodMillis) and defaulting to every 1 second, FTDC will collect the output of the following commands:

- [`serverStatus`](https://docs.mongodb.com/manual/reference/command/serverStatus/): `db.serverStatus({tcmalloc: true})`
- [`replSetGetStatus`](https://docs.mongodb.com/manual/reference/command/replSetGetStatus/): `rs.status()`
- [`collStats`](https://docs.mongodb.com/manual/reference/command/collStats/) for the [`local.oplog.rs`](https://docs.mongodb.com/manual/reference/local-database/#local.oplog.rs) collection ([mongod](https://docs.mongodb.com/manual/reference/program/mongod/#bin.mongod) only)
- [`connPoolStats`](https://docs.mongodb.com/manual/reference/command/connPoolStats/#dbcmd.connPoolStats) ([mongos](https://docs.mongodb.com/manual/reference/program/mongos/#bin.mongos) only)

When FTDC is enabled (per [`diagnosticDataCollectionEnabled`](https://docs.mongodb.com/manual/reference/parameters/index.html#param.diagnosticDataCollectionEnabled)), the `metrics.xxxxxxx` files will be stored in [`diagnosticDataCollectionDirectoryPath`](https://docs.mongodb.com/manual/reference/parameters/index.html#param.diagnosticDataCollectionDirectoryPath) which by default is the _diagnostic.data_ directory within the [`systemLog.path`](https://docs.mongodb.com/manual/reference/configuration-options/#systemLog.path).

With [SERVER-21818](https://jira.mongodb.org/browse/SERVER-21818) (introduced in MongoDB 3.2.13) and [SERVER-31400](https://jira.mongodb.org/browse/SERVER-31400) (introduced in MongoDB 3.4.16) the diagnostic data capture scope was broadened to not only include internal diagnostic commands but system metrics as well. Depending on the host operating system, the diagnostic data may include one or more of the following statistics:

- CPU utilization (ex: [`/proc/stat`](http://www.linuxhowtos.org/System/procstat.htm))
- Memory utilization (ex: [`/proc/meminfo`](https://www.thegeekdiary.com/understanding-proc-meminfo-file-analyzing-memory-utilization-in-linux/))
- Disk utilization related to performance (ex: [`*/sys/block/\*/stat*`](https://www.kernel.org/doc/Documentation/block/stat.txt))
- Network performance statistics ([`/proc/net/netstat`](https://unix.stackexchange.com/questions/435579/is-there-documentation-for-proc-net-netstat-and-proc-net-snmp))

The `metrics.xxxxxxx` files in the `diagnostic.data` directory contain only statistics about the performance of the system and the database. They are stored in a compressed format, and are not human-readable.

Just a quick note regarding privacy, regardless of the version, the data in _diagnostic.data_ never contains:

- Samples of queries, query predicates, or query results
- Data sampled from any end-user collection or index
- System or MongoDB user credentials or security certificates

FTDC data contains certain host machine information such as hostnames, operating system information, and the options or settings used to start the `mongod` or `mongos`. This information may be considered protected or confidential by some organizations or regulatory bodies, but is not typically considered to be [Personally Identifiable Information (PII)](https://en.wikipedia.org/wiki/Personal_data).

If you want to have a closer look at the diagnostic data collection process, you can inspect the [FTDC code](https://github.com/mongodb/mongo/tree/master/src/mongo/db/ftdc).

## FTDC Structure

<!-- MORE -->

There are two types of FTDC documents: a [BSON metadata document](https://github.com/mongodb/mongo/blob/r4.2.3/src/mongo/db/ftdc/util.h#L136), or a [BSON metric chunk](https://github.com/mongodb/mongo/blob/r4.2.3/src/mongo/db/ftdc/util.h#L150).

Each document is made up of an `_id`, a `type` and either a `doc` or `data` field. The `type` field is used to identify the document type:

- 0: Metadata Document
- 1: Metric Chunk

The `doc` or `data` fields will contain "samples" in the form of:

```javascript
{
  "start" : DateTime, /* Time at which all collecting started */
  "name" : String, /* name is from name() in FTDCCollectorInterface */
  {
        "start" : DateTime, /* Time at which name() collection started */
        "data" : { ... },   /* data comes from collect() in FTDCCollectorInterface */
        "end" : DateTime,   /* Time at which name() collection ended */
  },
  ... /* more than 1 collector be sampled */
  "end" : DateTime /* Time at which all collecting ended */
}
```

Samples are [collected by `FTDCCollectorInterface`](https://github.com/mongodb/mongo/blob/r4.2.3/src/mongo/db/ftdc/collector.h#L110) instances.

### Metadata Document

```javascript
{
  "_id":  DateTime,
  "type": 0,
  "doc":  { .. } /* Samples from collectors */
}
```

On log rotation or startup, the first FTDC entry will be collected and stored. This is a BSON document that contains information sampled by running [`getCmdLineOpts`](https://docs.mongodb.com/manual/reference/command/getCmdLineOpts/), [`buildInfo`](https://docs.mongodb.com/manual/reference/command/buildInfo/) and [`hostInfo`](https://docs.mongodb.com/manual/reference/command/hostInfo/).

```javascript
// example
{
  "start": DateTime,
  "buildInfo": { ... },
  "getCmdLineOpts": { ... },
  "hostInfo": { ... },
  "end": DateTime
}
```

This sample will be stored in the `doc` field of the metadata document.

### Metric Chunk

```javascript
{
  "_id":  DateTime,
  "type": 1
  "data": BinData(...)
}
```

During each collection interval (as configured by [`diagnosticDataCollectionPeriodMillis`](https://docs.mongodb.com/manual/reference/parameters/index.html#param.diagnosticDataCollectionPeriodMillis)), a metric chunk will be created and a sample will be collected, compressed and stored to the `data` document as Binary Data.

This sample can contain the results of internal commands such as [`serverStatus`](https://docs.mongodb.com/manual/reference/command/serverStatus/),[`replSetGetStatus`](https://docs.mongodb.com/manual/reference/command/replSetGetStatus/), [`collStats`](https://docs.mongodb.com/manual/reference/command/collStats/) for the [`local.oplog.rs`](https://docs.mongodb.com/manual/reference/local-database/#local.oplog.rs) collection or [`connPoolStats`](https://docs.mongodb.com/manual/reference/command/connPoolStats/#dbcmd.connPoolStats), as well as external system metrics.

```javascript
// example
{
  "start": DateTime,
  "serverStatus": { ... },
  "connPoolStats": { ... },
  "systemMetrics": { ... },
  "end": DateTime
}
```

## Decoding FTDC `metrics.xxxxxxx` files

FTDC files, such as the `metrics.2019-10-28T19-02-23Z-00000` example file we'll be working with below are just [BSON](http://bsonspec.org/) files. As such, the [`bsondump`](https://docs.mongodb.com/manual/reference/program/bsondump/) utility can be used to inspect the contents:

```bash
METRICS=metrics.2019-10-28T19-02-23Z-00000
bsondump --quiet $METRICS | less
```

{% img /images/ftdc-001.png %}

`bsondump` will default to emitting JSON, so we can interact with this using the [`jq`]() utility. For example, if we only want to review the _Metadata Document_ this could be done as follows:

```bash
# bsondump < 4.0
bsondump --quiet $METRICS | jq -s '.[] | select( .type == 0)' | less

# bsondump >= 4.0
bsondump --quiet $METRICS | jq -s '.[] | select( .type | ."$numberInt" == "0")' | less
```

{% img /images/ftdc-002.png %}

Working with _Metric Chunks_ is a little more complicated as they are actually zlib compressed BSON documents. We'll use the `jq` utility to only select the first chunk and the [Ruby](https://www.ruby-lang.org/en/) interpreter to decompress the zlib data. Note that the following command can be altered to navigate to other chunks (not only the first) as needed:

```bash
# bsondump < 4.0
METRICS=metrics.2019-12-20T14-22-56Z-00000
bsondump --quiet $METRICS | \
  jq -s '.[] | select( .type == 1)' | \
  jq -s 'first | .data ."$binary"' -Mc | \
  ruby -rzlib -rbase64 -e 'd = STDIN.read; print Zlib::Inflate.new.inflate(Base64.decode64(d)[4..-1])' | \
  bsondump --quiet

# bsondump >= 4.0
METRICS=metrics.2019-12-20T14-22-56Z-00000
bsondump --quiet $METRICS | \
  jq -s '.[] | select( .type | ."$numberInt" == "1")' | \
  jq -s 'first | .data ."$binary" .base64' -Mc | \
  ruby -rzlib -rbase64 -e 'd = STDIN.read; print Zlib::Inflate.new.inflate(Base64.decode64(d)[4..-1])' | \
  bsondump --quiet
```

You eagle-eyed Rubyists will notice that we're clipping the first 4 bytes from the binary data we're reading from STDIN. This is to drop the header before we try to decompress the stream.

If you don't do this [zlib](https://www.zlib.net/) will complain and fail:

```
Traceback (most recent call last):
        1: from -e:1:in `<main>'
-e:1:in `inflate': incorrect header check (Zlib::DataError)
```

The binary data has now been decompressed, and being BSON data we run it through `bsondump` again and voila:

{% img /images/ftdc-003.png %}

Hopefully this helps shed some light on what FTDC data is and what it contains. In a future post we'll look into doing something useful with this treasure trove of telemetry our clusters are generating every 1 second or so.]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Troubleshooting and Fixing Invariant Failure !_featureTracker on MongoDB Startup]]></title>
        <link href="http://www.alexbevi.com/blog/2020/01/23/troubleshooting-and-fixing-invariant-failure-featuretracker/"/>
        <updated>2020-01-23T05:34:53-05:00</updated>
        <id>http://www.alexbevi.com/blog/2020/01/23/troubleshooting-and-fixing-invariant-failure-featuretracker</id>
        <content type="html"><![CDATA[I recently found myself troubleshooting another [MongoDB](https://www.mongodb.com/) startup issue due to potential corruption within a [WiredTiger](https://docs.mongodb.com/manual/core/wiredtiger/) file. As I have previously covered this topic (see ["Recovering a WiredTiger collection from a corrupt MongoDB installation"]({% post_url 2016-02-10-recovering-a-wiredtiger-collection-from-a-corrupt-mongodb-installation %})), I wanted to share the diagnostic and troubleshooting journey in case it helps anyone who experiences this issue in the future.

To ensure I could troubleshoot this issue in isolation, I first collected a backup of the necessary files from the affected installation as follows:

```bash
tar -czvf metadata.tar.gz --exclude=WiredTigerStat* WiredTiger* _mdb_catalog.wt sizeStorer.wt
```

Once I had this backup I extracted it to a new location, then using [m](https://github.com/aheckmann/m) to select the versions of MongoDB to use tried to startup a standalone instance to see if I could reproduce the issue:

```bash
mkdir -p /tmp/repro
cd /tmp/repro
# move archive from earlier to the new directory first
tar xvf metadata.tar.gz
# This is the version of MongoDB reported to be crashing
m 3.4.18
mongod --dbpath .
```

Once the `mongod` started, we were able to see the failure and the process aborts (clipped log sample below).

```
2020-01-23T03:58:19.828-0500 I CONTROL  [initandlisten] db version v3.4.18
2020-01-23T03:58:19.828-0500 I CONTROL  [initandlisten] git version: 4410706bef6463369ea2f42399e9843903b31923
...
2020-01-23T03:58:20.187-0500 I -        [initandlisten] Invariant failure !_featureTracker src/mongo/db/storage/kv/kv_catalog.cpp 305
2020-01-23T03:58:20.187-0500 I -        [initandlisten]

***aborting after invariant() failure

2020-01-23T03:58:20.198-0500 F -        [initandlisten] Got signal: 6 (Aborted).
...
 mongod(_ZN5mongo15printStackTraceERSo+0x41) [0x55bb45c92111]
 mongod(+0x153F329) [0x55bb45c91329]
 mongod(+0x153F80D) [0x55bb45c9180d]
 libpthread.so.0(+0x12890) [0x7f5b7bee5890]
 libc.so.6(gsignal+0xC7) [0x7f5b7bb20e97]
 libc.so.6(abort+0x141) [0x7f5b7bb22801]
 mongod(_ZN5mongo17invariantOKFailedEPKcRKNS_6StatusES1_j+0x0) [0x55bb44f5b234]
 mongod(_ZN5mongo9KVCatalog4initEPNS_16OperationContextE+0x568) [0x55bb458db5e8]
 mongod(_ZN5mongo15KVStorageEngineC1EPNS_8KVEngineERKNS_22KVStorageEngineOptionsE+0x807) [0x55bb458e79f7]
 mongod(+0x124DFFA) [0x55bb4599fffa]
 mongod(_ZN5mongo20ServiceContextMongoD29initializeGlobalStorageEngineEv+0x697) [0x55bb45891627]
 mongod(+0x7F62AC) [0x55bb44f482ac]
 mongod(main+0x96B) [0x55bb44f66a6b]
 libc.so.6(__libc_start_main+0xE7) [0x7f5b7bb03b97]
 mongod(+0x86FFB1) [0x55bb44fc1fb1]
-----  END BACKTRACE  -----
Aborted (core dumped)
```

<!-- more -->

The `mongod` is failing to startup successfully due to an invariant failure during `KVCatalog::init`. We are able to determine this as the `mongod` log above tells us:

1. The MongoDB version in used (3.4.18)
2. The path to the source file where the failure occurred (file: `src/mongo/db/storage/kv/kv_catalog.cpp`, line: 305)

As MongoDB is open source, we can view the source for this release by going to https://github.com/mongodb/mongo/blob/r3.4.18/src/mongo/db/storage/kv/kv_catalog.cpp#L305, which will show us the following:

```cpp
if (FeatureTracker::isFeatureDocument(obj)) {
    // There should be at most one version document in the catalog.
    invariant(!_featureTracker);

    // Initialize the feature tracker and skip over the version document because it doesn't
    // correspond to a namespace entry.
    _featureTracker = FeatureTracker::get(opCtx, this, record->id);
    continue;
}
```

The comment preceding the invariant<sup id="f1">[1](#fn1)</sup> indicates that there's only one feature document to be present in the catalog, but what's the catalog?

```bash
ls -l *catalog*
-rw-r--r-- 1 alex 249856 Jan 23 03:58 _mdb_catalog.wt
```

As there's only one file that contains the word "catalog" this is good a place as any to start. The `_mdb_catalog` is a WiredTiger file, so to interact with it directly (outside of MongoDB) we will need to use the [WiredTiger command line utility](http://source.wiredtiger.com/mongodb-3.4/command_line.html), also know as `wt`.

The documentation link for `mongodb-3.4` points us to WiredTiger 2.9.2, so following the [build and installation instructions](http://source.wiredtiger.com/mongodb-3.4/build-posix.html) we compile a `wt` binary with support for the snappy compressor. This is due to MongoDB's WiredTiger storage engine using snappy as the default block compressor (see ["Compression"](https://docs.mongodb.com/manual/core/wiredtiger/#compression)).

```bash
cd /tmp/repro
git clone git://github.com/wiredtiger/wiredtiger.git
cd wiredtiger
git checkout 2.9.2
sh autogen.sh
# ensure you have the necessary development headers for the snappy compression
# library before compiling
./configure --enable-snappy && make
```

Once we've successfully build the `wt` utility with snappy compression we can dump our catalog to see if we can find a duplicate entry for the feature document.

```bash
cd /tmp/repro
# to shorten the amount of typing required, wrap the wt utility invocation in
# a function we can call instead
WT() { /tmp/repro/wiredtiger/wt -v -C "extensions=[\"/tmp/repro/wiredtiger/ext/compressors/snappy/.libs/libwiredtiger_snappy.so\"]" $@; }
# write the catalog dump out to a file
WT dump _mdb_catalog > dump.dat
```

NOTE: If you receive the following error, just re-run the command.

```
[1579773800:589375][9348:0x7fc9a8e17140], txn-recover: Recovery failed: WT_RUN_RECOVERY: recovery must be run to continue
wt: WT_RUN_RECOVERY: recovery must be run to continue
```

This error is due to the presence of content in the `journal/` that was created when we last ran the `mongod`.

With the catalog dumped we can now search it for the feature document:

```bash
grep isFeatureDoc dump.dat -B 1 -n

935-\c2\e5
936:C\00\00\00\08isFeatureDoc\00\01\0ans\00\12nonRepairable\00\00\00\00\00\00\00\00\00\12repairable\00\01\00\00\00\00\00\00\00\00
937-\c2\e6
938:C\00\00\00\08isFeatureDoc\00\01\0ans\00\12nonRepairable\00\00\00\00\00\00\00\00\00\12repairable\00\01\00\00\00\00\00\00\00\00
```

INTERESTING! I'm not really sure how the catalog was able to get into a state where two feature documents exist, but since we have a dump of the catalog let's try to remove one of those entries and then load the dump back into the catalog.

As the results appear to be identical, we'll just drop the first one and then try to load it back into the catalog.

```bash
# remove lines 935-936 and overwrite the file
sed -i -e '935,936d' dump.dat
# drop the contents of the _mdb_catalog table
WT truncate _mdb_catalog
# reload the table from the dump file
WT load -f dump.dat
```

If the table loaded successfully the output of the command should be something like `table:_mdb_catalog: 822`.

With a reloaded catalog, let's try spinning up the `mongod` again:

```
2020-01-23T05:24:54.911-0500 I CONTROL  [initandlisten] db version v3.4.18
...
2020-01-23T05:24:56.247-0500 E STORAGE  [initandlisten] no cursor for uri: table:SomeCollection/collection/34-1349843775853912065
2020-01-23T05:24:56.247-0500 F -        [initandlisten] Invalid access at address: 0x58
2020-01-23T05:24:56.259-0500 F -        [initandlisten] Got signal: 11 (Segmentation fault).
```

SUCCESS! The `mongod` is still crashing as the backing files for the database don't exist, but we should now be able to take our recovered files back to our node that was previously failing.

From our recovered directory compress the following files:

```
tar -czvf recovered.tar.gz --exclude=WiredTigerStat* WiredTiger* _mdb_catalog.wt sizeStorer.wt
```

Note that if the `mongod` fails to start with the recovered files you may have to clear out the `journal/` directory.

Hopefully this helps someone someday ;)

<em>If you enjoyed this post and like solving these types of problems, [MongoDB is hiring!](https://grnh.se/dcd90aac1)</em>

<hr/>
<small><b id="fn1">1</b> An invariant is a condition to test, that on failure will log the test condition, source file and line of code. [↩](#f1)</small>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Current Date Math in MongoDB Aggregations]]></title>
        <link href="http://www.alexbevi.com/blog/2020/01/17/current-date-math-in-mongodb-aggregations/"/>
        <updated>2020-01-17T06:30:17-05:00</updated>
        <id>http://www.alexbevi.com/blog/2020/01/17/current-date-math-in-mongodb-aggregations</id>
        <content type="html"><![CDATA[A challenge that I've had in the past while working with my data in MongoDB has been how to incorporate
date math into my aggregations.

``` javascript
db.foo.insertMany([
{ lastUpdated: new Date(new Date().setDate(new Date().getDate() - 1)) },
{ lastUpdated: new Date(new Date().setDate(new Date().getDate() - 5)) },
{ lastUpdated: new Date(new Date().setDate(new Date().getDate() - 9)) }
]);
db.foo.find();
/*
{ "_id" : ObjectId("5e219c6ecc99b35bb2975d9e"), "lastUpdated" : ISODate("2020-01-16T11:37:18.522Z") }
{ "_id" : ObjectId("5e219c6ecc99b35bb2975d9f"), "lastUpdated" : ISODate("2020-01-12T11:37:18.522Z") }
{ "_id" : ObjectId("5e219c6ecc99b35bb2975da0"), "lastUpdated" : ISODate("2020-01-08T11:37:18.522Z") }
*/
```

Given the 3 documents we've setup above, if I wanted to filter a pipeline to only [`$match`](https://docs.mongodb.com/manual/reference/operator/aggregation/match)
documents that are newer than 1 week old, I would have to resort to using Javascript:

``` javascript
// compare lastUpdated to a new Javascript Date object set to
// 7 days from the current date
db.foo.aggregate(
{ $match:
  { lastUpdated: { $gte: new Date(new Date().setDate(new Date().getDate() - 7)) } }
});
/*
{ "_id" : ObjectId("5e219c6ecc99b35bb2975d9e"), "lastUpdated" : ISODate("2020-01-16T11:37:18.522Z") }
{ "_id" : ObjectId("5e219c6ecc99b35bb2975d9f"), "lastUpdated" : ISODate("2020-01-12T11:37:18.522Z") }
*/
```

Now if your pipeline is running in a non-Javascript environment, the `new Date()` call within the pipeline
would likely throw an exception.

If you're working with MongoDB 4.2 or newer though, a new [`$$NOW` aggregation variable](https://docs.mongodb.com/manual/reference/aggregation-variables/#variable.NOW
) is available that can be combined with existing pipeline operators to [`$subtract`](https://docs.mongodb.com/manual/reference/operator/aggregation/subtract/index.html
) the number of milliseconds in the number of days to filter from the current date:

``` javascript
// compare lastUpdated to the number of milliseconds in
// 7 days subtracted from the current
db.foo.aggregate(
{ $match:
  { $expr:
    { $let:
      { vars:
        { start:
          { $subtract: ["$$NOW", (7 * 86400000)] }
        },
        in: { $gte: ["$lastUpdated", "$$start"] }
      }
    }
  }
});
/*
{ "_id" : ObjectId("5e219c6ecc99b35bb2975d9e"), "lastUpdated" : ISODate("2020-01-16T11:37:18.522Z") }
{ "_id" : ObjectId("5e219c6ecc99b35bb2975d9f"), "lastUpdated" : ISODate("2020-01-12T11:37:18.522Z") }
*/
```

I hope you find this as useful as I did. With each major release of MongoDB new features and functionality
are being introduced that reduce the "hacks" or "workarounds" we've had to do in the past.

If you're looking for more MongoDB tips and tricks, head on over to Asya's [Stupid Tricks With MongoDB](http://www.kamsky.org/stupid-tricks-with-mongodb).

Let me know in the comments below if you have any questions, or if you found this useful.
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Technical Services Engineering at MongoDB]]></title>
        <link href="http://www.alexbevi.com/blog/2018/10/01/technical-services-engineering-at-mongodb/"/>
        <updated>2018-10-01T15:39:28-04:00</updated>
        <id>http://www.alexbevi.com/blog/2018/10/01/technical-services-engineering-at-mongodb</id>
        <content type="html"><![CDATA[The goal of this post is to provide a first hand account of what it means to be a *Technical Services Engineer* at [MongoDB](https://www.mongodb.com/careers/jobs/791258), as well as what the journey getting to this point has looked like for me.

### WHO AM I?

I have been working in Application Development and Software Engineering for nearly two decades. I started off writing desktop applications in QuickBASIC and Turbo Pascal, then eventually in VB6, VB.NET, C++ and C#. When it was time to shift focus to web development I started off with HTML/JS/CSS (as we all do :P), then in Flash/AS3, Flex, Python, Ruby/Rails and Node.js.

I have been writing software since I was a kid, starting with some automation tools for my mom's business. I then moved on to building tools to help me cheat at various games I was playing at the time, and eventually got more into emulator programming and reverse engineering. I guess you could say I've always loved solving problems programmatically, and especially enjoyed identifying opportunities for automation and custom tooling.

This led me down an informal DevOps track, as I was finding there was a need for optimization in the infrastructure layers that my applications were deployed to. This led me deeper into Linux internals, system administration and network operations.

While I was gaining these new skill-sets my primary focus was always on application development and delivery. Before coming to MongoDB I was working as a Development Lead / System Architect, but I found that my focus was always being drawn back to solving performance challenges at the infrastructure level.

<!-- MORE -->

### WHY MONGODB?

I started working with MongoDB on a number of "hobby" projects around 2012. At the time I really only had experience with RDBMS', but due to the unstructured nature of the data I was working with decided to give this new technology a whirl.

I fell in love with the database almost immediately, and have since carried it forward to multiple new employers, as well as contract opportunities and consulting engagements.

The low barrier to entry from a development bootstrapping perspective made it the ideal backend for proof-of-concept development through to production deployment.

As a result of this increased activity with MongoDB, I found my self doing a lot more investigation into [performance issues](/blog/2018/05/28/troubleshooting-a-mongodb-performance-issue/) and [internals](/blog/2016/02/10/recovering-a-wiredtiger-collection-from-a-corrupt-mongodb-installation/) (links are to blog posts of challenges I encountered and resolved).

### WHY TECHNICAL SERVICES?

This was initially very challenging for me, as I had pre-conceived notions as to what "technical services" actually implied. The first thoughts that popped in my head were "technical support", "client support", "call center style support", etc.

While researching this position I came across a blog post from about six years ago by a MongoDB employee who blogged about his experience as a Support Engineer (in this [two](http://blog.markofu.com/2012/07/being-support-engineer-10gen-part-1.html) [part](http://blog.markofu.com/2012/10/being-support-engineer-10gen-part-2.html) series).

I found his reasons for joining MongoDB (10gen at the time), description of what kinds of challenges the job poses on a daily basis and how there is a constant push for self improvement and continuing education to align with what I was looking for in a new opportunity.

### WHAT'S A TECHNICAL SERVICES ENGINEER ON PAPER

To answer this question, let's start off by analyzing the [job posting](https://www.mongodb.com/careers/jobs/791258) that kicked off this journey for me in the first place.

{% img /images/why_tse/why_tse_001.png %}

So they're looking for people that are able to solve problems and communicate clearly. This could be a call center gig after all ... oh wait, *experts in MongoDB related database servers, drivers, tools, services* ... hrm, maybe there's a bit more to this.

{% img /images/why_tse/why_tse_002.png %}

*Architecture, performance, recovery, security*, those are a lot more complex than what you would face in a traditional support role. What really sold me though was the *contribute to internal projects* statement, as this aligned perfectly with my desire for process improvement through custom tooling.

{% img /images/why_tse/why_tse_003.png %}

By the time I got to this point in the job posting I was already sold. MongoDB is either trying to staff their first tier support with ridiculously over-qualified employees, or Technical Services really isn't what I would have thought.

I proceeded to fill out the application, attach my resume and cover letter and crossed my fingers.

### WHAT'S A TECHNICAL SERVICES ENGINEER IN PRACTICE

After working with other TSEs for the past two months and having had an opportunity to handle some of my own cases I think I can shed a bit of light on what this role really entails.

#### HOW IS IT A SUPPORT ROLE?

A Technical Services Engineer interacts with MongoDB's clients via a support queue. This allows incoming "cases" to be prioritized and categorized to allow engineers to quickly identify what form of subject matter expertise may be required (ex: `Indexing`, `Replication`, `Sharding`, `Performance`, `Networking`, etc).

As a TSE you're responsible for claiming cases from a queue and providing feedback in a timely fashion that is clear, concise and technically accurate.

#### HOW IS IT AN ENGINEERING ROLE?

Here's the juicy part of this job. Although replying to client requests is the "deliverable" for a TSE, how you go about reproducing their issues requires a very deep understanding of MongoDB internals, software engineering, network engineering, infrastructure architecture and technical troubleshooting.

Depending on the type of issue, a reproduction is likely in store. These involve recreating the environment (locally or in the cloud) to either benchmark or replicate the identified client challenge. There is a vast library of tools available to TSEs for these types of tasks, but on some occasions the right tool for the job may not exist.

In these cases, you have an opportunity to write your own scripts or tools to parse logs, measure performance, record telemetry or verify a hypothesis. Although MongoDB doesn't require TSEs to have any programming experience, for those like me that come from product engineering it's refreshing to know there's still an opportunity to scratch the development itch.

With each case you learn more about the inner working of the database, the tools, the drivers and OS level performance.

### CONCLUSION?

I'm leaving the closing section here as a question, as the TSE role continues to be redefined and refined as new MongoDB products come on board and new challenges present themselves.

What will likely remain constant though is the need for new engineers to have the following characteristics:

* a passion for continuing technical education
* a willingness to step outside their comfort zone
* an interest in software engineering
* an interest in network operations

I encourage you to check out MongoDB's [available jobs](https://grnh.se/dcd90aac1) if what I've described here interests you (I swear HR is not putting me up to this ...) as we could use more engineers like you in our ranks :)

Feel free to leave a comment below or shoot me an email at [alex@alexbevi.com](mailto:alex@alexbevi.com) if you have any questions.
]]></content>
    </entry>
    
</feed>
