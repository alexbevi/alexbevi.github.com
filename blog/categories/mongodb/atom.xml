<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: mongodb | ALEX BEVILACQUA]]></title>
    <link href="http://www.alexbevi.com/blog/categories/mongodb/atom.xml" rel="self"/>
    <link href="http://www.alexbevi.com/"/>
    <updated>2018-11-28T15:43:41-05:00</updated>
    <id>http://www.alexbevi.com/</id>
    <author>
        <name><![CDATA[Alex Bevilacqua]]></name>
        <email><![CDATA[alex@alexbevi.com]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[Technical Services Engineering at MongoDB]]></title>
        <link href="http://www.alexbevi.com/blog/2018/10/01/technical-services-engineering-at-mongodb/"/>
        <updated>2018-10-01T15:39:28-04:00</updated>
        <id>http://www.alexbevi.com/blog/2018/10/01/technical-services-engineering-at-mongodb</id>
        <content type="html"><![CDATA[The goal of this post is to provide a first hand account of what it means to be a *Technical Services Engineer* at [MongoDB](https://www.mongodb.com/careers/jobs/791258), as well as what the journey getting to this point has looked like for me.

### WHO AM I?

I have been working in Application Development and Software Engineering for nearly two decades. I started off writing desktop applications in QuickBASIC and Turbo Pascal, then eventually in VB6, VB.NET, C++ and C#. When it was time to shift focus to web development I started off with HTML/JS/CSS (as we all do :P), then in Flash/AS3, Flex, Python, Ruby/Rails and Node.js.

I have been writing software since I was a kid, starting with some automation tools for my mom's business. I then moved on to building tools to help me cheat at various games I was playing at the time, and eventually got more into emulator programming and reverse engineering. I guess you could say I've always loved solving problems programmatically, and especially enjoyed identifying opportunities for automation and custom tooling.

This led me down an informal DevOps track, as I was finding there was a need for optimization in the infrastructure layers that my applications were deployed to. This led me deeper into Linux internals, system administration and network operations.

While I was gaining these new skill-sets my primary focus was always on application development and delivery. Before coming to MongoDB I was working as a Development Lead / System Architect, but I found that my focus was always being drawn back to solving performance challenges at the infrastructure level.

<!-- MORE -->

### WHY MONGODB?

I started working with MongoDB on a number of "hobby" projects around 2012. At the time I really only had experience with RDBMS', but due to the unstructured nature of the data I was working with decided to give this new technology a whirl.

I fell in love with the database almost immediately, and have since carried it forward to multiple new employers, as well as contract opportunities and consulting engagements.

The low barrier to entry from a development bootstrapping perspective made it the ideal backend for proof-of-concept development through to production deployment.

As a result of this increased activity with MongoDB, I found my self doing a lot more investigation into [performance issues](/blog/2018/05/28/troubleshooting-a-mongodb-performance-issue/) and [internals](/blog/2016/02/10/recovering-a-wiredtiger-collection-from-a-corrupt-mongodb-installation/) (links are to blog posts of challenges I encountered and resolved).

### WHY TECHNICAL SERVICES?

This was initially very challenging for me, as I had pre-conceived notions as to what "technical services" actually implied. The first thoughts that popped in my head were "technical support", "client support", "call center style support", etc.

While researching this position I came across a blog post from about six years ago by a MongoDB employee who blogged about his experience as a Support Engineer (in this [two](http://blog.markofu.com/2012/07/being-support-engineer-10gen-part-1.html) [part](http://blog.markofu.com/2012/10/being-support-engineer-10gen-part-2.html) series).

I found his reasons for joining MongoDB (10gen at the time), description of what kinds of challenges the job poses on a daily basis and how there is a constant push for self improvement and continuing education to align with what I was looking for in a new opportunity.

### WHAT'S A TECHNICAL SERVICES ENGINEER ON PAPER

To answer this question, let's start off by analyzing the [job posting](https://www.mongodb.com/careers/jobs/791258) that kicked off this journey for me in the first place.

<img src="/images/why_tse/why_tse_001.png">

So they're looking for people that are able to solve problems and communicate clearly. This could be a call center gig after all ... oh wait, *experts in MongoDB related database servers, drivers, tools, services* ... hrm, maybe there's a bit more to this.

<img src="/images/why_tse/why_tse_002.png">

*Architecture, performance, recovery, security*, those are a lot more complex than what you would face in a traditional support role. What really sold me though was the *contribute to internal projects* statement, as this aligned perfectly with my desire for process improvement through custom tooling.

<img src="/images/why_tse/why_tse_003.png">

By the time I got to this point in the job posting I was already sold. MongoDB is either trying to staff their first tier support with ridiculously over-qualified employees, or Technical Services really isn't what I would have thought.

I proceeded to fill out the application, attach my resume and cover letter and crossed my fingers.

### WHAT'S A TECHNICAL SERVICES ENGINEER IN PRACTICE

After working with other TSEs for the past two months and having had an opportunity to handle some of my own cases I think I can shed a bit of light on what this role really entails.

#### HOW IS IT A SUPPORT ROLE?

A Technical Services Engineer interacts with MongoDB's clients via a support queue. This allows incoming "cases" to be prioritized and categorized to allow engineers to quickly identify what form of subject matter expertise may be required (ex: `Indexing`, `Replication`, `Sharding`, `Performance`, `Networking`, etc).

As a TSE you're responsible for claiming cases from a queue and providing feedback in a timely fashion that is clear, concise and technically accurate.

#### HOW IS IT AN ENGINEERING ROLE?

Here's the juicy part of this job. Although replying to client requests is the "deliverable" for a TSE, how you go about reproducing their issues requires a very deep understanding of MongoDB internals, software engineering, network engineering, infrastructure architecture and technical troubleshooting.

Depending on the type of issue, a reproduction is likely in store. These involve recreating the environment (locally or in the cloud) to either benchmark or replicate the identified client challenge. There is a vast library of tools available to TSEs for these types of tasks, but on some occasions the right tool for the job may not exist.

In these cases, you have an opportunity to write your own scripts or tools to parse logs, measure performance, record telemetry or verify a hypothesis. Although MongoDB doesn't require TSEs to have any programming experience, for those like me that come from product engineering it's refreshing to know there's still an opportunity to scratch the development itch.

With each case you learn more about the inner working of the database, the tools, the drivers and OS level performance.

### CONCLUSION?

I'm leaving the closing section here as a question, as the TSE role continues to be redefined and refined as new MongoDB products come on board and new challenges present themselves.

What will likely remain constant though is the need for new engineers to have the following characteristics:

* a passion for continuing technical education
* a willingness to step outside their comfort zone
* an interest in software engineering
* an interest in network operations

I encourage you to check out MongoDB's [available jobs](https://grnh.se/dcd90aac1) if what I've described here interests you (I swear HR is not putting me up to this ...) as we could use more engineers like you in our ranks :)

Feel free to leave a comment below or shoot me an email at [alex@alexbevi.com](mailto:alex@alexbevi.com) if you have any questions.
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Hello MongoDB]]></title>
        <link href="http://www.alexbevi.com/blog/2018/08/14/hello-mongodb/"/>
        <updated>2018-08-14T15:31:04-04:00</updated>
        <id>http://www.alexbevi.com/blog/2018/08/14/hello-mongodb</id>
        <content type="html"><![CDATA[As of August 13th, I am no longer a System Architect at DAC Group. I have a public post on [LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:6432589236368601088/) that got some good traction, but to summarize it was time to move on.

I've been a software engineer in some capacity or another for nearly 20 years now. The position I've taken is as a *Technical Services Engineer*, which is more of a support role than an active development role.

The decision to make this move wasn't make lightly. I've been working hands on with code or overseeing a team of developers on a day to day basis for most of my professional career. As such, I was also involved with software engineering, and this was no different in my role as a *System Architect*.

In that role, I was still committing code on a nearly daily basis. If not, I was performing code review, or working on a design for a new system or solution. I would consider this all to still be "hands on", though I had found myself mired in DevOps work a lot more than I would have liked (there were not sufficient Linux Sysadmins available to assist with the type of server operations oversight that was required).

The role at MongoDB isn't a traditional "Tech Support" type of role, as it requires a strong knowledge of networking, databases, system design, programming and client services. I've been a fan of the MongoDB server for over 8 years now, and have brought it along with me to several new consulting opportunities as well as the full time jobs I've help. I believe very strongly in the quality of this product, as well as the peripheral products that they've developed.

I think the time has come for a new adventure. This is the first step towards a new career journey with a new company, as opposed to an incremental move upwards within the same professional space.]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Troubleshooting a MongoDB Performance Issue]]></title>
        <link href="http://www.alexbevi.com/blog/2018/05/28/troubleshooting-a-mongodb-performance-issue/"/>
        <updated>2018-05-28T09:14:03-04:00</updated>
        <id>http://www.alexbevi.com/blog/2018/05/28/troubleshooting-a-mongodb-performance-issue</id>
        <content type="html"><![CDATA[**UPDATE (2018-06-28):** *I actually sent a link to this article to the author of the previous blog post and in her reply she indicates that the improvements to cache management and checkpoint areas were more likely to have improved my situation. Just wanted to call out how approachable the MongoDB team is even with these one-off type issues :). Thanks Sue!*

**UPDATE (2018-06-21):** *As we were running MongoDB 3.0.15 while all these issues were going on it's entirely possible that the [optimizations made to the write-ahead log of WiredTiger](https://engineering.mongodb.com/post/breaking-the-wiredtiger-logjam-the-write-ahead-log-1-2) may have also contributed to this improvement in performance :)*

The following is an edited excerpt from an email I sent out internally about an intermittent performance issue we've been experiencing for several years now. The daily processing challenges we've been experiencing revolved around running server-side javascript in order to produce daily reports. As our data ingestion rates rose and our data processing needs climbed, our server performance continued to degrade. This would occur regardless of the size of the VMs we would spin up.

### Postmortem

Our MongoDB cluster is configured with three (3) servers: 1x primary (write-enabled) and 2x secondaries (read-only). These are running on Azure DS14v2 VMs with 8TB of storage (8x 1TB striped via LVM as these were the largest premium SSD-based data disks available at the time).

Aside from the servers being scaled up periodically, this configuration has been constant since the inception of the product.

The only major upgrade came in the form of a migration from 2.6 to 3.0 in 2015. At the time this was a major shift as it required rewriting a number of the underlying system scripts as well as introducing LRS-based storage to try and squeeze some additional performance out of the disks. Why optimize for IOPS? Because the reporting platform was designed to copy a lot of data back and forth in order to generate reports segmented by dimension ("Group", "Company", "Country", "State", "City").

<img class="center" src="/images/20180528-mongo-001.png">

This chart (48 hours sampled from 1 week ago) shows *Cache Usage* spiking and *Replication Lag* spiking. The cache spikes occur as new writes trigger index activity, which invalidates (dirties) cached memory and causes cache eviction.

<!-- more -->

This slows down the speed at which the secondaries can request data from the primary, which spikes the lag. When the secondaries request more data, it would lock up the primary, which in turn affected the primary server’s ability to ingest new content and write it to disk. The read/write buffers back up and new write requests are throttled.

**Note** &mdash; As of MongoDB 4.0, [non-blocking secondary reads](https://www.mongodb.com/blog/post/mongodb-40-release-candidate-0-has-landed) have been added to address these types of latency issues.

This type of cascading failure was almost exclusively seen when a large batch process was being run in the morning directly on the primary mongod instance in the mornings..

<img class="center" src="/images/20180528-mongo-002.png">

This chart (48 hours sampled from 2 weeks ago) shows similar behaviour. The vertical lines show points at which we were forced to restart instances or cycle the primary server in order to recover resources.

You’ll notice that cache usage hits a certain point on the primary (left) server after which we have to kill the instance. The replication lag on the secondaries is also inconsistent, which would lead us to believe that the consumption rates from the primary are being affected by either network performance or disk performance.

In the absence of dedicated DevOps, DBAs or Infrastructure Engineers, the development teams have spent a significant amount of time learning to tune and troubleshoot this installation. Due to lack of specialization though occasionally issues may be misdiagnosed.

We completed a significant upgrade on Tuesday that brings our cluster up to mongodb-server 3.4.15 (from 3.0.15). The 3.0 series was first introduced in March 2015, with an end of life of February 2018. As no further security updates are being released, we’ve been coordinating tests with the product development teams for the past 12 months in order to prepare for a major upgrade.

This involved the deprecation of client-side javascript calls, as well as rewriting several map/reduce operations as aggregation pipelines to ensure when the transition happened there was no sudden outage.

Now that we’ve been running 3.4 in production for a few days I checked the same 48 hour sample and found something interesting …

<img class="center" src="/images/20180528-mongo-003.png">

The cache usage has remained steady since we turned the instances on. The replication lag also hasn’t gone much higher than a minute in the past few days (this could creep up to over an hour in the past!).

These samples include report generation for all products as well, so they represent the same load. We’ll have to continue to monitor this, but the initial results seem to show that a lot of the pain we’ve been suffering through may have stemmed from outdated software.

The way the review report is generated is still extremely inefficient, but if we continue seeing results like this for the foreseeable future then the urgency of redesigning that product drops and can be properly managed.

Here are some lessons we learned as a result of this investigation:

**Measure Everything** &mdash; Without proper telemetry in place, not only is it difficult to identify negative trends, but it’s almost impossible to showcase the success of any change or action.

**Understand Your Tech**  &mdash; Whether you’re using hosted, provisioned, on-premise, containerized, PAAS or some other solution as part of the application architecture, make sure you really understand how to use it, and how to support it. When MongoDB was introduced to the project it was done so to fill a specific need. Once that need was filled, resourcing discussions surrounding maintainability and support should likely have been prioritized.

**Document Everything**  &mdash; As discoveries are made, write them down and share them. Knowledge sharing is even more important when you’re dealing with issues that go beyond the standard requirements of "application development".

**Ask For Help** &mdash; When it becomes necessary to step outside your comfort zone to solve a problem, a fresh perspective can be welcome.

Hopefully this journey benefits someone else in a similar situation.]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Recovering a WiredTiger collection from a corrupt MongoDB installation]]></title>
        <link href="http://www.alexbevi.com/blog/2016/02/10/recovering-a-wiredtiger-collection-from-a-corrupt-mongodb-installation/"/>
        <updated>2016-02-10T20:38:38-05:00</updated>
        <id>http://www.alexbevi.com/blog/2016/02/10/recovering-a-wiredtiger-collection-from-a-corrupt-mongodb-installation</id>
        <content type="html"><![CDATA[Recently at work, we experienced a series of events that could have proven to be catastrophic for one of our datasets. We have a daily process that does daily cleanup, but relies on the presence of control data that is ETL'd in from another process.

The secondary process failed, and as a result, *everything* was "cleaned" ... aka, we purged an entire dataset.

This data happens to be on a 5 node replicaset (primary-secondary-secondary-arbiter-hidden), and the hidden node died over the holidays and I waited too long to recover it, so it was unable to ever catch up to the primary (always stuck in a RECOVERING state).

My incredible foresight (... laziness ... ) resulted in us having a backup of the data ready to be extracted from the out of sync hidden node. All we had to do was start up **mongod** ... right?

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2016-01-29T21:06:05.180-0500 I CONTROL  ***** SERVER RESTARTED *****
</span><span class='line'>2016-01-29T21:06:05.241-0500 I CONTROL  [initandlisten] MongoDB starting : pid=1745 port=27021 dbpath=/data 64-bit host=xxx
</span><span class='line'>2016-01-29T21:06:05.241-0500 I CONTROL  [initandlisten] db version v3.0.8
</span><span class='line'>2016-01-29T21:06:05.241-0500 I CONTROL  [initandlisten] git version: 83d8cc25e00e42856924d84e220fbe4a839e605d
</span><span class='line'>2016-01-29T21:06:05.241-0500 I CONTROL  [initandlisten] build info: Linux build3.ny.cbi.10gen.cc 2.6.32-431.3.1.el6.x86_64 #1 SMP Fri Jan 3 21:39:27 UTC 2014 x86_64 BOOST_LIB_VERSION=1_49
</span><span class='line'>2016-01-29T21:06:05.241-0500 I CONTROL  [initandlisten] allocator: tcmalloc
</span><span class='line'>...
</span><span class='line'>2016-01-29T21:06:05.315-0500 W -        [initandlisten] Detected unclean shutdown - /data/mongod.lock is not empty.
</span><span class='line'>2016-01-29T21:06:05.315-0500 W STORAGE  [initandlisten] Recovering data from the last clean checkpoint.
</span><span class='line'>2016-01-29T21:06:05.324-0500 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=13G,session_max=20000,eviction=(threads_max=4),statistics=(fast),log=(enabled=true,archive=true
</span><span class='line'>,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
</span><span class='line'>2016-01-29T21:06:05.725-0500 E STORAGE  [initandlisten] WiredTiger (0) [1454119565:724960][1745:0x7f2ac9534bc0], file:WiredTiger.wt, cursor.next: read checksum error for 4096B block at offset 6
</span><span class='line'>799360: block header checksum of 1769173605 doesn't match expected checksum of 4176084783
</span><span class='line'>2016-01-29T21:06:05.725-0500 E STORAGE  [initandlisten] WiredTiger (0) [1454119565:725067][1745:0x7f2ac9534bc0], file:WiredTiger.wt, cursor.next: WiredTiger.wt: encountered an illegal file form
</span><span class='line'>at or internal value
</span><span class='line'>2016-01-29T21:06:05.725-0500 E STORAGE  [initandlisten] WiredTiger (-31804) [1454119565:725088][1745:0x7f2ac9534bc0], file:WiredTiger.wt, cursor.next: the process must exit and restart: WT_PANI
</span><span class='line'>C: WiredTiger library panic
</span><span class='line'>2016-01-29T21:06:05.725-0500 I -        [initandlisten] Fatal Assertion 28558</span></code></pre></td></tr></table></div></figure>

Aw crap. I could not for the life of me get the node back up and running. Since this was a replica-set member, I thought maybe if I just copied the failing file from the (working) primary it would just work. Apparently that's not the way MongoDB or WiredTiger works :P. Back to the drawing board.

<!-- more -->

I could see that my data directory contained a bunch of `collection-*.wt` and `index-*.wt` files, so I assumed these were the WiredTiger collection and index files. These are binary files so `grep`-ing didn't help me identify the collection I needed.

I wanted to next see if I could just copy the collection's backing file directly to a new (working) MongoDB installation, so I started up a new `mongod`, created a new collection with a document in it, then copied over any `collection-*.wt` file to see what would happen.

Guess what ... didn't work.

**Identify the WiredTiger collection's backing file**

Since we had access to a working node, plus the collection hadn't been dropped (just purged), I thought maybe the files on each node would be the same. I logged into the primary via the shell to get some info from my collection.

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>db.getCollection('borkedCollection').stats()
</span><span class='line'>
</span><span class='line'>{
</span><span class='line'>    "ns" : "production.borkedCollection",
</span><span class='line'>    "count" : 0,
</span><span class='line'>    "size" : 0,
</span><span class='line'>    "storageSize" : 1138688,
</span><span class='line'>    "capped" : false,
</span><span class='line'>    "wiredTiger" : {
</span><span class='line'>        "metadata" : {
</span><span class='line'>            "formatVersion" : 1
</span><span class='line'>        },
</span><span class='line'>        "creationString" : "allocation_size=4KB,app_metadata=(formatVersion=1),block_allocation=best,block_compressor=snappy,cache_resident=0,checkpoint=(WiredTigerCheckpoint.5149=(addr=\"01808080808080c0b081e40ebe4855808080e3113fc0e401417fc0\",order=5149,time=1454966060,size=21078016,write_gen=119495)),checkpoint_lsn=(224134,44112768),checksum=on,collator=,columns=,dictionary=0,format=btree,huffman_key=,huffman_value=,id=178668,internal_item_max=0,internal_key_max=0,internal_key_truncate=,internal_page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=1MB,memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=0,prefix_compression_min=4,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,value_format=u,version=(major=1,minor=1)",
</span><span class='line'>        "type" : "file",
</span><span class='line'>        "uri" : "statistics:table:collection-7895--1435676552983097781",
</span><span class='line'>        "LSM" : {
</span><span class='line'>            //...
</span><span class='line'>        },
</span><span class='line'>        "block-manager" : {
</span><span class='line'>            //...
</span><span class='line'>        },
</span><span class='line'>        "btree" : {
</span><span class='line'>            // ...
</span><span class='line'>        },
</span><span class='line'>        // ...
</span><span class='line'>    },
</span><span class='line'>    "nindexes" : 4,
</span><span class='line'>    "totalIndexSize" : 1437696,
</span><span class='line'>    "indexSizes" : {
</span><span class='line'>        "_id_" : 212992,
</span><span class='line'>        // ...
</span><span class='line'>    },
</span><span class='line'>    "ok" : 1
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

That `"uri" : "statistics:table:collection-7895--1435676552983097781"` entry looked promising.

I started hunting for a way to extract the data from this file without having to "mount" the file in another MongoDB installation, as I assumed this was not possible. I stumbled across a command line utility for WiredTiger that [happened to have a 'salvage' command](http://source.wiredtiger.com/2.7.0/command_line.html#util_salvage). 

**Salvaging the WiredTiger collection**

In order to use the `wt` utility, you have to build it from source. Being comfortable in Linux, this was not daunting ;)

    wget http://source.wiredtiger.com/releases/wiredtiger-2.7.0.tar.bz2
    tar xvf wiredtiger-2.7.0.tar.bz2
    cd wiredtiger-2.7.0
    sudo apt-get install libsnappy-dev build-essential
    ./configure --enable-snappy
    make

**NOTE** adding support for Google's [snappy](https://github.com/google/snappy) compressor when building WiredTiger will save you some errors that I initially encountered when trying to salvage the data.

Now that I had a `wt` utility, I wanted to test it out on the collection file. It turns out that you need additional supporting files before you can do this. Once I'd copied over the necessary files, my working directory (called `mongo-bak`) looked like this:

    -rw-r--r-- 1 root      root      4738772992 Feb  9 14:06 collection-2657--1723320556100349955.wt
    -rw-r--r-- 1 root      root         1155072 Feb  9 14:05 _mdb_catalog.wt
    -rw-r--r-- 1 root      root        26935296 Feb  9 14:05 sizeStorer.wt
    -rw-r--r-- 1 root      root              95 Feb  9 14:05 storage.bson
    -rw-r--r-- 1 root      root              46 Feb  9 14:04 WiredTiger
    -rw-r--r-- 1 root      root             495 Feb  9 14:04 WiredTiger.basecfg
    -rw-r--r-- 1 root      root              21 Feb  9 14:04 WiredTiger.lock
    -rw-r--r-- 1 root      root             916 Feb  9 14:04 WiredTiger.turtle
    -rw-r--r-- 1 root      root        10436608 Feb  9 14:04 WiredTiger.wt

Now, from the directory where we compiled WiredTiger, we started salvaging the collection:

    ./wt -v -h ../mongo-bak -C "extensions=[./ext/compressors/snappy/.libs/libwiredtiger_snappy.so]" -R salvage collection-2657--1723320556100349955.wt

You know it's working if you see output along the lines of:

    WT_SESSION.salvage 639400             

which I believe is just counting up the number of documents recovered. Once the operation has completed, it will have overwritten the source `*.wt` collection file with whatever it could salvage.

The only issue is that you still can't load this into MongoDB yet.

**Importing the WiredTiger collection via dump/load into MongoDB**

In order to get the data into MongoDB, first we need to generate a dump file from the WiredTiger collection file. This is done using the `wt` utility:

    ./wt -v -h ../data -C "extensions=[./ext/compressors/snappy/.libs/libwiredtiger_snappy.so]" -R dump -f ../collection.dump collection-2657--1723320556100349955

This operation produces no output, so you'll just have to sit tight and wait a while. You can always `watch ls -l` in another console if you want to make sure it's working ;)

Once completed, you'll have a `collection.dump` file, but this *still* can't be loaded directly into MongoDB. You can however, using the `wt` utility one more time, load the dump back into a WiredTiger collection.

First, let's startup a new `mongod` instance that we can try this out on.

    mongod --dbpath tmp-mongo --storageEngine wiredTiger --nojournal                                  

Next, let's connect to this instance via the mongo shell and create a new collection:

    use Recovery
    db.borkedCollection.insert({test: 1})
    db.borkedCollection.remove({})
    db.borkedCollection.stats()

I've created a new db called *Recovery*, and inserted/removed a document so the collection's backing file would be generated. You can use the `stats()` method to get the collection name, but since we're only using one collection, it's easy enough to find just using `ls`.

Now we're going to take the backing file name of the collection we just created and use that to load our WiredTiger dump file:

    ./wt -v -h ../data -C "extensions=[./ext/compressors/snappy/.libs/libwiredtiger_snappy.so]" -R load -f ../collection.dump -r collection-2-880383588247732034

Note that we drop the `.wt` extension from the collection file above. Also, the `-h` flag needs to point to the directory where our `mongod` has it's `dbPath`. Finally, `mongod` should not be running.

This operation also provides a progress indicator showing how much data has been loaded:

    table:collection-4--4286091263744514813: 1386220

Once completed, we can start `mongod` back up, shell in and have a look:

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mongo
</span><span class='line'>MongoDB shell version: 3.2.1
</span><span class='line'>connecting to: test
</span><span class='line'>Mongo-Hacker 0.0.9
</span><span class='line'>laptop(mongod-3.2.1) test&gt; show dbs
</span><span class='line'>Recovery → 0.087GB
</span><span class='line'>local    → 0.000GB
</span><span class='line'>laptop(mongod-3.2.1) test&gt; use Recovery
</span><span class='line'>switched to db Recovery
</span><span class='line'>laptop(mongod-3.2.1) Recovery&gt; show collections
</span><span class='line'>borkedCollection → 0.000MB / 88.801MB
</span><span class='line'>laptop(mongod-3.2.1) Recovery&gt; db.borkedCollection.count()
</span><span class='line'>0</span></code></pre></td></tr></table></div></figure>

WTF? The size looks right, but there are no documents???

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>laptop(mongod-3.2.1) Recovery&gt; db.borkedCollection.find({}, {_id: 1})
</span><span class='line'>{
</span><span class='line'>  "_id": ObjectId("55e07f3b2e967329c888ac74")
</span><span class='line'>}
</span><span class='line'>{
</span><span class='line'>  "_id": ObjectId("55e07f3b2e967329c888ac76")
</span><span class='line'>}
</span><span class='line'>...
</span><span class='line'>{
</span><span class='line'>  "_id": ObjectId("55e07f402e967329c888ac85")
</span><span class='line'>}
</span><span class='line'>Fetched 20 record(s) in 29ms -- More[true]</span></code></pre></td></tr></table></div></figure>

Well that's promising, but the collection still hasn't been *properly* restored yet.

**Restoring the MongoDB collection to a usable state**

This final part is pretty straightforward, as we're just going to do a `mongodump`, followed by a `mongorestore`.

**NOTE** The `mongodump` will fail if you're using a version of MongoDB < 3.2, as 3.2 is built against WiredTiger 2.7. I initially tested this using MongoDB 3.0.9 and the dump operation just returned 0 results.

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mongodump
</span><span class='line'>2016-02-10T22:04:00.580-0500    writing Recovery.borkedCollection to 
</span><span class='line'>2016-02-10T22:04:03.579-0500    Recovery.borkedCollection  268219
</span><span class='line'>2016-02-10T22:04:06.579-0500    Recovery.borkedCollection  340655
</span><span class='line'>2016-02-10T22:04:09.579-0500    Recovery.borkedCollection  496787
</span><span class='line'>2016-02-10T22:04:12.579-0500    Recovery.borkedCollection  670894
</span><span class='line'>2016-02-10T22:04:15.579-0500    Recovery.borkedCollection  778539
</span><span class='line'>2016-02-10T22:04:18.579-0500    Recovery.borkedCollection  848525
</span><span class='line'>2016-02-10T22:04:21.579-0500    Recovery.borkedCollection  991277
</span><span class='line'>2016-02-10T22:04:24.579-0500    Recovery.borkedCollection  1147718
</span><span class='line'>2016-02-10T22:04:27.579-0500    Recovery.borkedCollection  1187600
</span><span class='line'>2016-02-10T22:04:30.579-0500    Recovery.borkedCollection  1353665
</span><span class='line'>2016-02-10T22:04:33.579-0500    Recovery.borkedCollection  1376255
</span><span class='line'>2016-02-10T22:04:33.681-0500    Recovery.borkedCollection  1386220
</span><span class='line'>2016-02-10T22:04:33.682-0500    done dumping Recovery.borkedCollection (1386220 documents)
</span><span class='line'>
</span><span class='line'>$ mongorestore --drop
</span><span class='line'>2016-02-10T22:05:51.959-0500    using default 'dump' directory
</span><span class='line'>2016-02-10T22:05:51.960-0500    building a list of dbs and collections to restore from dump dir
</span><span class='line'>2016-02-10T22:05:52.307-0500    reading metadata for Recovery.borkedCollection from dump/Recovery/borkedCollection.metadata.json
</span><span class='line'>2016-02-10T22:05:52.330-0500    restoring Recovery.borkedCollection from dump/Recovery/borkedCollection.bson
</span><span class='line'>2016-02-10T22:05:54.962-0500    [#.......................]  Recovery.borkedCollection  32.8 MB/569.8 MB  (5.8%)
</span><span class='line'>2016-02-10T22:05:57.962-0500    [###.....................]  Recovery.borkedCollection  80.5 MB/569.8 MB  (14.1%)
</span><span class='line'>2016-02-10T22:06:00.962-0500    [#####...................]  Recovery.borkedCollection  131.5 MB/569.8 MB  (23.1%)
</span><span class='line'>2016-02-10T22:06:03.962-0500    [#######.................]  Recovery.borkedCollection  178.5 MB/569.8 MB  (31.3%)
</span><span class='line'>2016-02-10T22:06:06.962-0500    [#########...............]  Recovery.borkedCollection  230.1 MB/569.8 MB  (40.4%)
</span><span class='line'>2016-02-10T22:06:09.962-0500    [###########.............]  Recovery.borkedCollection  271.6 MB/569.8 MB  (47.7%)
</span><span class='line'>2016-02-10T22:06:12.962-0500    [#############...........]  Recovery.borkedCollection  320.6 MB/569.8 MB  (56.3%)
</span><span class='line'>2016-02-10T22:06:15.962-0500    [###############.........]  Recovery.borkedCollection  366.3 MB/569.8 MB  (64.3%)
</span><span class='line'>2016-02-10T22:06:18.962-0500    [#################.......]  Recovery.borkedCollection  414.9 MB/569.8 MB  (72.8%)
</span><span class='line'>2016-02-10T22:06:21.962-0500    [###################.....]  Recovery.borkedCollection  464.8 MB/569.8 MB  (81.6%)
</span><span class='line'>2016-02-10T22:06:24.962-0500    [#####################...]  Recovery.borkedCollection  504.0 MB/569.8 MB  (88.4%)
</span><span class='line'>2016-02-10T22:06:27.962-0500    [#######################.]  Recovery.borkedCollection  554.5 MB/569.8 MB  (97.3%)
</span><span class='line'>2016-02-10T22:06:29.082-0500    [########################]  Recovery.borkedCollection  569.8 MB/569.8 MB  (100.0%)
</span><span class='line'>2016-02-10T22:06:29.082-0500    restoring indexes for collection Recovery.borkedCollection from metadata
</span><span class='line'>2016-02-10T22:06:29.104-0500    finished restoring Recovery.borkedCollection (1386220 documents)
</span><span class='line'>2016-02-10T22:06:29.104-0500    done</span></code></pre></td></tr></table></div></figure>

Now that we've dumped and reloaded the collection *yet again*, we can shell back in and validate that our recovery attempt has succeeded:

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mongo
</span><span class='line'>MongoDB shell version: 3.2.1
</span><span class='line'>connecting to: test
</span><span class='line'>Mongo-Hacker 0.0.9
</span><span class='line'>laptop(mongod-3.2.1) test&gt; show dbs
</span><span class='line'>Recovery → 0.099GB
</span><span class='line'>local    → 0.000GB
</span><span class='line'>laptop(mongod-3.2.1) test&gt; use Recovery
</span><span class='line'>switched to db Recovery
</span><span class='line'>laptop(mongod-3.2.1) Recovery&gt; show collections
</span><span class='line'>borkedCollection → 569.845MB / 88.594MB
</span><span class='line'>laptop(mongod-3.2.1) Recovery&gt; db.borkedCollection.count()
</span><span class='line'>1386220</span></code></pre></td></tr></table></div></figure>

BOOYA! Everything is back and properly accessible.

The `mongorestore` could actually have been done to the primary node in order to recover the data for production purposes. Once that's done, just recreate the necessary indexes and you're back in business.]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Identifying failing system.js functions in MongoDB]]></title>
        <link href="http://www.alexbevi.com/blog/2016/02/10/identifying-failing-system-dot-js-functions-in-mongodb/"/>
        <updated>2016-02-10T15:17:56-05:00</updated>
        <id>http://www.alexbevi.com/blog/2016/02/10/identifying-failing-system-dot-js-functions-in-mongodb</id>
        <content type="html"><![CDATA[``` bash
laptop(mongod-3.2.1) test> db.loadServerScripts()
2016-02-10T15:18:42.322-0500 E QUERY    [thread1] SyntaxError: unterminated string literal :
DB.prototype.loadServerScripts/<@src/mongo/shell/db.js:1158:9
DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1
DB.prototype.loadServerScripts@src/mongo/shell/db.js:1157:5
@(shell):1:1

2016-02-10T15:18:42.323-0500 E QUERY    [thread1] Error: SyntaxError: unterminated string literal :
DB.prototype.loadServerScripts/<@src/mongo/shell/db.js:1158:9
DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1
DB.prototype.loadServerScripts@src/mongo/shell/db.js:1157:5
@(shell):1:1
 :
DB.prototype.loadServerScripts/<@src/mongo/shell/db.js:1158:9
DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1
DB.prototype.loadServerScripts@src/mongo/shell/db.js:1157:5
@(shell):1:15:17:56
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>Occasionally we'll run into these scenarios where we need to load the `system.js` functions into the global context, but for whatever reason one (or more) scripts are borked.
</span><span class='line'>
</span><span class='line'>I created on that essentially looks like the following to illustrate this point.
</span></code></pre></td></tr></table></div></figure> javascript
var thisFunctionShouldFail = function() {
    return "Fail
}
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>When you try to execute a `db.loadServerScripts()` call, the entire process will fail as there is a malformed script.
</span><span class='line'>
</span><span class='line'>This is a major pain in the ass when you have large background processes that rely heavily on internal system scripts.
</span><span class='line'>
</span><span class='line'>In order to address this, we wrote a small script that you can run against any database to validate the internal scripts:
</span></code></pre></td></tr></table></div></figure> javascript
var testSystemJs = function() {
    var coll = db.system.js;
    coll.find({}, {_id: 1}).forEach(function(doc) {
       try {
           var func = coll.findOne({_id: doc._id});
           eval(func.value);
       } catch (ex) {
           print("LOAD_ERROR: " + doc._id);
       }
    });
}
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>Now if you run the above, it will give you a bit more context into the failures you may have ;)
</span></code></pre></td></tr></table></div></figure>
laptop(mongod-3.2.1) test> testSystemJs()
2016-02-10T15:52:13.086-0500 E QUERY    [thread1] SyntaxError: unterminated string literal :
testSystemJs/<@(shell):1:190
DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1
testSystemJs@(shell):1:66
@(shell):1:1

LOAD_ERROR: thisFunctionShouldFail
2016-02-10T15:52:13.088-0500 E QUERY    [thread1] SyntaxError: unterminated string literal :
testSystemJs/<@(shell):1:190
DBQuery.prototype.forEach@src/mongo/shell/query.js:477:1
testSystemJs@(shell):1:66
@(shell):1:1

LOAD_ERROR: thisFunctionShouldAlsoFail
```

I'm testing this on a mongo 3.2.1 system, but this method should be applicable to older releases as well.]]></content>
    </entry>
    
</feed>
